{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: augmentation_instance.py (deflated 80%)\n",
      "  adding: compare_training_and_test_distributions.py (deflated 68%)\n",
      "  adding: constants.py (deflated 71%)\n",
      "  adding: dataset.py (deflated 70%)\n",
      "  adding: feature_factory.py (deflated 83%)\n",
      "  adding: filter-training-and-test-data-given-features.py (deflated 65%)\n",
      "  adding: filter-training-and-test-data.py (deflated 63%)\n",
      "  adding: generate_data_for_augmentation_learning.py (deflated 64%)\n",
      "  adding: generate_data_for_augmentation_learning_spark.py (deflated 70%)\n",
      "  adding: learn_to_augment.py (deflated 57%)\n",
      "  adding: learning_task.py (deflated 79%)\n",
      "  adding: recommend_datasets.py (deflated 61%)\n",
      "  adding: recommender.py (deflated 79%)\n",
      "  adding: util/ (stored 0%)\n",
      "  adding: util/metrics.py (deflated 75%)\n",
      "  adding: util/misc.py (deflated 54%)\n",
      "  adding: util/feature_selection.py (deflated 62%)\n",
      "  adding: util/instance_parser.py (deflated 71%)\n",
      "  adding: util/__pycache__/ (stored 0%)\n",
      "  adding: util/__pycache__/instance_parser.cpython-37.pyc (deflated 53%)\n",
      "  adding: util/__pycache__/file_manager.cpython-37.pyc (deflated 46%)\n",
      "  adding: util/__pycache__/metrics.cpython-37.pyc (deflated 65%)\n",
      "  adding: util/__pycache__/misc.cpython-37.pyc (deflated 30%)\n",
      "  adding: util/debug.py (deflated 75%)\n",
      "  adding: util/graphic_functions.py (deflated 52%)\n",
      "  adding: util/file_manager.py (deflated 70%)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 87, in <module>\n",
      "    '\\n'.join(learning_instances.collect()),\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/rdd.py\", line 816, in collect\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 75, in <lambda>\n",
      "    lambda x: generate_learning_instance(file_dir, json.loads(x), params)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 31, in generate_learning_instance\n",
      "    features = augmentation_instance.generate_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 220, in generate_features\n",
      "    pairwise_features =  self.compute_pairwise_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 190, in compute_pairwise_features\n",
      "    feature_factory_candidate_with_target = FeatureFactory(self.get_joined_candidate_data_and_target())\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 126, in get_joined_candidate_data_and_target\n",
      "    return self.joined_dataset.get_data_columns(column_names, '_r')\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/dataset.py\", line 98, in get_data_columns\n",
      "    return self.data[correct_column_names]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 2986, in __getitem__\n",
      "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
      "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1092, in _get_listlike_indexer\n",
      "    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1185, in _validate_read_indexer\n",
      "    raise KeyError(\"{} not in index\".format(not_found))\n",
      "KeyError: \"['BP_APPLNO_r'] not in index\"\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 75, in <lambda>\n",
      "    lambda x: generate_learning_instance(file_dir, json.loads(x), params)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 31, in generate_learning_instance\n",
      "    features = augmentation_instance.generate_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 220, in generate_features\n",
      "    pairwise_features =  self.compute_pairwise_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 190, in compute_pairwise_features\n",
      "    feature_factory_candidate_with_target = FeatureFactory(self.get_joined_candidate_data_and_target())\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 126, in get_joined_candidate_data_and_target\n",
      "    return self.joined_dataset.get_data_columns(column_names, '_r')\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/dataset.py\", line 98, in get_data_columns\n",
      "    return self.data[correct_column_names]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 2986, in __getitem__\n",
      "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
      "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1092, in _get_listlike_indexer\n",
      "    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1185, in _validate_read_indexer\n",
      "    raise KeyError(\"{} not in index\".format(not_found))\n",
      "KeyError: \"['BP_APPLNO_r'] not in index\"\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/02/08 22:44:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/02/08 22:44:47 INFO SparkContext: Running Spark version 2.4.4\n",
      "20/02/08 22:44:47 INFO SparkContext: Submitted application: Feature Generation\n",
      "20/02/08 22:44:48 INFO SecurityManager: Changing view acls to: fchirigati\n",
      "20/02/08 22:44:48 INFO SecurityManager: Changing modify acls to: fchirigati\n",
      "20/02/08 22:44:48 INFO SecurityManager: Changing view acls groups to: \n",
      "20/02/08 22:44:48 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/02/08 22:44:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fchirigati); groups with view permissions: Set(); users  with modify permissions: Set(fchirigati); groups with modify permissions: Set()\n",
      "20/02/08 22:44:48 INFO Utils: Successfully started service 'sparkDriver' on port 65326.\n",
      "20/02/08 22:44:48 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/02/08 22:44:48 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/02/08 22:44:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/02/08 22:44:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/02/08 22:44:48 INFO DiskBlockManager: Created local directory at /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/blockmgr-78286d82-a3c5-49b0-aa8f-1c33e686a595\n",
      "20/02/08 22:44:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/02/08 22:44:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/02/08 22:44:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/02/08 22:44:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://mctest1.shc.sa.nyu.edu:4040\n",
      "20/02/08 22:44:48 INFO SparkContext: Added file file:///Users/fchirigati/projects/dataset-ranking/improvement-prediction/.params_feature_generation.json at file:///Users/fchirigati/projects/dataset-ranking/improvement-prediction/.params_feature_generation.json with timestamp 1581219888943\n",
      "20/02/08 22:44:48 INFO Utils: Copying /Users/fchirigati/projects/dataset-ranking/improvement-prediction/.params_feature_generation.json to /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/spark-74868911-ddf7-4863-94a3-91c8481daa0f/userFiles-58a10373-2e81-4c22-9658-29ee4f28730a/.params_feature_generation.json\n",
      "20/02/08 22:44:49 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/02/08 22:44:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65327.\n",
      "20/02/08 22:44:49 INFO NettyBlockTransferService: Server created on mctest1.shc.sa.nyu.edu:65327\n",
      "20/02/08 22:44:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/02/08 22:44:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, mctest1.shc.sa.nyu.edu, 65327, None)\n",
      "20/02/08 22:44:49 INFO BlockManagerMasterEndpoint: Registering block manager mctest1.shc.sa.nyu.edu:65327 with 366.3 MB RAM, BlockManagerId(driver, mctest1.shc.sa.nyu.edu, 65327, None)\n",
      "20/02/08 22:44:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, mctest1.shc.sa.nyu.edu, 65327, None)\n",
      "20/02/08 22:44:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, mctest1.shc.sa.nyu.edu, 65327, None)\n",
      "20/02/08 22:44:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 236.7 KB, free 366.1 MB)\n",
      "20/02/08 22:44:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.9 KB, free 366.0 MB)\n",
      "20/02/08 22:44:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on mctest1.shc.sa.nyu.edu:65327 (size: 22.9 KB, free: 366.3 MB)\n",
      "20/02/08 22:44:50 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "20/02/08 22:44:51 INFO FileInputFormat: Total input paths to process : 1\n",
      "20/02/08 22:44:51 INFO SparkContext: Starting job: collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Got job 0 (collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87) with 2 output partitions\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Final stage: ResultStage 0 (collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87)\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Missing parents: List()\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87), which has no missing parents\n",
      "20/02/08 22:44:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)\n",
      "20/02/08 22:44:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.6 KB, free 366.0 MB)\n",
      "20/02/08 22:44:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on mctest1.shc.sa.nyu.edu:65327 (size: 5.6 KB, free: 366.3 MB)\n",
      "20/02/08 22:44:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "20/02/08 22:44:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[2] at collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87) (first 15 tasks are for partitions Vector(0, 1))\n",
      "20/02/08 22:44:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks\n",
      "20/02/08 22:44:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7965 bytes)\n",
      "20/02/08 22:44:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7965 bytes)\n",
      "20/02/08 22:44:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "20/02/08 22:44:51 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "20/02/08 22:44:51 INFO Executor: Fetching file:///Users/fchirigati/projects/dataset-ranking/improvement-prediction/.params_feature_generation.json with timestamp 1581219888943\n",
      "20/02/08 22:44:51 INFO Utils: /Users/fchirigati/projects/dataset-ranking/improvement-prediction/.params_feature_generation.json has been previously copied to /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/spark-74868911-ddf7-4863-94a3-91c8481daa0f/userFiles-58a10373-2e81-4c22-9658-29ee4f28730a/.params_feature_generation.json\n",
      "20/02/08 22:44:51 INFO HadoopRDD: Input split: file:/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records/datamart-records:3797+3798\n",
      "20/02/08 22:44:51 INFO HadoopRDD: Input split: file:/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records/datamart-records:0+3797\n",
      "/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets/poverty-estimation/joined-datasets/7.csv\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets/poverty-estimation/joined-datasets/7.csv\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/cluster/supervised.py:859: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets/poverty-estimation/joined-datasets/8.csv\n",
      "20/02/08 22:44:57 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 75, in <lambda>\n",
      "    lambda x: generate_learning_instance(file_dir, json.loads(x), params)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 31, in generate_learning_instance\n",
      "    features = augmentation_instance.generate_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 220, in generate_features\n",
      "    pairwise_features =  self.compute_pairwise_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 190, in compute_pairwise_features\n",
      "    feature_factory_candidate_with_target = FeatureFactory(self.get_joined_candidate_data_and_target())\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 126, in get_joined_candidate_data_and_target\n",
      "    return self.joined_dataset.get_data_columns(column_names, '_r')\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/dataset.py\", line 98, in get_data_columns\n",
      "    return self.data[correct_column_names]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 2986, in __getitem__\n",
      "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
      "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1092, in _get_listlike_indexer\n",
      "    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1185, in _validate_read_indexer\n",
      "    raise KeyError(\"{} not in index\".format(not_found))\n",
      "KeyError: \"['BP_APPLNO_r'] not in index\"\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "20/02/08 22:44:57 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 75, in <lambda>\n",
      "    lambda x: generate_learning_instance(file_dir, json.loads(x), params)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 31, in generate_learning_instance\n",
      "    features = augmentation_instance.generate_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 220, in generate_features\n",
      "    pairwise_features =  self.compute_pairwise_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 190, in compute_pairwise_features\n",
      "    feature_factory_candidate_with_target = FeatureFactory(self.get_joined_candidate_data_and_target())\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 126, in get_joined_candidate_data_and_target\n",
      "    return self.joined_dataset.get_data_columns(column_names, '_r')\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/dataset.py\", line 98, in get_data_columns\n",
      "    return self.data[correct_column_names]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 2986, in __getitem__\n",
      "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
      "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1092, in _get_listlike_indexer\n",
      "    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1185, in _validate_read_indexer\n",
      "    raise KeyError(\"{} not in index\".format(not_found))\n",
      "KeyError: \"['BP_APPLNO_r'] not in index\"\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "20/02/08 22:44:57 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job\n",
      "20/02/08 22:44:57 INFO TaskSchedulerImpl: Cancelling stage 0\n",
      "20/02/08 22:44:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled\n",
      "20/02/08 22:44:57 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled\n",
      "20/02/08 22:44:57 INFO TaskSchedulerImpl: Stage 0 was cancelled\n",
      "20/02/08 22:44:57 INFO DAGScheduler: ResultStage 0 (collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87) failed in 5.948 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 393, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 75, in <lambda>\n",
      "    lambda x: generate_learning_instance(file_dir, json.loads(x), params)\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py\", line 31, in generate_learning_instance\n",
      "    features = augmentation_instance.generate_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 220, in generate_features\n",
      "    pairwise_features =  self.compute_pairwise_features()\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 190, in compute_pairwise_features\n",
      "    feature_factory_candidate_with_target = FeatureFactory(self.get_joined_candidate_data_and_target())\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/augmentation_instance.py\", line 126, in get_joined_candidate_data_and_target\n",
      "    return self.joined_dataset.get_data_columns(column_names, '_r')\n",
      "  File \"/Users/fchirigati/projects/dataset-ranking/improvement-prediction/dataset.py\", line 98, in get_data_columns\n",
      "    return self.data[correct_column_names]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 2986, in __getitem__\n",
      "    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1285, in _convert_to_indexer\n",
      "    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1092, in _get_listlike_indexer\n",
      "    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1185, in _validate_read_indexer\n",
      "    raise KeyError(\"{} not in index\".format(not_found))\n",
      "KeyError: \"['BP_APPLNO_r'] not in index\"\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "20/02/08 22:44:57 INFO DAGScheduler: Job 0 failed: collect at /Users/fchirigati/projects/dataset-ranking/improvement-prediction/generate_data_for_augmentation_learning_spark.py:87, took 6.010448 s\n",
      "20/02/08 22:44:57 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "20/02/08 22:44:57 INFO SparkUI: Stopped Spark web UI at http://mctest1.shc.sa.nyu.edu:4040\n",
      "20/02/08 22:44:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/02/08 22:44:57 INFO Executor: Executor killed task 0.0 in stage 0.0 (TID 0), reason: Stage cancelled\n",
      "20/02/08 22:44:57 INFO MemoryStore: MemoryStore cleared\n",
      "20/02/08 22:44:57 INFO BlockManager: BlockManager stopped\n",
      "20/02/08 22:44:57 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/02/08 22:44:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/02/08 22:44:57 INFO SparkContext: Successfully stopped SparkContext\n",
      "20/02/08 22:44:57 INFO ShutdownHookManager: Shutdown hook called\n",
      "20/02/08 22:44:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/spark-74868911-ddf7-4863-94a3-91c8481daa0f/pyspark-a5ccb544-f7b3-4e19-bac4-19a92d3a6a45\n",
      "20/02/08 22:44:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/spark-74868911-ddf7-4863-94a3-91c8481daa0f\n",
      "20/02/08 22:44:57 INFO ShutdownHookManager: Deleting directory /private/var/folders/2m/l7l9md295llfp5dvnqrwvydh0000gn/T/spark-b4bf5631-61f9-4448-af70-4435005dbc41\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ncd ../../improvement-prediction\\n\\nif test -f /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features; then\\n    rm /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\\nfi\\n\\ncat > .params_feature_generation.json << EOL\\n{\\n    \"learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records\",\\n    \"augmentation_learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\",\\n    \"file_dir\": \"data/\",\\n    \"cluster\": false,\\n    \"hdfs_address\": \"http://gray01.poly.edu:50070\",\\n    \"hdfs_user\": \"fsc234\"\\n}\\nEOL\\n\\n./run-spark-client\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0eadaacc56d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ncd ../../improvement-prediction\\n\\nif test -f /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features; then\\n    rm /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\\nfi\\n\\ncat > .params_feature_generation.json << EOL\\n{\\n    \"learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records\",\\n    \"augmentation_learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\",\\n    \"file_dir\": \"data/\",\\n    \"cluster\": false,\\n    \"hdfs_address\": \"http://gray01.poly.edu:50070\",\\n    \"hdfs_user\": \"fsc234\"\\n}\\nEOL\\n\\n./run-spark-client\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ncd ../../improvement-prediction\\n\\nif test -f /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features; then\\n    rm /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\\nfi\\n\\ncat > .params_feature_generation.json << EOL\\n{\\n    \"learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records\",\\n    \"augmentation_learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\",\\n    \"file_dir\": \"data/\",\\n    \"cluster\": false,\\n    \"hdfs_address\": \"http://gray01.poly.edu:50070\",\\n    \"hdfs_user\": \"fsc234\"\\n}\\nEOL\\n\\n./run-spark-client\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../../improvement-prediction\n",
    "\n",
    "if test -f /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features; then\n",
    "    rm /Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\n",
    "fi\n",
    "\n",
    "cat > .params_feature_generation.json << EOL\n",
    "{\n",
    "    \"learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records\",\n",
    "    \"augmentation_learning_data_filename\": \"/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features\",\n",
    "    \"file_dir\": \"data/\",\n",
    "    \"cluster\": false,\n",
    "    \"hdfs_address\": \"http://gray01.poly.edu:50070\",\n",
    "    \"hdfs_user\": \"fsc234\"\n",
    "}\n",
    "EOL\n",
    "\n",
    "./run-spark-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = pd.read_csv(\n",
    "    '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features',\n",
    "    header=None,\n",
    "    names=[\n",
    "        'query',\n",
    "        'target',\n",
    "        'candidate',\n",
    "        'query_num_of_columns',\n",
    "        'query_num_of_rows',\n",
    "        'query_row_column_ratio',\n",
    "        'query_max_mean',\n",
    "        'query_max_outlier_percentage',\n",
    "        'query_max_skewness',\n",
    "        'query_max_kurtosis',\n",
    "        'query_max_unique',\n",
    "        'candidate_num_of_columns',\n",
    "        'candidate_num_rows',\n",
    "        'candidate_row_column_ratio',\n",
    "        'candidate_max_mean',\n",
    "        'candidate_max_outlier_percentage',\n",
    "        'candidate_max_skewness',\n",
    "        'candidate_max_kurtosis',\n",
    "        'candidate_max_unique',\n",
    "        'query_target_max_pearson',\n",
    "        'query_target_max_spearman',\n",
    "        'query_target_max_covariance',\n",
    "        'query_target_max_mutual_info',\n",
    "        'candidate_target_max_pearson',\n",
    "        'candidate_target_max_spearman',\n",
    "        'candidate_target_max_covariance',\n",
    "        'candidate_target_max_mutual_info',\n",
    "        'max_pearson_difference',\n",
    "        'containment_fraction',\n",
    "        'decrease_in_mae',\n",
    "        'decrease_in_mse',\n",
    "        'decrease_in_medae',\n",
    "        'gain_in_r2_score'\n",
    "    ]\n",
    ")\n",
    "features.to_csv(\n",
    "    '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/datamart-records-features',\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
