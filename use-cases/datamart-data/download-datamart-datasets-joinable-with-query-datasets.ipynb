{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Version Requirement:** Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3m import container\n",
    "import datamart\n",
    "import datamart_nyu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, \\\n",
    "    mean_squared_log_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAMART_PATH = '/Users/fchirigati/projects/d3m/datamart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(data, target_variable_name):\n",
    "    \"\"\"Builds a model using data to predict the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(target_variable_name, axis=1),\n",
    "        data[target_variable_name],\n",
    "        test_size=0.33,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # normalizing data first\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train.values.reshape(-1, 1))\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train.values.reshape(-1, 1))\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    y_test = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    forest = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=len(data.columns)-1\n",
    "    )\n",
    "    forest.fit(X_train, y_train.ravel())\n",
    "    yfit = forest.predict(X_test)\n",
    "\n",
    "    return dict(\n",
    "        mean_absolute_error=mean_absolute_error(y_test, yfit),\n",
    "        mean_squared_error=mean_squared_error(y_test, yfit),\n",
    "        median_absolute_error=median_absolute_error(y_test, yfit),\n",
    "        r2_score=r2_score(y_test, yfit)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_scores(data, target_variable_name, missing_value_imputation):\n",
    "    \"\"\"Builds a model using data to predict the target variable,\n",
    "    returning different performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if missing_value_imputation:\n",
    "        \n",
    "        # imputation on data\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(data))\n",
    "        new_data.columns = data.columns\n",
    "        new_data.index = data.index\n",
    "\n",
    "        # training and testing model\n",
    "        return train_and_test_model(new_data, target_variable_name)\n",
    "\n",
    "    else:\n",
    "        return train_and_test_model(data, target_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    if not results:\n",
    "        return\n",
    "    for result in results:\n",
    "        print(result.score())\n",
    "        print(result.get_json_metadata()['metadata']['name'])\n",
    "        if (result.get_augment_hint()):\n",
    "            left_columns = []\n",
    "            for column_ in result.get_augment_hint().left_columns:\n",
    "                left_columns.append([])\n",
    "                for column in column_:\n",
    "                    left_columns[-1].append((column.resource_id, column.column_index))\n",
    "            print(\"Left Columns: %s\" % str(left_columns))\n",
    "            right_columns = []\n",
    "            for column_ in result.get_augment_hint().right_columns:\n",
    "                right_columns.append([])\n",
    "                for column in column_:\n",
    "                    right_columns[-1].append((column.resource_id, column.column_index))\n",
    "            print(\"Right Columns: %s\" % str(right_columns))\n",
    "        else:\n",
    "            print(result.id())\n",
    "        print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_materialize_info(results):\n",
    "    if not results:\n",
    "        return\n",
    "    id_to_materialize = dict()\n",
    "    for result in results:\n",
    "        id_ = result.get_json_metadata()['id']\n",
    "        if id_ in id_to_materialize:\n",
    "            continue\n",
    "        id_to_materialize[id_] = dict(\n",
    "            has_info=False,\n",
    "            url=None,\n",
    "            path=None\n",
    "        )\n",
    "        if 'direct_url' in result.get_json_metadata()['metadata']['materialize']:\n",
    "            id_to_materialize[id_]['url'] = result.get_json_metadata()['metadata']['materialize']['direct_url']\n",
    "            id_to_materialize[id_]['has_info'] = True\n",
    "        else:\n",
    "            # try to find them on volumes\n",
    "            datamart_file_path = os.path.join(DATAMART_PATH, 'volumes/datasets', id_, 'main.csv')\n",
    "            if os.path.exists(datamart_file_path):\n",
    "                id_to_materialize[id_]['path'] = datamart_file_path\n",
    "                id_to_materialize[id_]['has_info'] = True\n",
    "    return id_to_materialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_datasets_and_generate_training_records(results, supplied_data, supplied_data_path, target,\n",
    "                                                    id_to_materialize, dir_):\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    training_records = list()\n",
    "    \n",
    "    current_working_dir = os.getcwd()\n",
    "    os.chdir(dir_)\n",
    "    try:\n",
    "        # downloading candidate datasets\n",
    "        for id_ in id_to_materialize:\n",
    "            if id_to_materialize[id_]['url'] or id_to_materialize[id_]['path']:\n",
    "                if id_to_materialize[id_]['url']:\n",
    "                    subprocess.call('wget -O %s %s'%(id_, id_to_materialize[id_]['url']), shell=True)\n",
    "                else:\n",
    "                    shutil.copyfile(id_to_materialize[id_]['path'], id_)\n",
    "                    \n",
    "                if (not os.path.exists(id_)) or os.stat(id_).st_size <= 0:\n",
    "                    print('%s has no valid materialization information for download.' % id_)\n",
    "                    id_to_materialize[id_]['has_info'] = False\n",
    "                    continue\n",
    "                    \n",
    "            else:\n",
    "                print('%s has no materialization information for download.' % id_)\n",
    "            \n",
    "        os.mkdir('joined-datasets')\n",
    "        for i in range(len(results)):\n",
    "            time.sleep(2)\n",
    "            metadata = results[i].get_json_metadata()\n",
    "            id_ = metadata['id']\n",
    "            if not id_to_materialize[id_]['has_info']:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                join_ = results[i].augment(\n",
    "                    supplied_data=supplied_data,\n",
    "                    connection_url='http://localhost:8002/'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "            # excluding d3mIndex\n",
    "            join_['learningData'].drop(['d3mIndex'], axis=1, inplace=True)\n",
    "            \n",
    "            # query and candidate keys\n",
    "            left_column_index = results[i].get_augment_hint().left_columns[0][0].column_index\n",
    "            right_column_index = results[i].get_augment_hint().right_columns[0][0].column_index\n",
    "            query_key = list(supplied_data['learningData'].columns)[left_column_index]\n",
    "            candidate_key = metadata['metadata']['columns'][right_column_index]['name']\n",
    "            \n",
    "            # paths\n",
    "            join_path = 'joined-datasets/%d.csv'%i\n",
    "            candidate_path = '%s_%s'%(id_, candidate_key.replace('%s'%os.path.sep, '_'))\n",
    "            \n",
    "            if not os.path.exists(candidate_path):\n",
    "                companion_data = pd.read_csv(id_)\n",
    "                # collecting candidate key column\n",
    "                candidate_key_column = companion_data[candidate_key]\n",
    "                # excluding categorical / textual attributes\n",
    "                companion_data = companion_data.select_dtypes(exclude=['object'])\n",
    "                if candidate_key not in companion_data.columns:\n",
    "                    companion_data[candidate_key] = candidate_key_column\n",
    "                # excluding columns with all NaN values\n",
    "                companion_data.dropna(axis=1, how='all', inplace=True)\n",
    "                # if the final dataset has only the key, ignore\n",
    "                if len(companion_data.columns) < 2:\n",
    "                    continue\n",
    "                # saving candidate dataset\n",
    "                companion_data.to_csv(candidate_path, index=False)\n",
    "            \n",
    "            # need to load and save again to exclude categorical / textual attributes\n",
    "            join_['learningData'].to_csv(join_path, index=False)\n",
    "            joined_data = pd.read_csv(join_path)\n",
    "            # collecting key column\n",
    "            key_column = joined_data[query_key]\n",
    "            # if key column is not unique, this means that aggregation is necessary\n",
    "            #   so we ignore\n",
    "            if len(set(key_column.tolist())) != len(key_column.tolist()):\n",
    "                os.remove(candidate_path)\n",
    "                os.remove(join_path)\n",
    "                continue\n",
    "            # excluding categorical / textual attributes\n",
    "            joined_data = joined_data.select_dtypes(exclude=['object'])\n",
    "            if query_key not in joined_data.columns:\n",
    "                joined_data[query_key] = key_column\n",
    "            # excluding columns with all NaN values\n",
    "            joined_data.dropna(axis=1, how='all', inplace=True)\n",
    "            # if number of columns in joined dataset is the same as in query data,\n",
    "            #   it means that there was no join (no intersection), and we ignore\n",
    "            if len(supplied_data['learningData'].columns) -1 == len(joined_data.columns):\n",
    "                os.remove(candidate_path)\n",
    "                os.remove(join_path)\n",
    "                continue\n",
    "            joined_data.to_csv(join_path, index=False)\n",
    "            \n",
    "            # scores before augmentation\n",
    "            scores_query = get_performance_scores(\n",
    "                pd.read_csv(supplied_data_path).drop([query_key], axis=1),\n",
    "                target,\n",
    "                True\n",
    "            )\n",
    "            \n",
    "            # scores after augmentation\n",
    "            scores_query_candidate = get_performance_scores(\n",
    "                joined_data.drop([query_key], axis=1),\n",
    "                target,\n",
    "                True\n",
    "            )\n",
    "            \n",
    "            training_records.append(dict(\n",
    "                query_dataset=supplied_data_path,\n",
    "                query_key=query_key,\n",
    "                target=target,\n",
    "                candidate_dataset=os.path.abspath(candidate_path),\n",
    "                candidate_key=candidate_key,\n",
    "                joined_dataset=os.path.abspath('joined-datasets/%d.csv'%i),\n",
    "                imputation_strategy='mean',\n",
    "                mean_absolute_error=[scores_query['mean_absolute_error'],\n",
    "                                     scores_query_candidate['mean_absolute_error']],\n",
    "                mean_squared_error=[scores_query['mean_squared_error'],\n",
    "                                    scores_query_candidate['mean_squared_error']],\n",
    "                median_absolute_error=[scores_query['median_absolute_error'],\n",
    "                                       scores_query_candidate['median_absolute_error']],\n",
    "                r2_score=[scores_query['r2_score'],\n",
    "                          scores_query_candidate['r2_score']]\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        os.chdir(current_working_dir)\n",
    "        \n",
    "    return training_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('companion-datasets'):\n",
    "    os.mkdir('companion-datasets')\n",
    "for p in ['taxi-vehicle-collision', 'ny-taxi-demand', 'college-debt', 'poverty-estimation']:\n",
    "    if not os.path.exists('companion-datasets/%s'%p):\n",
    "        os.mkdir('companion-datasets/%s'%p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = datamart_nyu.NYUDatamart('http://localhost:8002/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Taxi and Vehicle Collision Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/taxi-vehicle-collision/' +\\\n",
    "       'taxi-vehicle-collision-v2.csv'\n",
    "taxi_vehicle_collision = container.Dataset.load('file://' + taxi_vehicle_collision_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=taxi_vehicle_collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    taxi_vehicle_collision_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_results(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_info = get_materialize_info(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datamart.url.0a41288b3f9256e9906062a5fd75169a has no valid materialization information for download.\n",
      "datamart.upload.a031bc4968cb4838967e4709e63a0ddc has no materialization information for download.\n",
      "datamart.upload.83ee6db44a3f434aa0031dc4eb266094 has no materialization information for download.\n",
      "datamart.upload.469f627ada7349f285ad22d3028bc38d has no materialization information for download.\n",
      "datamart.upload.c90cd58ac0c54b169580b49b387cc59e has no materialization information for download.\n",
      "datamart.upload.4095f5182de54d2fb4ceb5bc6268f627 has no materialization information for download.\n",
      "datamart.upload.4eb2156e6a994f33ba71dd59f44c4c59 has no materialization information for download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (17,22,29,32,34,35,36,37,40,41,42,46,51,60,61,62,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (8,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (10,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (11,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (10,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (41,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (14,26,38,39,40,41,43,44,45,47,48,49,50,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0,22,26,32,34,35,51,62,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (16,23,24,25,26,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (13,14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (41,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5,6,10,11,12,13,20,21,26,27,35,37,38,39,40,51,53,57,59,60,61,64,65,67,68,76,82,83,88,94,95,97,98) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (9,12,13,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (1,8,9,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (41,51) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (23,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0,22,26,32,34,35,51,62,64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (23,29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "Error from DataMart: 500 Internal Server Error\n",
      "Error from DataMart: 500 Internal Server Error\n"
     ]
    }
   ],
   "source": [
    "taxi_vehicle_collision_training_records = download_datasets_and_generate_training_records(\n",
    "    taxi_vehicle_collision_results,\n",
    "    taxi_vehicle_collision,\n",
    "    taxi_vehicle_collision_path,\n",
    "    'n. trips',\n",
    "    taxi_vehicle_collision_info,\n",
    "    'companion-datasets/taxi-vehicle-collision/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NY Taxi Demand Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_taxi_demand_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/ny-taxi-demand/' +\\\n",
    "       'yellow-taxi-2017-v2.csv'\n",
    "ny_taxi_demand = container.Dataset.load('file://' + ny_taxi_demand_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## College Debt Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/college-debt/' +\\\n",
    "       'college-debt-v2.csv'\n",
    "college_debt = container.Dataset.load('file://' + college_debt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=college_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    college_debt_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9575737\n",
      "Most- Recent- Cohorts- Scorecard- Elements\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.9575737\n",
      "College Scorecard Data - Most Recent\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.8439322\n",
      "Most- Recent- Cohorts- Scorecard- Elements\n",
      "Left Columns: [[('0', 12)]]\n",
      "Right Columns: [[('0', 83)]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "print_results(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_info = get_materialize_info(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python-3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (131,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "college_debt_training_records = download_datasets_and_generate_training_records(\n",
    "    college_debt_results,\n",
    "    college_debt,\n",
    "    college_debt_path,\n",
    "    'DEBT_EARNINGS_RATIO',\n",
    "    college_debt_info,\n",
    "    'companion-datasets/college-debt/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poverty Estimation Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/poverty-estimation/' +\\\n",
    "       'poverty-estimation-v2.csv'\n",
    "poverty_estimation = container.Dataset.load('file://' + poverty_estimation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=poverty_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    poverty_estimation_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "SF Development Pipeline 2017 Q3\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "1.0\n",
      "SF Development Pipeline 2017 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 31)]]\n",
      "-------------------\n",
      "1.0\n",
      "SF Development Pipeline 2019 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "0.93730605\n",
      "Zillow Median Listing Prices 2017\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 3)]]\n",
      "-------------------\n",
      "0.9362234\n",
      "FIPS Population\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.9362234\n",
      "Unemployment in the US\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.012638724\n",
      "SF Development Pipeline 2016 Q3\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 23)]]\n",
      "-------------------\n",
      "0.0092448\n",
      "SF Development Pipeline 2016 Q4\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "0.007953859\n",
      "SF Development Pipeline 2016 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 22)]]\n",
      "-------------------\n",
      "0.007537427\n",
      "SF Development Pipeline 2014 Q4\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 18)]]\n",
      "-------------------\n",
      "0.0065796324\n",
      "SF Development Pipeline 2017 Q1\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 27)]]\n",
      "-------------------\n",
      "0.001811481\n",
      "State of New York Mortgage Agency (SONYMA) Target Areas by Census Tract\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 1)]]\n",
      "-------------------\n",
      "0.0013117621\n",
      "Reported Facility Emissions (2014)\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 1)]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "print_results(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_info = get_materialize_info(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datamart.upload.a8241c91db1e4d75a4e4dd37cce12cd1 has no materialization information for download.\n",
      "datamart.upload.2f6a998b4f5c4c589aaf990c867446b9 has no materialization information for download.\n"
     ]
    }
   ],
   "source": [
    "poverty_estimation_training_records = download_datasets_and_generate_training_records(\n",
    "    poverty_estimation_results,\n",
    "    poverty_estimation,\n",
    "    poverty_estimation_path,\n",
    "    'POVALL_2016',\n",
    "    poverty_estimation_info,\n",
    "    'companion-datasets/poverty-estimation/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating file with training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('taxi-vehicle-collision-datamart-records/'):\n",
    "    shutil.rmtree('taxi-vehicle-collision-datamart-records/')\n",
    "os.mkdir('taxi-vehicle-collision-datamart-records/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('taxi-vehicle-collision-datamart-records/datamart-records', 'w')\n",
    "for record in taxi_vehicle_collision_training_records:\n",
    "    training_records.write(json.dumps(record) + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('college-debt-datamart-records/'):\n",
    "    shutil.rmtree('college-debt-datamart-records/')\n",
    "os.mkdir('college-debt-datamart-records/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('college-debt-datamart-records/datamart-records', 'w')\n",
    "for record in college_debt_training_records:\n",
    "    training_records.write(json.dumps(record) + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('poverty-estimation-datamart-records/'):\n",
    "    shutil.rmtree('poverty-estimation-datamart-records/')\n",
    "os.mkdir('poverty-estimation-datamart-records/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('poverty-estimation-datamart-records/datamart-records', 'w')\n",
    "for record in poverty_estimation_training_records:\n",
    "    training_records.write(json.dumps(record) + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
