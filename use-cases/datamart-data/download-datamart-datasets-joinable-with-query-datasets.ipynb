{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Version Requirement:** Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3m import container\n",
    "import datamart\n",
    "import datamart_nyu\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAMART_PATH = '/Users/fchirigati/projects/d3m/datamart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    if not results:\n",
    "        return\n",
    "    for result in results:\n",
    "        print(result.score())\n",
    "        print(result.get_json_metadata()['metadata']['name'])\n",
    "        if (result.get_augment_hint()):\n",
    "            left_columns = []\n",
    "            for column_ in result.get_augment_hint().left_columns:\n",
    "                left_columns.append([])\n",
    "                for column in column_:\n",
    "                    left_columns[-1].append((column.resource_id, column.column_index))\n",
    "            print(\"Left Columns: %s\" % str(left_columns))\n",
    "            right_columns = []\n",
    "            for column_ in result.get_augment_hint().right_columns:\n",
    "                right_columns.append([])\n",
    "                for column in column_:\n",
    "                    right_columns[-1].append((column.resource_id, column.column_index))\n",
    "            print(\"Right Columns: %s\" % str(right_columns))\n",
    "        else:\n",
    "            print(result.id())\n",
    "        print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_materialize_info(results):\n",
    "    if not results:\n",
    "        return\n",
    "    id_to_materialize = dict()\n",
    "    for result in results:\n",
    "        id_ = result.get_json_metadata()['id']\n",
    "        if id_ in id_to_materialize:\n",
    "            continue\n",
    "        id_to_materialize[id_] = dict(\n",
    "            has_info=False,\n",
    "            url=None,\n",
    "            path=None\n",
    "        )\n",
    "        if 'direct_url' in result.get_json_metadata()['metadata']['materialize']:\n",
    "            id_to_materialize[id_]['url'] = result.get_json_metadata()['metadata']['materialize']['direct_url']\n",
    "            id_to_materialize[id_]['has_info'] = True\n",
    "        else:\n",
    "            # try to find them on volumes\n",
    "            datamart_file_path = os.path.join(DATAMART_PATH, 'volumes/datasets', id_, 'main.csv')\n",
    "            if os.path.exists(datamart_file_path):\n",
    "                id_to_materialize[id_]['path'] = datamart_file_path\n",
    "                id_to_materialize[id_]['has_info'] = True\n",
    "    return id_to_materialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_datasets_and_generate_training_records(results, supplied_data, supplied_data_path, target,\n",
    "                                                    id_to_materialize, dir_):\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    training_records = list()\n",
    "    \n",
    "    current_working_dir = os.getcwd()\n",
    "    os.chdir(dir_)\n",
    "    try:\n",
    "        # downloading candidate datasets\n",
    "        for id_ in id_to_materialize:\n",
    "            if id_to_materialize[id_]['url'] or id_to_materialize[id_]['path']:\n",
    "                if id_to_materialize[id_]['url']:\n",
    "                    subprocess.call('wget -O %s %s'%(id_, id_to_materialize[id_]['url']), shell=True)\n",
    "                else:\n",
    "                    shutil.copyfile(id_to_materialize[id_]['path'], id_)\n",
    "                \n",
    "                companion_data = pd.read_csv(id_)\n",
    "                # excluding categorical / textual attributes\n",
    "                companion_data = companion_data.select_dtypes(exclude=['object'])\n",
    "                # excluding columns with all NaN values\n",
    "                companion_data.dropna(axis=1, how='all', inplace=True)\n",
    "                companion_data.to_csv(id_, index=False)\n",
    "            else:\n",
    "                print('%s has no materialization information for download.' % id_)\n",
    "            \n",
    "        os.mkdir('joined-datasets')\n",
    "        for i in range(len(results)):\n",
    "            metadata = results[i].get_json_metadata()\n",
    "            id_ = metadata['id']\n",
    "            if not id_to_materialize[id_]['has_info']:\n",
    "                continue\n",
    "            join_ = results[i].augment(\n",
    "                supplied_data=supplied_data,\n",
    "                connection_url='http://localhost:8002/'\n",
    "            )\n",
    "            # excluding d3mIndex\n",
    "            join_['learningData'].drop(['d3mIndex'], axis=1, inplace=True)\n",
    "            join_['learningData'].to_csv('joined-datasets/%d.csv'%i, index=False)\n",
    "            \n",
    "            # need to load and save again to exclude categorical / textual attributes\n",
    "            joined_data = pd.read_csv('joined-datasets/%d.csv'%i)\n",
    "            joined_data = joined_data.select_dtypes(exclude=['object'])\n",
    "            # excluding columns with all NaN values\n",
    "            joined_data.dropna(axis=1, how='all', inplace=True)\n",
    "            joined_data.to_csv('joined-datasets/%d.csv'%i, index=False)\n",
    "            \n",
    "            left_column_index = results[i].get_augment_hint().left_columns[0][0].column_index\n",
    "            right_column_index = results[i].get_augment_hint().right_columns[0][0].column_index\n",
    "            \n",
    "            query_key = list(supplied_data['learningData'].columns)[left_column_index]\n",
    "            candidate_key = metadata['metadata']['columns'][right_column_index]['name']\n",
    "            \n",
    "            training_records.append(dict(\n",
    "                query_dataset=supplied_data_path,\n",
    "                query_key=query_key,\n",
    "                target=target,\n",
    "                candidate_dataset=os.path.abspath('%s'%id_),\n",
    "                candidate_key=candidate_key,\n",
    "                joined_dataset=os.path.abspath('joined-datasets/%d.csv'%i),\n",
    "                imputation_strategy='mean'\n",
    "            ))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        os.chdir(current_working_dir)\n",
    "        \n",
    "    return training_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('companion-datasets'):\n",
    "    os.mkdir('companion-datasets')\n",
    "for p in ['taxi-vehicle-collision', 'ny-taxi-demand', 'college-debt', 'poverty-estimation']:\n",
    "    if not os.path.exists('companion-datasets/%s'%p):\n",
    "        os.mkdir('companion-datasets/%s'%p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = datamart_nyu.NYUDatamart('http://localhost:8002/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Taxi and Vehicle Collision Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/taxi-vehicle-collision/' +\\\n",
    "       'taxi-vehicle-collision-v2.csv'\n",
    "taxi_vehicle_collision = container.Dataset.load('file://' + taxi_vehicle_collision_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=taxi_vehicle_collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    taxi_vehicle_collision_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print_results(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_info = get_materialize_info(taxi_vehicle_collision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_vehicle_collision_training_records = download_datasets_and_generate_training_records(\n",
    "    taxi_vehicle_collision_results,\n",
    "    taxi_vehicle_collision,\n",
    "    taxi_vehicle_collision_path,\n",
    "    'n. trips',\n",
    "    taxi_vehicle_collision_info,\n",
    "    'companion-datasets/taxi-vehicle-collision/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Taxi Demand Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_taxi_demand_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/ny-taxi-demand/' +\\\n",
    "       'yellow-taxi-2017-v2.csv'\n",
    "ny_taxi_demand = container.Dataset.load('file://' + ny_taxi_demand_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## College Debt Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/college-debt/' +\\\n",
    "       'college-debt-v2.csv'\n",
    "college_debt = container.Dataset.load('file://' + college_debt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=college_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    college_debt_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9575737\n",
      "Most- Recent- Cohorts- Scorecard- Elements\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.9575737\n",
      "College Scorecard Data - Most Recent\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.8439322\n",
      "Most- Recent- Cohorts- Scorecard- Elements\n",
      "Left Columns: [[('0', 12)]]\n",
      "Right Columns: [[('0', 83)]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "print_results(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_info = get_materialize_info(college_debt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_training_records = download_datasets_and_generate_training_records(\n",
    "    college_debt_results,\n",
    "    college_debt,\n",
    "    college_debt_path,\n",
    "    'DEBT_EARNINGS_RATIO',\n",
    "    college_debt_info,\n",
    "    'companion-datasets/college-debt/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poverty Estimation Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_path = str(Path.home()) + '/projects/dataset-ranking/use-cases/data/poverty-estimation/' +\\\n",
    "       'poverty-estimation-v2.csv'\n",
    "poverty_estimation = container.Dataset.load('file://' + poverty_estimation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = client.search_with_data(query=None, supplied_data=poverty_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_results = list()\n",
    "results = cursor.get_next_page()\n",
    "while results:\n",
    "    poverty_estimation_results += results\n",
    "    results = cursor.get_next_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "SF Development Pipeline 2017 Q3\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "1.0\n",
      "SF Development Pipeline 2017 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 31)]]\n",
      "-------------------\n",
      "1.0\n",
      "SF Development Pipeline 2019 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "0.93730605\n",
      "Zillow Median Listing Prices 2017\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 3)]]\n",
      "-------------------\n",
      "0.9362234\n",
      "FIPS Population\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.9362234\n",
      "Unemployment in the US\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 0)]]\n",
      "-------------------\n",
      "0.012638724\n",
      "SF Development Pipeline 2016 Q3\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 23)]]\n",
      "-------------------\n",
      "0.0092448\n",
      "SF Development Pipeline 2016 Q4\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 25)]]\n",
      "-------------------\n",
      "0.007953859\n",
      "SF Development Pipeline 2016 Q2\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 22)]]\n",
      "-------------------\n",
      "0.007537427\n",
      "SF Development Pipeline 2014 Q4\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 18)]]\n",
      "-------------------\n",
      "0.0065796324\n",
      "SF Development Pipeline 2017 Q1\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 27)]]\n",
      "-------------------\n",
      "0.001811481\n",
      "State of New York Mortgage Agency (SONYMA) Target Areas by Census Tract\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 1)]]\n",
      "-------------------\n",
      "0.0013117621\n",
      "Reported Facility Emissions (2014)\n",
      "Left Columns: [[('0', 1)]]\n",
      "Right Columns: [[('0', 1)]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "print_results(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_info = get_materialize_info(poverty_estimation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datamart.upload.a8241c91db1e4d75a4e4dd37cce12cd1 has no materialization information for download.\n",
      "datamart.upload.2f6a998b4f5c4c589aaf990c867446b9 has no materialization information for download.\n"
     ]
    }
   ],
   "source": [
    "poverty_estimation_training_records = download_datasets_and_generate_training_records(\n",
    "    poverty_estimation_results,\n",
    "    poverty_estimation,\n",
    "    poverty_estimation_path,\n",
    "    'POVALL_2016',\n",
    "    poverty_estimation_info,\n",
    "    'companion-datasets/poverty-estimation/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating file with training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('datamart-records/'):\n",
    "    shutil.rmtree('datamart-records/')\n",
    "os.mkdir('datamart-records/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = list()\n",
    "# all_records += taxi_vehicle_collision_training_records\n",
    "all_records += college_debt_training_records\n",
    "all_records += poverty_estimation_training_records\n",
    "\n",
    "training_records = open('datamart-records/datamart-records', 'w')\n",
    "for record in all_records:\n",
    "    training_records.write(json.dumps(record) + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
