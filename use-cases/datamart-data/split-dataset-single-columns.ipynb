{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, \\\n",
    "    mean_squared_log_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets Into Single Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(data, target_variable_name):\n",
    "    \"\"\"Builds a model using data to predict the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(target_variable_name, axis=1),\n",
    "        data[target_variable_name],\n",
    "        test_size=0.33,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # normalizing data first\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train.values.reshape(-1, 1))\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train.values.reshape(-1, 1))\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    y_test = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    forest = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=len(data.columns)-1\n",
    "    )\n",
    "    forest.fit(X_train, y_train.ravel())\n",
    "    yfit = forest.predict(X_test)\n",
    "\n",
    "    return dict(\n",
    "        mean_absolute_error=mean_absolute_error(y_test, yfit),\n",
    "        mean_squared_error=mean_squared_error(y_test, yfit),\n",
    "        median_absolute_error=median_absolute_error(y_test, yfit),\n",
    "        r2_score=r2_score(y_test, yfit)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_scores(data, target_variable_name, missing_value_imputation):\n",
    "    \"\"\"Builds a model using data to predict the target variable,\n",
    "    returning different performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if missing_value_imputation:\n",
    "        \n",
    "        # imputation on data\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(data))\n",
    "        new_data.columns = data.columns\n",
    "        new_data.index = data.index\n",
    "\n",
    "        # training and testing model\n",
    "        return train_and_test_model(new_data, target_variable_name)\n",
    "\n",
    "    else:\n",
    "        return train_and_test_model(data, target_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('companion-datasets-single-column'):\n",
    "    os.mkdir('companion-datasets-single-column')\n",
    "for p in ['taxi-vehicle-collision', 'ny-taxi-demand', 'college-debt', 'poverty-estimation']:\n",
    "    if not os.path.exists('companion-datasets-single-column/%s'%p):\n",
    "        os.mkdir('companion-datasets-single-column/%s'%p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_companion_and_join_datasets(path_to_datamart_records, dir_, sample=False):\n",
    "    \n",
    "    records = open(path_to_datamart_records).readlines()\n",
    "    new_records = list()\n",
    "    \n",
    "    if sample:\n",
    "        records = random.sample(records, int(len(records)*0.1))\n",
    "    \n",
    "    for record in records:\n",
    "        test_record = json.loads(record)\n",
    "        \n",
    "        query_dataset = test_record['query_dataset']\n",
    "        query_key = test_record['query_key']\n",
    "        target = test_record['target']\n",
    "        candidate_dataset = test_record['candidate_dataset']\n",
    "        candidate_key = test_record['candidate_key']\n",
    "        joined_dataset = test_record['joined_dataset']\n",
    "        imputation_strategy = test_record['imputation_strategy']\n",
    "        mean_absolute_error = test_record['mean_absolute_error']\n",
    "        mean_squared_error = test_record['mean_squared_error']\n",
    "        median_absolute_error = test_record['median_absolute_error']\n",
    "        r2_score = test_record['r2_score']\n",
    "        \n",
    "        # reading query data\n",
    "        query_data = pd.read_csv(query_dataset)\n",
    "        \n",
    "        # reading candidate data\n",
    "        candidate_data = pd.read_csv(candidate_dataset)\n",
    "        candidate_data = candidate_data.select_dtypes(exclude=['bool'])\n",
    "        \n",
    "        if len(candidate_data.columns) < 2:\n",
    "            continue\n",
    "        \n",
    "        # reading joined dataset\n",
    "        joined_data = pd.read_csv(joined_dataset)\n",
    "        joined_data = joined_data.select_dtypes(exclude=['bool'])\n",
    "        \n",
    "        for column in candidate_data.columns:\n",
    "            if column == candidate_key:\n",
    "                continue\n",
    "                \n",
    "            # new test record\n",
    "            new_record = copy.deepcopy(test_record)\n",
    "            \n",
    "            # creating new candidate dataset\n",
    "            columns_to_drop = set(list(candidate_data.columns)).difference(\n",
    "                set([candidate_key, column])\n",
    "            )\n",
    "            single_column_data = candidate_data.drop(\n",
    "                list(columns_to_drop),\n",
    "                axis=1\n",
    "            )\n",
    "            candidate_path = os.path.join(\n",
    "                'companion-datasets-single-column',\n",
    "                '%s_%s'%(\n",
    "                    os.path.basename(candidate_dataset),\n",
    "                    column.replace('%s'%os.path.sep, '_').strip()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # creating new join dataset\n",
    "            columns_to_drop = set(list(joined_data.columns)).difference(\n",
    "                set(list(query_data.columns))\n",
    "            ).difference(set([column]))\n",
    "            single_column_joined_data = joined_data.drop(\n",
    "                list(columns_to_drop),\n",
    "                axis=1\n",
    "            )\n",
    "            join_path = os.path.join(\n",
    "                dir_,\n",
    "                '%s_%s.csv'%(\n",
    "                    os.path.splitext(os.path.basename(joined_dataset))[0],\n",
    "                    column.replace('%s'%os.path.sep, '_').strip()\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if single_column_joined_data.shape[1] == query_data.shape[1]:\n",
    "                continue  # no join was performed\n",
    "            \n",
    "            # saving datasets\n",
    "            if not os.path.exists(candidate_path):\n",
    "                try:\n",
    "                    single_column_data.to_csv(candidate_path, index=False)\n",
    "                except:\n",
    "                    continue\n",
    "            new_record['candidate_dataset'] = os.path.abspath(candidate_path)\n",
    "            if not os.path.exists(join_path):\n",
    "                try:\n",
    "                    single_column_joined_data.to_csv(join_path, index=False)\n",
    "                except:\n",
    "                    continue\n",
    "            new_record['joined_dataset'] = os.path.abspath(join_path)\n",
    "            \n",
    "#             # scores before augmentation\n",
    "#             scores_query = get_performance_scores(\n",
    "#                 query_data.drop([query_key], axis=1),\n",
    "#                 target,\n",
    "#                 True\n",
    "#             )\n",
    "\n",
    "            # scores after augmentation\n",
    "            scores_query_candidate = get_performance_scores(\n",
    "                single_column_joined_data.drop([query_key], axis=1),\n",
    "                target,\n",
    "                True\n",
    "            )\n",
    "            \n",
    "            new_record['mean_absolute_error'] = [mean_absolute_error[0],\n",
    "                                                 scores_query_candidate['mean_absolute_error']]\n",
    "            new_record['mean_squared_error'] = [mean_squared_error[0],\n",
    "                                               scores_query_candidate['mean_squared_error']]\n",
    "            new_record['median_absolute_error'] = [median_absolute_error[0],\n",
    "                                                   scores_query_candidate['median_absolute_error']]\n",
    "            new_record['r2_score'] = [r2_score[0],\n",
    "                                      scores_query_candidate['r2_score']]\n",
    "            \n",
    "            new_records.append(json.dumps(new_record))\n",
    "            \n",
    "    return new_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NY Taxi and Vehicle Collision Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_records = break_companion_and_join_datasets(\n",
    "    'taxi-vehicle-collision-datamart-records/datamart-records',\n",
    "    'companion-datasets-single-column/taxi-vehicle-collision/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('taxi-vehicle-collision-datamart-records-single-column/'):\n",
    "    shutil.rmtree('taxi-vehicle-collision-datamart-records-single-column/')\n",
    "os.mkdir('taxi-vehicle-collision-datamart-records-single-column/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('taxi-vehicle-collision-datamart-records-single-column/datamart-records', 'w')\n",
    "for record in taxi_records:\n",
    "    training_records.write(record + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## College Debt Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_debt_records = break_companion_and_join_datasets(\n",
    "    'college-debt-datamart-records/datamart-records',\n",
    "    'companion-datasets-single-column/college-debt/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('college-debt-datamart-records-single-column/'):\n",
    "    shutil.rmtree('college-debt-datamart-records-single-column/')\n",
    "os.mkdir('college-debt-datamart-records-single-column/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('college-debt-datamart-records-single-column/datamart-records', 'w')\n",
    "for record in college_debt_records:\n",
    "    training_records.write(record + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poverty Estimation Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty_estimation_records = break_companion_and_join_datasets(\n",
    "    'poverty-estimation-datamart-records/datamart-records',\n",
    "    'companion-datasets-single-column/poverty-estimation/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('poverty-estimation-datamart-records-single-column/'):\n",
    "    shutil.rmtree('poverty-estimation-datamart-records-single-column/')\n",
    "os.mkdir('poverty-estimation-datamart-records-single-column/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records = open('poverty-estimation-datamart-records-single-column/datamart-records', 'w')\n",
    "for record in poverty_estimation_records:\n",
    "    training_records.write(record + \"\\n\")\n",
    "training_records.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
