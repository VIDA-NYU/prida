{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we analyze whether the use of our method helps speed up feature selection as a preprocessing, pruning step. Naturally, it only makes sense to do so if the \"pre-pruned\" results are compatible in quality with what we would get with the pruners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's assume for now that the dataset in question involves using the number of taxi trips to predict the number  of accidents in traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = pd.read_csv('nyc_indicators/taxi_count.csv', sep=SEPARATOR)\n",
    "taxi_trips = taxi_trips.rename(columns={'count': 'taxi_count'})\n",
    "traffic_accidents = pd.read_csv('nyc_indicators/crash_count.csv', sep=SEPARATOR)\n",
    "traffic_accidents = traffic_accidents.rename(columns={'count': 'crash_count'})\n",
    "taxi_crash = pd.merge(taxi_trips, traffic_accidents, on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_datasets(base_dataset, dataset_directory, key, mean_data_imputation=True, rename_numerical=True, separator='|'):\n",
    "    '''\n",
    "    Given (1) a base dataset, (2) a directory with datasets that only have two \n",
    "    columns (one key and one numerical attribute), and (3) a key that is present \n",
    "    in all of them and helps for joining purposes, this function generates a big\n",
    "    table composed of all joined datasets.\n",
    "    '''\n",
    "    \n",
    "    augmented_dataset = base_dataset\n",
    "    dataset_names = [f for f in os.listdir(dataset_directory) if '.csv' in f]\n",
    "    for name in dataset_names:\n",
    "        try:\n",
    "            ### Step 1: read the dataset in the directory\n",
    "            dataset = pd.read_csv(os.path.join(dataset_directory, name), \n",
    "                                  sep=separator)\n",
    "            \n",
    "            ### Step 2 (optional):  rename the numerical column in the dataset\n",
    "            if rename_numerical:\n",
    "                numerical_column = [i for i in dataset.columns if i != key][0]\n",
    "                dataset = dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    \n",
    "            ### Step 3: augment the table\n",
    "            augmented_dataset = pd.merge(augmented_dataset, \n",
    "                                         dataset,\n",
    "                                         how='left',\n",
    "                                         on=key)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "    \n",
    "    augmented_dataset = augmented_dataset.set_index(key)\n",
    "\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_data.index = augmented_dataset.index\n",
    "        new_data.columns = augmented_dataset.columns\n",
    "        return new_data\n",
    "    \n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = join_datasets(taxi_crash, 'nyc_indicators/', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>311_category_Agency_Issues_added_zeros</th>\n",
       "      <th>taxispeed_speed_avg</th>\n",
       "      <th>311_category_SCRIE_added_zeros</th>\n",
       "      <th>cyclist_killed_sum</th>\n",
       "      <th>311_category_electric_added_zeros</th>\n",
       "      <th>311_category_Illegal_parking_added_zeros</th>\n",
       "      <th>311_category_Vacant_Lot_added_zeros</th>\n",
       "      <th>311_category_consumer_complaint_added_zeros</th>\n",
       "      <th>...</th>\n",
       "      <th>311_category_Noise_added_zeros</th>\n",
       "      <th>311_category_Literature_request_added_zeros</th>\n",
       "      <th>311_category_taxi_added_zeros</th>\n",
       "      <th>311_category_collection_added_zeros</th>\n",
       "      <th>311_category_homeless_person_assistance_added_zeros</th>\n",
       "      <th>311_category_Traffic_added_zeros</th>\n",
       "      <th>311_category_Damaged_Tree_added_zeros</th>\n",
       "      <th>motorist_killed_sum</th>\n",
       "      <th>311_category_Enforcement_added_zeros</th>\n",
       "      <th>311_category_graffiti_added_zeros</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-01</th>\n",
       "      <td>354746.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>934.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>61.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-02</th>\n",
       "      <td>412337.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>453.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>88.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-03</th>\n",
       "      <td>495375.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>487.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-04</th>\n",
       "      <td>345717.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>882.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>70.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-05</th>\n",
       "      <td>417036.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>617.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            taxi_count  crash_count  311_category_Agency_Issues_added_zeros  \\\n",
       "time                                                                          \n",
       "2012-07-01    354746.0        443.0                                     0.0   \n",
       "2012-07-02    412337.0        475.0                                     0.0   \n",
       "2012-07-03    495375.0        577.0                                     0.0   \n",
       "2012-07-04    345717.0        353.0                                     0.0   \n",
       "2012-07-05    417036.0        517.0                                     0.0   \n",
       "\n",
       "            taxispeed_speed_avg  311_category_SCRIE_added_zeros  \\\n",
       "time                                                              \n",
       "2012-07-01            17.902601                             1.0   \n",
       "2012-07-02            16.507096                            74.0   \n",
       "2012-07-03            15.954152                            63.0   \n",
       "2012-07-04            17.808810                             4.0   \n",
       "2012-07-05            16.603352                            52.0   \n",
       "\n",
       "            cyclist_killed_sum  311_category_electric_added_zeros  \\\n",
       "time                                                                \n",
       "2012-07-01                 0.0                               89.0   \n",
       "2012-07-02                 0.0                              177.0   \n",
       "2012-07-03                 0.0                              140.0   \n",
       "2012-07-04                 0.0                               61.0   \n",
       "2012-07-05                 0.0                              201.0   \n",
       "\n",
       "            311_category_Illegal_parking_added_zeros  \\\n",
       "time                                                   \n",
       "2012-07-01                                      72.0   \n",
       "2012-07-02                                      93.0   \n",
       "2012-07-03                                     102.0   \n",
       "2012-07-04                                      51.0   \n",
       "2012-07-05                                     108.0   \n",
       "\n",
       "            311_category_Vacant_Lot_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                  5.0   \n",
       "2012-07-02                                 15.0   \n",
       "2012-07-03                                 14.0   \n",
       "2012-07-04                                  6.0   \n",
       "2012-07-05                                 10.0   \n",
       "\n",
       "            311_category_consumer_complaint_added_zeros  ...  \\\n",
       "time                                                     ...   \n",
       "2012-07-01                                         25.0  ...   \n",
       "2012-07-02                                         72.0  ...   \n",
       "2012-07-03                                         57.0  ...   \n",
       "2012-07-04                                         18.0  ...   \n",
       "2012-07-05                                         66.0  ...   \n",
       "\n",
       "            311_category_Noise_added_zeros  \\\n",
       "time                                         \n",
       "2012-07-01                           934.0   \n",
       "2012-07-02                           453.0   \n",
       "2012-07-03                           487.0   \n",
       "2012-07-04                           882.0   \n",
       "2012-07-05                           617.0   \n",
       "\n",
       "            311_category_Literature_request_added_zeros  \\\n",
       "time                                                      \n",
       "2012-07-01                                          9.0   \n",
       "2012-07-02                                        112.0   \n",
       "2012-07-03                                         85.0   \n",
       "2012-07-04                                         10.0   \n",
       "2012-07-05                                         84.0   \n",
       "\n",
       "            311_category_taxi_added_zeros  \\\n",
       "time                                        \n",
       "2012-07-01                           25.0   \n",
       "2012-07-02                           40.0   \n",
       "2012-07-03                           28.0   \n",
       "2012-07-04                           31.0   \n",
       "2012-07-05                           46.0   \n",
       "\n",
       "            311_category_collection_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                 70.0   \n",
       "2012-07-02                                 24.0   \n",
       "2012-07-03                                 48.0   \n",
       "2012-07-04                                 19.0   \n",
       "2012-07-05                                  2.0   \n",
       "\n",
       "            311_category_homeless_person_assistance_added_zeros  \\\n",
       "time                                                              \n",
       "2012-07-01                                          26.341051     \n",
       "2012-07-02                                          26.341051     \n",
       "2012-07-03                                          26.341051     \n",
       "2012-07-04                                          26.341051     \n",
       "2012-07-05                                          26.341051     \n",
       "\n",
       "            311_category_Traffic_added_zeros  \\\n",
       "time                                           \n",
       "2012-07-01                              61.0   \n",
       "2012-07-02                              88.0   \n",
       "2012-07-03                             121.0   \n",
       "2012-07-04                              70.0   \n",
       "2012-07-05                             100.0   \n",
       "\n",
       "            311_category_Damaged_Tree_added_zeros  motorist_killed_sum  \\\n",
       "time                                                                     \n",
       "2012-07-01                                   32.0                  0.0   \n",
       "2012-07-02                                   80.0                  1.0   \n",
       "2012-07-03                                   73.0                  1.0   \n",
       "2012-07-04                                   25.0                  0.0   \n",
       "2012-07-05                                   60.0                  0.0   \n",
       "\n",
       "            311_category_Enforcement_added_zeros  \\\n",
       "time                                               \n",
       "2012-07-01                                  13.0   \n",
       "2012-07-02                                 123.0   \n",
       "2012-07-03                                  58.0   \n",
       "2012-07-04                                   8.0   \n",
       "2012-07-05                                  71.0   \n",
       "\n",
       "            311_category_graffiti_added_zeros  \n",
       "time                                           \n",
       "2012-07-01                                6.0  \n",
       "2012-07-02                              169.0  \n",
       "2012-07-03                               38.0  \n",
       "2012-07-04                                5.0  \n",
       "2012-07-05                               96.0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see which of these features would be classified by our approach as bad for augmentation. To this end, let's build our model over openml-training instances with $\\theta = 1$, svm-rbf, and one candidate per query in the training examples. NOTE THAT THESE TRAINING INSTANCES REFER TO REGRESSION PROBLEMS.\n",
    "\n",
    "### For now we are going to use the following class policy: if the gain in R2-score is predicted as \"above zero\", we consider that the feature should not be pruned. Otherwise it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "FEATURES = ['query_num_of_columns', 'query_num_of_rows', 'query_row_column_ratio',\n",
    "            'query_max_skewness', 'query_max_kurtosis', 'query_max_unique', \n",
    "            'candidate_num_rows', 'candidate_max_skewness', 'candidate_max_kurtosis',\n",
    "            'candidate_max_unique', 'query_target_max_pearson', \n",
    "            'query_target_max_spearman', 'query_target_max_covariance', \n",
    "            'query_target_max_mutual_info', 'candidate_target_max_pearson', \n",
    "            'candidate_target_max_spearman', 'candidate_target_max_covariance', \n",
    "            'candidate_target_max_mutual_info']\n",
    "THETA = 1\n",
    "\n",
    "def train_rbf_svm(features, classes):\n",
    "    '''\n",
    "    Builds a model using features to predict associated classes,\n",
    "    '''\n",
    "\n",
    "    feature_scaler = MinMaxScaler().fit(features)\n",
    "    features_train = feature_scaler.transform(features)\n",
    "    clf = SVC(max_iter=1000, gamma='auto')\n",
    "    clf.fit(features_train, classes)\n",
    "\n",
    "    return feature_scaler, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7566, 36)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml_training = pd.read_csv('../classification/training-simplified-data-generation.csv')\n",
    "openml_training['class_pos_neg'] = ['gain' if row['gain_in_r2_score'] > 0 else 'loss' \n",
    "                                    for index, row in openml_training.iterrows()]\n",
    "openml_training_high_containment = openml_training.loc[openml_training['containment_fraction'] >= THETA]\n",
    "openml_training_high_containment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, for each single-feature dataset in nyc_indicators/, we will join it with the base table, generate all features and classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two lines are important for importing files that are in the parent directory, \n",
    "# necessary to generate the features\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from feature_factory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(query_dataset, \n",
    "                     candidate_dataset, \n",
    "                     key, \n",
    "                     target_name, \n",
    "                     augmented_dataset=pd.DataFrame([]),\n",
    "                     mean_data_imputation=True):\n",
    "    '''\n",
    "    This function generates all the features required to determine, through classification, \n",
    "    whether an augmentation with the candidate_dataset (which is single-feature) is likely to \n",
    "    hamper the model (or simply bring no gain)\n",
    "    '''\n",
    "    \n",
    "    # Step 1: individual query features\n",
    "    feature_factory_query = FeatureFactory(query_dataset.drop([target_name], axis=1))\n",
    "    query_dataset_individual_features = feature_factory_query.get_individual_features(func=max_in_modulus)\n",
    "    ## In order, the returned features are number_of_columns, number_of_rows, row_to_column_ratio,\n",
    "    ## max_mean, max_outlier_percentage, max_skewness, max_kurtosis, max_number_of_unique_values.\n",
    "    ## For now, we're only using number_of_columns, number_of_rows, row_to_column_ratio, \n",
    "    ## max_skewness, max_kurtosis, max_number_of_unique_values, so we remove the unnecessary elements \n",
    "    ## in the lines below\n",
    "    query_dataset_individual_features = [query_dataset_individual_features[index] for index in [0, 1, 2, 5, 6, 7]]\n",
    "    \n",
    "    # Step 2: individual candidate features\n",
    "    feature_factory_candidate = FeatureFactory(candidate_dataset)\n",
    "    candidate_dataset_individual_features = feature_factory_candidate.get_individual_features(func=max_in_modulus)\n",
    "    ## For now, we're only using number_of_rows, max_skewness, max_kurtosis, max_number_of_unique_values, \n",
    "    ## so we remove the unnecessary elements in the lines below \n",
    "    candidate_dataset_individual_features = [candidate_dataset_individual_features[index] for index in [1, 5, 6, 7]]\n",
    "    \n",
    "    # Step 3: join the datasets and compute pairwise features\n",
    "    if augmented_dataset.empty:\n",
    "        augmented_dataset = pd.merge(query_dataset, \n",
    "                                     candidate_dataset,\n",
    "                                     how='left',\n",
    "                                     on=key)\n",
    "    #augmented_dataset = augmented_dataset.set_index(key)\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_dataset.columns = augmented_dataset.columns\n",
    "        new_dataset.index = augmented_dataset.index\n",
    "        augmented_dataset = new_dataset\n",
    "    \n",
    "    # Step 3.1: get query-target features \n",
    "    ## The features are, in order: max_query_target_pearson, max_query_target_spearman, \n",
    "    ## max_query_target_covariance, max_query_target_mutual_info\n",
    "    feature_factory_full_query = FeatureFactory(query_dataset)\n",
    "    query_features_target = feature_factory_full_query.get_pairwise_features_with_target(target_name,\n",
    "                                                                                         func=max_in_modulus)\n",
    "    # Step 3.2: get candidate-target features\n",
    "    ## The features are, in order: max_query_candidate_pearson, max_query_candidate_spearman, \n",
    "    ## max_query_candidate_covariance, max_query_candidate_mutual_info\n",
    "    column_names = candidate_dataset.columns.tolist() + [target_name]\n",
    "    feature_factory_candidate_target = FeatureFactory(augmented_dataset[column_names])\n",
    "    candidate_features_target = feature_factory_candidate_target.get_pairwise_features_with_target(target_name,\n",
    "                                                                                                   func=max_in_modulus)\n",
    "    \n",
    "    # Step 4: get query-candidate feature \"containment ratio\". We may not use it in models, but it's \n",
    "    ## important to have this value in order to filter candidates in baselines, for example.\n",
    "    query_key_values = query_dataset.index.values\n",
    "    candidate_key_values = candidate_dataset.index.values\n",
    "    intersection_size = len(set(query_key_values) & set(candidate_key_values))\n",
    "    containment_ratio = [intersection_size/len(query_key_values)]\n",
    "\n",
    "    return np.array(query_dataset_individual_features + \n",
    "                    candidate_dataset_individual_features + \n",
    "                    query_features_target + \n",
    "                    candidate_features_target + \n",
    "                    containment_ratio)\n",
    "\n",
    "candidate_dataset = pd.read_csv('nyc_indicators/citibike_count.csv', sep=SEPARATOR)\n",
    "candidate_dataset = candidate_dataset.rename(columns={'count':'citibike_count'})\n",
    "features = compute_features(taxi_crash.set_index('time'), \n",
    "                            candidate_dataset.set_index('time'), \n",
    "                            'time',\n",
    "                            'crash_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.46200000e+03 1.46200000e+03 3.86883428e-01\n",
      " 1.47702153e+00 1.45600000e+03 1.63300000e+03 3.02000052e-01\n",
      " 5.28414244e-01 1.60300000e+03 1.60072219e-01 1.24021640e-01\n",
      " 9.50707731e+05 8.68223838e-01 2.62378575e-01 2.87505489e-01\n",
      " 2.27171493e+05 7.52732960e-01 7.47606019e-01]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quickly sanity-check these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query number of rows 1462 query skewness -0.38688342830440225 query kurtosis 1.4770215280107495 candidate number of unique values 1456\n",
      "candidate number of rows 1633 candidate skewness 0.30200005209988534 candidate kurtosis -0.5284142437311292 candidate number of unique values 1603\n",
      "query target pearson (0.160072218826492, 7.506211089340906e-10) query target spearman SpearmanrResult(correlation=0.12402163995232865, pvalue=1.970536577824118e-06) query target covariance 950707.7308413646 query target mutual info 0.8682238377526429\n",
      "candidate target pearson (0.2623785747241163, 1.9218632466061835e-24) candidate target spearman SpearmanrResult(correlation=0.2875054890817096, pvalue=3.2000264303199555e-29) candidate target covariance 227171.49311560777 candidate target mutual info 0.7527329601416918\n"
     ]
    }
   ],
   "source": [
    "print('query number of rows', taxi_crash.shape[0], \n",
    "      'query skewness', taxi_crash['taxi_count'].skew(), \n",
    "      'query kurtosis', taxi_crash['taxi_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(taxi_crash['taxi_count'])))\n",
    "\n",
    "print('candidate number of rows', candidate_dataset.shape[0],\n",
    "      'candidate skewness', candidate_dataset['citibike_count'].skew(), \n",
    "      'candidate kurtosis', candidate_dataset['citibike_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(candidate_dataset['citibike_count'])))\n",
    "\n",
    "augmented_dataset = pd.merge(taxi_crash,\n",
    "                             candidate_dataset,\n",
    "                             how='left',\n",
    "                             on='time').set_index('time')\n",
    "fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "new_dataset.columns = augmented_dataset.columns\n",
    "new_dataset.index = augmented_dataset.index\n",
    "augmented_dataset = new_dataset\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import cov\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "print('query target pearson', pearsonr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target spearman', spearmanr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target covariance', cov(augmented_dataset['taxi_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'query target mutual info', normalized_mutual_info_score(augmented_dataset['taxi_count'], augmented_dataset['crash_count']))\n",
    "\n",
    "print('candidate target pearson', pearsonr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target spearman', spearmanr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target covariance', cov(augmented_dataset['citibike_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'candidate target mutual info', normalized_mutual_info_score(augmented_dataset['citibike_count'], augmented_dataset['crash_count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok! So let's see what label is predicted once we present these features to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss']\n"
     ]
    }
   ],
   "source": [
    "def normalize_features(features, scaler=None):\n",
    "    '''\n",
    "    This function normalizes features using sklearn's StandardScaler\n",
    "    '''\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler().fit(features)\n",
    "    return scaler.transform(features)\n",
    "\n",
    "print(model.predict(normalize_features([features], feature_scaler)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create predictions for all candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_names = [f for f in os.listdir('nyc_indicators/') if '.csv' in f]\n",
    "feature_vectors = []\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        features = compute_features(taxi_crash.set_index('time'), \n",
    "                                    candidate_dataset.set_index('time'), \n",
    "                                    'time',\n",
    "                                    'crash_count')\n",
    "        feature_vectors.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(normalize_features(np.array(feature_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, let's see what gain is brought by each feature individually to an initial prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def  compute_adjusted_r2_score(number_of_regressors, r2_score, number_of_samples):\n",
    "    '''\n",
    "    This function adjusts the value of an r2 score based on its numbers \n",
    "    of regressors and samples.\n",
    "    '''\n",
    "    try:\n",
    "        num = (1.0 - r2_score) * (number_of_samples - 1.0)\n",
    "        den = number_of_samples - number_of_regressors - 1.0\n",
    "        return 1.0 - (num/den)  \n",
    "    except ZeroDivisionError:\n",
    "        print('**** ERROR', number_of_samples, number_of_regressors)\n",
    "        return r2_score\n",
    "    \n",
    "def compute_model_performance_improvement(query_dataset, \n",
    "                                          candidate_dataset, \n",
    "                                          target_name, \n",
    "                                          key, \n",
    "                                          mean_data_imputation=True, \n",
    "                                          adjusted_r2_score=False, \n",
    "                                          model=None):\n",
    "    '''\n",
    "    This function computes the change in (adjusted) R2 score obtained when we try to predict 'target_name' with an \n",
    "    augmented dataset (query_dataset + candidate_dataset)\n",
    "    '''\n",
    "    \n",
    "    # To make sure that we compare apples to apples, let's perform the augmentation and any \n",
    "    # necessary missing data imputation first\n",
    "    augmented_dataset = pd.merge(query_dataset, \n",
    "                                 candidate_dataset,\n",
    "                                 how='left',\n",
    "                                 on=key)\n",
    "    \n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_dataset.columns = augmented_dataset.columns\n",
    "        new_dataset.index = augmented_dataset.index\n",
    "        augmented_dataset = new_dataset\n",
    "    \n",
    "    # Now let's split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop([target_name], axis=1), \n",
    "                                                        augmented_dataset[target_name], \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Computing the initial and final r-squared scores\n",
    "    if not model:\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    model.fit(X_train[query_dataset.drop([target_name], axis=1).columns], y_train.ravel())\n",
    "    y_pred_initial = model.predict(X_test[query_dataset.drop([target_name], axis=1).columns])\n",
    "    r2_score_initial = r2_score(y_test, y_pred_initial)\n",
    "    if adjusted_r2_score:\n",
    "        r2_score_initial = compute_adjusted_r2_score(len(query_dataset.drop([target_name], axis=1).columns),\n",
    "                                                     r2_score_initial, \n",
    "                                                     len(y_test))\n",
    "        \n",
    "    model.fit(X_train[augmented_dataset.drop([target_name], axis=1).columns], y_train.ravel())\n",
    "    y_pred_final = model.predict(X_test[augmented_dataset.drop([target_name], axis=1).columns])\n",
    "    r2_score_final = r2_score(y_test, y_pred_final)\n",
    "    if adjusted_r2_score:\n",
    "        r2_score_final = compute_adjusted_r2_score(len(augmented_dataset.drop([target_name], axis=1).columns),\n",
    "                                                   r2_score_final,\n",
    "                                                   len(y_test))\n",
    "    \n",
    "    performance_difference = (r2_score_final - r2_score_initial)/np.fabs(r2_score_initial)\n",
    "    \n",
    "    return r2_score_initial, r2_score_final, performance_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "gains_in_r2_score = []\n",
    "\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(taxi_crash.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "        \n",
    "        \n",
    "candidates_info = {}\n",
    "for name, pred, feat_vector, gain, true_label in zip(candidate_names, predictions, feature_vectors, gains_in_r2_score, true_labels):\n",
    "    candidates_info[name] = (feat_vector, gain, pred, true_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how well our model works for this example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.99      0.86      0.92        80\n",
      "        loss       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.85        81\n",
      "   macro avg       0.49      0.43      0.46        81\n",
      "weighted avg       0.97      0.85      0.91        81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is too easy in the sense that most candidates do lead to good augmentations. I need to create an example where most candidates do not help. \n",
    "\n",
    "### Let me first see which candidates brought the highest gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cyclist_killed_sum.csv', -0.013631028026993834), ('311_category_Boilers_added_zeros.csv', 0.07997961729100868), ('motorist_killed_sum.csv', 0.08119269554132075), ('persons_killed_sum.csv', 0.14147012684452737), ('311_category_Snow_added_zeros.csv', 0.1472460737106083), ('weather_snow_mean.csv', 0.16316251153613825), ('pedestrians_killed_sum.csv', 0.195407026426819), ('311_category_Animal_in_a_Park_added_zeros.csv', 0.21614843896030195), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.24732513214012192), ('311_category_Smoking_added_zeros.csv', 0.3475493857864856), ('311_category_Vacant_Lot_added_zeros.csv', 0.4318863599771732), ('311_category_Drinking_added_zeros.csv', 0.43968255649418525), ('311_category_Lead_added_zeros.csv', 0.4920252978669224), ('311_category_Street_Sign_added_zeros.csv', 0.5369216015199827), ('cyclist_injured_sum.csv', 0.5549550374398279), ('311_category_Heating_added_zeros.csv', 0.5638690035157186), ('311_category_collection_added_zeros.csv', 0.5661264782155653), ('weather_pluviometry_mean.csv', 0.5950838992563379), ('nypd_count.csv', 0.5991260351199055), ('311_category_Industrial_waste_added_zeros.csv', 0.599126351767138), ('311_category_DHS_Advantage_added_zeros.csv', 0.6109504921675661), ('311_category_Vending_added_zeros.csv', 0.6320437774555394), ('311_category_maintenance_added_zeros.csv', 0.6899335642426384), ('weather_windspeed_mean.csv', 0.7088692286845254), ('311_category_Food_Poisoning_added_zeros.csv', 0.7301406104227237), ('311_category_nonconst_added_zeros.csv', 0.8052009161437185), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.8065169158690201), ('311_category_Hazardous_Materials_added_zeros.csv', 0.8207777471768148), ('311_category_Animal_Abuse_added_zeros.csv', 0.840459330927071), ('311_category_School_Maintenance_added_zeros.csv', 0.8454233615351752), ('citibike_count.csv', 0.8523515896211515), ('311_category_Litter_basket_added_zeros.csv', 0.8541937006530771), ('311_category_homeless_person_assistance_added_zeros.csv', 0.8717705612514153), ('311_category_SPIT_added_zeros.csv', 0.8733940014450833), ('311_category_Food_Establishment_added_zeros.csv', 0.8839546773867323), ('311_category_rodent_added_zeros.csv', 0.9187816767207636), ('311_category_graffiti_added_zeros.csv', 0.9298341832308393), ('311_category_Homeless_Encampment_added_zeros.csv', 0.932207279837008), ('weather_temperature_mean.csv', 0.9499582773535991), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 0.9615561590716805), ('311_category_taxi_added_zeros.csv', 0.9622712365284978), ('311_category_Construction_added_zeros.csv', 0.9646974598449585), ('311_category_Highway_condition_added_zeros.csv', 0.9658468771789152), ('turnstile_count.csv', 0.9911351535388618), ('311_category_sewer_added_zeros.csv', 0.9959965997970991), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 1.0018936252881714), ('311_category_Air_Quality_added_zeros.csv', 1.0195214905589256), ('311_category_Elevator_added_zeros.csv', 1.0220695772034494), ('311_category_Noise_added_zeros.csv', 1.075975682736832), ('311_category_Illegal_parking_added_zeros.csv', 1.0775990545140037), ('311_category_Water_added_zeros.csv', 1.081080563597186), ('motorist_injured_sum.csv', 1.1841390265454164), ('311_category_Damaged_Tree_added_zeros.csv', 1.1918647722024667), ('311_category_derelict_added_zeros.csv', 1.19863582225308), ('311_category_Housing_Options_added_zeros.csv', 1.1988793933726742), ('311_category_Traffic_added_zeros.csv', 1.2118312820420336), ('311_category_Agency_Issues_added_zeros.csv', 1.2189405575371879), ('311_category_Asbestos_added_zeros.csv', 1.236906384516526), ('311_category_broken_meter_added_zeros.csv', 1.2451851684468938), ('311_category_Sidewalk_Condition_added_zeros.csv', 1.250993633300947), ('311_category_Blocked_Driveway_added_zeros.csv', 1.2510445024716812), ('311_category_unsanitary_added_zeros.csv', 1.2964248399396994), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 1.2987499695196114), ('311_category_appliance_added_zeros.csv', 1.3172186110598556), ('311_category_electric_added_zeros.csv', 1.3352904065786741), ('311_category_Enforcement_added_zeros.csv', 1.3801474860508403), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 1.4120142698550413), ('311_category_flooring_stairs_added_zeros.csv', 1.4129537141059982), ('311_category_consumer_complaint_added_zeros.csv', 1.413932820334395), ('311_category_Plumbing_added_zeros.csv', 1.420473410065331), ('311_category_door_window_added_zeros.csv', 1.4607207810337388), ('311_category_Literature_request_added_zeros.csv', 1.463920667746351), ('311_category_Housing_added_zeros.csv', 1.4965957993945693), ('311_category_Paint_added_zeros.csv', 1.5064213337234849), ('pedestrians_injured_sum.csv', 1.5327962014796779), ('311_category_SCRIE_added_zeros.csv', 1.558712778274975), ('taxispeed_speed_avg.csv', 1.6687303454865614), ('311_category_building_added_zeros.csv', 1.6989229020921517), ('persons_injured_sum.csv', 1.7857929489227986), ('311_category_Street_light_condition_added_zeros.csv', 1.8006921112988048), ('311_category_dof_added_zeros.csv', 1.8230098443216247)]\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for name in candidate_names:\n",
    "    tmp.append((name, candidates_info[name][1]))\n",
    "print(sorted(tmp, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few of them seem to bring gains by accident, but let's see what happens if we start with a bigger table including taxispeed_speed_avg.csv and 311_category_Street_light_condition_added_zeros.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints\n",
       "0  2012-07-01      354746          443  17.902601                      37\n",
       "1  2012-07-02      412337          475  16.507096                     410\n",
       "2  2012-07-03      495375          577  15.954152                     300\n",
       "3  2012-07-04      345717          353  17.808810                      31\n",
       "4  2012-07-05      417036          517  16.603352                     290"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxispeed = pd.read_csv('nyc_indicators/taxispeed_speed_avg.csv', sep=SEPARATOR)\n",
    "taxispeed = taxispeed.rename(columns={'mean': 'taxispeed'})\n",
    "streetlight_complaints = pd.read_csv('nyc_indicators/311_category_Street_light_condition_added_zeros.csv', sep=SEPARATOR)\n",
    "streetlight_complaints = streetlight_complaints.rename(columns={'count':'streetlight_complaints'})\n",
    "taxi_crash_taxispeed = pd.merge(taxi_crash, taxispeed, on='time', how='inner')\n",
    "taxi_crash_taxispeed_streetlight = pd.merge(taxi_crash_taxispeed, streetlight_complaints, on='time', how='inner')\n",
    "\n",
    "taxi_crash_taxispeed_streetlight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_candidate_names = list(set(candidate_names) - set(['nyc_indicators/taxispeed_speed_avg.csv', \n",
    "                                         'nyc_indicators/311_category_Street_light_condition_added_zeros.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turnstile_count.csv', -0.04203859621234797), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', -0.038392664194468615), ('311_category_Smoking_added_zeros.csv', -0.032251622626679396), ('311_category_Vending_added_zeros.csv', -0.026410682357405905), ('311_category_Elevator_added_zeros.csv', -0.021587380873384954), ('311_category_Food_Poisoning_added_zeros.csv', -0.020836944765305725), ('311_category_Housing_added_zeros.csv', -0.017828113175924074), ('311_category_Snow_added_zeros.csv', -0.010012004624826565), ('311_category_graffiti_added_zeros.csv', -0.009935261814290716), ('311_category_Housing_Options_added_zeros.csv', -0.007073414864474615), ('311_category_door_window_added_zeros.csv', -0.001869179999049201), ('311_category_Street_light_condition_added_zeros.csv', 0.0004541862035178613), ('motorist_killed_sum.csv', 0.0009208130232868357), ('cyclist_killed_sum.csv', 0.0025005275485507307), ('311_category_building_added_zeros.csv', 0.006593334845260233), ('weather_windspeed_mean.csv', 0.007421391176620442), ('persons_killed_sum.csv', 0.007465257650047305), ('311_category_Asbestos_added_zeros.csv', 0.00793960121428493), ('311_category_Enforcement_added_zeros.csv', 0.008678921808321135), ('311_category_consumer_complaint_added_zeros.csv', 0.012274415894491172), ('taxispeed_speed_avg.csv', 0.012362514410712167), ('311_category_Litter_basket_added_zeros.csv', 0.01359778987950106), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.01464496275250513), ('311_category_unsanitary_added_zeros.csv', 0.016913401560116326), ('311_category_Highway_condition_added_zeros.csv', 0.02109606705634574), ('citibike_count.csv', 0.02182387510687942), ('311_category_Literature_request_added_zeros.csv', 0.02243339143364649), ('311_category_Industrial_waste_added_zeros.csv', 0.023252781049097556), ('311_category_Boilers_added_zeros.csv', 0.023744772723192753), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 0.02498469935085285), ('311_category_appliance_added_zeros.csv', 0.02669463791972731), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 0.02760667188064529), ('311_category_SCRIE_added_zeros.csv', 0.02768409432172275), ('311_category_Animal_Abuse_added_zeros.csv', 0.03011491193272067), ('311_category_flooring_stairs_added_zeros.csv', 0.03365579885375608), ('311_category_Drinking_added_zeros.csv', 0.03511571218764594), ('311_category_Noise_added_zeros.csv', 0.03538602424780189), ('pedestrians_killed_sum.csv', 0.03777745667516245), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.04316219012390527), ('311_category_dof_added_zeros.csv', 0.04546485304060655), ('311_category_sewer_added_zeros.csv', 0.045809683497652395), ('311_category_Animal_in_a_Park_added_zeros.csv', 0.04669015496589134), ('311_category_broken_meter_added_zeros.csv', 0.04959735657491392), ('weather_snow_mean.csv', 0.05034073883661519), ('311_category_taxi_added_zeros.csv', 0.052583442285616705), ('311_category_DHS_Advantage_added_zeros.csv', 0.052695863694794245), ('weather_pluviometry_mean.csv', 0.05442283898362796), ('311_category_SPIT_added_zeros.csv', 0.05588810739597394), ('311_category_homeless_person_assistance_added_zeros.csv', 0.05642144565172061), ('311_category_Lead_added_zeros.csv', 0.06182422200364748), ('311_category_electric_added_zeros.csv', 0.06310784716335481), ('311_category_Food_Establishment_added_zeros.csv', 0.063685660969185), ('311_category_Air_Quality_added_zeros.csv', 0.06717197465263751), ('311_category_Paint_added_zeros.csv', 0.0707988854972309), ('311_category_School_Maintenance_added_zeros.csv', 0.07107965163839249), ('311_category_Street_Sign_added_zeros.csv', 0.07262788787299182), ('311_category_Blocked_Driveway_added_zeros.csv', 0.0726921278410629), ('311_category_nonconst_added_zeros.csv', 0.07446477747916748), ('311_category_Vacant_Lot_added_zeros.csv', 0.07558844695663454), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.08601418465455163), ('nypd_count.csv', 0.08661085724406982), ('311_category_Hazardous_Materials_added_zeros.csv', 0.09283999979553813), ('311_category_derelict_added_zeros.csv', 0.09397457358804051), ('311_category_Plumbing_added_zeros.csv', 0.09705511506702465), ('311_category_Heating_added_zeros.csv', 0.10057327561772338), ('311_category_collection_added_zeros.csv', 0.10103256244306102), ('311_category_Construction_added_zeros.csv', 0.10308428986399404), ('311_category_maintenance_added_zeros.csv', 0.10587122948380777), ('311_category_Water_added_zeros.csv', 0.11149994536219014), ('311_category_Illegal_parking_added_zeros.csv', 0.11896038977220035), ('311_category_Homeless_Encampment_added_zeros.csv', 0.11900133716536598), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.12000633978829857), ('311_category_Agency_Issues_added_zeros.csv', 0.12934216653930272), ('311_category_Traffic_added_zeros.csv', 0.13093605183172538), ('311_category_Damaged_Tree_added_zeros.csv', 0.13866688548707104), ('311_category_rodent_added_zeros.csv', 0.1555682755373543), ('cyclist_injured_sum.csv', 0.16522206505476497), ('weather_temperature_mean.csv', 0.18334116543226417), ('pedestrians_injured_sum.csv', 0.3364552997091622), ('motorist_injured_sum.csv', 0.5098302860747994), ('persons_injured_sum.csv', 0.6213380600652699)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "for name in new_candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(taxi_crash_taxispeed_streetlight.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now there are more candidates that are bringing no gains, but a bunch of them still do. Let's add a few more features to the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_injured = pd.read_csv('nyc_indicators/persons_injured_sum.csv', sep=SEPARATOR)\n",
    "persons_injured = persons_injured.rename(columns={'sum': 'persons_injured'})\n",
    "motorist_injured = pd.read_csv('nyc_indicators/motorist_injured_sum.csv', sep=SEPARATOR)\n",
    "motorist_injured = motorist_injured.rename(columns={'sum':'motorist_injured'})\n",
    "pedestrians_injured = pd.read_csv('nyc_indicators/pedestrians_injured_sum.csv', sep=SEPARATOR)\n",
    "pedestrians_injured = pedestrians_injured.rename(columns={'sum': 'pedestrians_injured'})\n",
    "\n",
    "crash_many_predictors = pd.merge(taxi_crash_taxispeed_streetlight, persons_injured, on='time', how='inner')\n",
    "crash_many_predictors = pd.merge(crash_many_predictors, motorist_injured, on='time', how='inner')\n",
    "crash_many_predictors = pd.merge(crash_many_predictors, pedestrians_injured, on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "      <th>persons_injured</th>\n",
       "      <th>motorist_injured</th>\n",
       "      <th>pedestrians_injured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "      <td>143</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "      <td>138</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "      <td>187</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "      <td>124</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints  \\\n",
       "0  2012-07-01      354746          443  17.902601                      37   \n",
       "1  2012-07-02      412337          475  16.507096                     410   \n",
       "2  2012-07-03      495375          577  15.954152                     300   \n",
       "3  2012-07-04      345717          353  17.808810                      31   \n",
       "4  2012-07-05      417036          517  16.603352                     290   \n",
       "\n",
       "   persons_injured  motorist_injured  pedestrians_injured  \n",
       "0              143               104                   30  \n",
       "1              138                91                   24  \n",
       "2              187               139                   34  \n",
       "3              125               100                   15  \n",
       "4              124                86                   30  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_many_predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_SCRIE_added_zeros.csv', -0.014791401791830274), ('311_category_Elevator_added_zeros.csv', -0.011451887625023294), ('311_category_graffiti_added_zeros.csv', -0.010378240905085438), ('311_category_Drinking_added_zeros.csv', -0.009650693505290275), ('311_category_Housing_Options_added_zeros.csv', -0.009210933861856407), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.009008898241276646), ('311_category_Agency_Issues_added_zeros.csv', -0.006840305893750853), ('311_category_Enforcement_added_zeros.csv', -0.006730938137458545), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.006630506830875576), ('weather_windspeed_mean.csv', -0.00590437224095922), ('311_category_taxi_added_zeros.csv', -0.005281794934152415), ('311_category_School_Maintenance_added_zeros.csv', -0.004932540289080163), ('cyclist_killed_sum.csv', -0.0041670723365609405), ('311_category_Smoking_added_zeros.csv', -0.003993332663934144), ('311_category_Boilers_added_zeros.csv', -0.0038589620177934612), ('311_category_Blocked_Driveway_added_zeros.csv', -0.0036319252255472907), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.0031571402587326747), ('311_category_Food_Establishment_added_zeros.csv', -0.003141459527207121), ('311_category_dof_added_zeros.csv', -0.0024019264621132196), ('311_category_building_added_zeros.csv', -0.001798518771233265), ('311_category_Asbestos_added_zeros.csv', -0.0017957655473006397), ('311_category_appliance_added_zeros.csv', -0.0008778663494616262), ('311_category_DHS_Advantage_added_zeros.csv', -0.0006985230355998947), ('311_category_Highway_condition_added_zeros.csv', -0.0005974348727275575), ('311_category_Housing_added_zeros.csv', -0.00039883120436045856), ('311_category_Water_added_zeros.csv', 0.00021604200708175448), ('311_category_Industrial_waste_added_zeros.csv', 0.0004621703540728183), ('weather_pluviometry_mean.csv', 0.0006030486837413879), ('pedestrians_killed_sum.csv', 0.0006258162654009127), ('311_category_Hazardous_Materials_added_zeros.csv', 0.0006541475057382856), ('311_category_SPIT_added_zeros.csv', 0.0010443652785830733), ('311_category_Damaged_Tree_added_zeros.csv', 0.002192186421106882), ('311_category_consumer_complaint_added_zeros.csv', 0.002471160508107418), ('motorist_killed_sum.csv', 0.0026835306676361507), ('turnstile_count.csv', 0.0028556589642834934), ('311_category_Vending_added_zeros.csv', 0.0028784381088425605), ('persons_killed_sum.csv', 0.0030835822455052876), ('311_category_Lead_added_zeros.csv', 0.0032483980993816153), ('311_category_broken_meter_added_zeros.csv', 0.0033480280366483648), ('311_category_Literature_request_added_zeros.csv', 0.003592970002269886), ('311_category_Air_Quality_added_zeros.csv', 0.0037123114328787353), ('311_category_Illegal_parking_added_zeros.csv', 0.003758432867717673), ('311_category_maintenance_added_zeros.csv', 0.003874908138763393), ('311_category_flooring_stairs_added_zeros.csv', 0.004199266281952063), ('311_category_sewer_added_zeros.csv', 0.004272661485836762), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.004730929338048661), ('cyclist_injured_sum.csv', 0.004816743096191344), ('311_category_Food_Poisoning_added_zeros.csv', 0.00493122527625582), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.004996833029735841), ('311_category_Construction_added_zeros.csv', 0.00515876233805323), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.006065647386353659), ('311_category_unsanitary_added_zeros.csv', 0.006459966505784307), ('311_category_Noise_added_zeros.csv', 0.006549679747360505), ('311_category_Snow_added_zeros.csv', 0.007318413917196507), ('311_category_nonconst_added_zeros.csv', 0.007687480081659194), ('311_category_Litter_basket_added_zeros.csv', 0.007770553525451754), ('311_category_Plumbing_added_zeros.csv', 0.008011444807634467), ('311_category_Paint_added_zeros.csv', 0.008726971667682682), ('311_category_door_window_added_zeros.csv', 0.008809899483951705), ('311_category_homeless_person_assistance_added_zeros.csv', 0.00900074607193958), ('nypd_count.csv', 0.009228980808455335), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.010382793483382908), ('311_category_collection_added_zeros.csv', 0.011854039006899285), ('311_category_Vacant_Lot_added_zeros.csv', 0.012090358207098131), ('311_category_Heating_added_zeros.csv', 0.012837605258711889), ('311_category_Animal_Abuse_added_zeros.csv', 0.013047080978332345), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.014827222354797226), ('weather_snow_mean.csv', 0.014994338714899547), ('311_category_Street_Sign_added_zeros.csv', 0.015311514857828178), ('311_category_Traffic_added_zeros.csv', 0.016984392739993175), ('311_category_rodent_added_zeros.csv', 0.017013324275929623), ('311_category_Homeless_Encampment_added_zeros.csv', 0.01776707408871248), ('weather_temperature_mean.csv', 0.01827550602554089), ('citibike_count.csv', 0.020876005042997744), ('311_category_electric_added_zeros.csv', 0.02413075614310067), ('311_category_derelict_added_zeros.csv', 0.02508041500425141)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "candidate_names = [f for f in os.listdir('nyc_indicators/') if '.csv' in f]\n",
    "\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the gains are considerably lower and much closer to zero, but many are still slightly above zero. What would happen if we used 'gains in adjusted R2-score' instead of 'gains in R2-score'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_SCRIE_added_zeros.csv', -0.0161776045624857), ('311_category_Elevator_added_zeros.csv', -0.012767376762512255), ('311_category_graffiti_added_zeros.csv', -0.011670995759528338), ('311_category_Drinking_added_zeros.csv', -0.01092804266975232), ('311_category_Housing_Options_added_zeros.csv', -0.01047897119215519), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.010272657502357395), ('311_category_Agency_Issues_added_zeros.csv', -0.0080581455880649), ('311_category_Enforcement_added_zeros.csv', -0.007946461988505446), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.00784390406642779), ('weather_windspeed_mean.csv', -0.0071023937025346505), ('311_category_taxi_added_zeros.csv', -0.006466633429256527), ('311_category_School_Maintenance_added_zeros.csv', -0.0061099833776611665), ('cyclist_killed_sum.csv', -0.005328306774035426), ('311_category_Smoking_added_zeros.csv', -0.005150888193979814), ('311_category_Boilers_added_zeros.csv', -0.005013672272716708), ('311_category_Blocked_Driveway_added_zeros.csv', -0.0047818280157795705), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.004296989559655137), ('311_category_Food_Establishment_added_zeros.csv', -0.004280976791359712), ('311_category_dof_added_zeros.csv', -0.003525784241901324), ('311_category_building_added_zeros.csv', -0.0029095994978419882), ('311_category_Asbestos_added_zeros.csv', -0.002906787974870067), ('311_category_appliance_added_zeros.csv', -0.0019694524207475896), ('311_category_DHS_Advantage_added_zeros.csv', -0.0017863115433261838), ('311_category_Highway_condition_added_zeros.csv', -0.001683082856141069), ('311_category_Housing_added_zeros.csv', -0.0014802737895487474), ('311_category_Water_added_zeros.csv', -0.000852380744518115), ('311_category_Industrial_waste_added_zeros.csv', -0.0006010406724762077), ('weather_pluviometry_mean.csv', -0.0004571792686152927), ('pedestrians_killed_sum.csv', -0.00043392958736371924), ('311_category_Hazardous_Materials_added_zeros.csv', -0.0004049984379270327), ('311_category_SPIT_added_zeros.csv', -6.517871434398492e-06), ('311_category_Damaged_Tree_added_zeros.csv', 0.0011656081846663298), ('311_category_consumer_complaint_added_zeros.csv', 0.0014504894995646335), ('motorist_killed_sum.csv', 0.0016673565603809578), ('turnstile_count.csv', 0.001843129643850201), ('311_category_Vending_added_zeros.csv', 0.0018663911328436833), ('persons_killed_sum.csv', 0.0020758791610132003), ('311_category_Lead_added_zeros.csv', 0.0022441849620042833), ('311_category_broken_meter_added_zeros.csv', 0.0023459245459095317), ('311_category_Literature_request_added_zeros.csv', 0.0025960531151623923), ('311_category_Air_Quality_added_zeros.csv', 0.0027179215798619036), ('311_category_Illegal_parking_added_zeros.csv', 0.002765019628082665), ('311_category_maintenance_added_zeros.csv', 0.0028839612427874754), ('311_category_flooring_stairs_added_zeros.csv', 0.003215187613390622), ('311_category_sewer_added_zeros.csv', 0.0032901369479856717), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.0037581085424709835), ('cyclist_injured_sum.csv', 0.003845739392055974), ('311_category_Food_Poisoning_added_zeros.csv', 0.003962645712423701), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.004029642698702623), ('311_category_Construction_added_zeros.csv', 0.004195000832033177), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.0051210890139112486), ('311_category_unsanitary_added_zeros.csv', 0.005523757772292231), ('311_category_Noise_added_zeros.csv', 0.005615370676195818), ('311_category_Snow_added_zeros.csv', 0.0064003826587236004), ('311_category_nonconst_added_zeros.csv', 0.0067772637351956894), ('311_category_Litter_basket_added_zeros.csv', 0.006862096244749576), ('311_category_Plumbing_added_zeros.csv', 0.007108088358044354), ('311_category_Paint_added_zeros.csv', 0.007838766375229572), ('311_category_door_window_added_zeros.csv', 0.007923450173622308), ('311_category_homeless_person_assistance_added_zeros.csv', 0.008118337905004122), ('nypd_count.csv', 0.008351405472473053), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.009529649930636902), ('311_category_collection_added_zeros.csv', 0.011032048822892259), ('311_category_Vacant_Lot_added_zeros.csv', 0.01127337204116032), ('311_category_Heating_added_zeros.csv', 0.012036441919464808), ('311_category_Animal_Abuse_added_zeros.csv', 0.012250353251109852), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.014068188812414993), ('weather_snow_mean.csv', 0.014238843832452367), ('311_category_Street_Sign_added_zeros.csv', 0.014562736125185946), ('311_category_Traffic_added_zeros.csv', 0.016271036906306515), ('311_category_rodent_added_zeros.csv', 0.016300581062497668), ('311_category_Homeless_Encampment_added_zeros.csv', 0.017070291396810912), ('weather_temperature_mean.csv', 0.01758948929170246), ('citibike_count.csv', 0.020245053424737525), ('311_category_electric_added_zeros.csv', 0.023568723314773832), ('311_category_derelict_added_zeros.csv', 0.02453849103756917)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time', \n",
    "                                                                            adjusted_r2_score=True)\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the model used by the user were a linear regression instead of a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_Illegal_parking_added_zeros.csv', -0.03130540327588831), ('311_category_Agency_Issues_added_zeros.csv', -0.03073419118268377), ('311_category_Benefit_Card_Replacement_added_zeros.csv', -0.03046877116163275), ('311_category_derelict_added_zeros.csv', -0.02452031173579836), ('311_category_unsanitary_added_zeros.csv', -0.022454693474382153), ('311_category_Food_Establishment_added_zeros.csv', -0.015877620756254485), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.01332642089887011), ('311_category_Air_Quality_added_zeros.csv', -0.012813742017743296), ('311_category_Smoking_added_zeros.csv', -0.010698977550442764), ('311_category_Blocked_Driveway_added_zeros.csv', -0.010537410309341412), ('turnstile_count.csv', -0.010501254477746812), ('311_category_taxi_added_zeros.csv', -0.007892035639892766), ('311_category_Construction_added_zeros.csv', -0.007677094577480214), ('311_category_Litter_basket_added_zeros.csv', -0.007642806104633979), ('311_category_rodent_added_zeros.csv', -0.006758328296166309), ('311_category_Hazardous_Materials_added_zeros.csv', -0.006298648111865432), ('311_category_graffiti_added_zeros.csv', -0.0056803414199031), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.005071020027254036), ('persons_killed_sum.csv', -0.0034417340882873762), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.0034415249219944236), ('311_category_Elevator_added_zeros.csv', -0.0033545023890863977), ('311_category_Street_Sign_added_zeros.csv', -0.0033170724279678668), ('311_category_Water_added_zeros.csv', -0.0032112828246019976), ('311_category_For_Hire_Vehicle_added_zeros.csv', -0.003076968713147718), ('pedestrians_killed_sum.csv', -0.00298610283627221), ('311_category_Homeless_Encampment_added_zeros.csv', -0.0024465557894288283), ('nypd_count.csv', -0.002106107230052795), ('311_category_building_added_zeros.csv', -0.0016952003826407813), ('motorist_killed_sum.csv', -0.0016907710190441616), ('311_category_Violation_of_Park_Rules_added_zeros.csv', -0.0016172150882183768), ('311_category_DHS_Advantage_added_zeros.csv', -0.001310656105277468), ('311_category_Drinking_added_zeros.csv', -0.0012425122796462789), ('311_category_Industrial_waste_added_zeros.csv', -0.0011539528905676203), ('311_category_Vacant_Lot_added_zeros.csv', -0.0011261072797019056), ('311_category_Enforcement_added_zeros.csv', -0.0010440726982364561), ('311_category_Vending_added_zeros.csv', -0.00099752654592355), ('311_category_Damaged_Tree_added_zeros.csv', -0.0009784121591290603), ('cyclist_killed_sum.csv', -0.0009399737140242625), ('311_category_broken_meter_added_zeros.csv', -0.0007178776067450228), ('311_category_Lead_added_zeros.csv', -0.000712617665547594), ('311_category_Sidewalk_Condition_added_zeros.csv', -0.0002616933065977665), ('citibike_count.csv', -0.0002580066328550412), ('311_category_nonconst_added_zeros.csv', -0.00019862912958005038), ('weather_windspeed_mean.csv', 8.960862845477669e-05), ('311_category_Asbestos_added_zeros.csv', 0.00017501172471854137), ('cyclist_injured_sum.csv', 0.0003792856583222003), ('311_category_consumer_complaint_added_zeros.csv', 0.0006353570879507665), ('311_category_Food_Poisoning_added_zeros.csv', 0.0006999371528479923), ('weather_pluviometry_mean.csv', 0.0007624690978327997), ('weather_temperature_mean.csv', 0.0008923555433203629), ('311_category_Heating_added_zeros.csv', 0.0009716000302208997), ('311_category_homeless_person_assistance_added_zeros.csv', 0.001034205551332079), ('311_category_SPIT_added_zeros.csv', 0.0013346953219257483), ('311_category_collection_added_zeros.csv', 0.0014294793087596973), ('311_category_sewer_added_zeros.csv', 0.002904220056693307), ('311_category_dof_added_zeros.csv', 0.0037344262601937666), ('311_category_Noise_added_zeros.csv', 0.003754136224249021), ('311_category_Housing_Options_added_zeros.csv', 0.004024852777166605), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.004038320864428939), ('311_category_Highway_condition_added_zeros.csv', 0.004331901707041547), ('311_category_Snow_added_zeros.csv', 0.005967984655853863), ('311_category_Boilers_added_zeros.csv', 0.0060951737777616725), ('311_category_SCRIE_added_zeros.csv', 0.00621753936720292), ('311_category_Literature_request_added_zeros.csv', 0.0076465808504478094), ('311_category_Housing_added_zeros.csv', 0.008280097635912467), ('311_category_Animal_Abuse_added_zeros.csv', 0.008874014827122734), ('311_category_appliance_added_zeros.csv', 0.010096924992505297), ('311_category_maintenance_added_zeros.csv', 0.011256166519726236), ('311_category_flooring_stairs_added_zeros.csv', 0.014732461966379607), ('311_category_Traffic_added_zeros.csv', 0.01728249365344201), ('311_category_door_window_added_zeros.csv', 0.020774468219504433), ('311_category_Paint_added_zeros.csv', 0.020948429970024805), ('311_category_Plumbing_added_zeros.csv', 0.024000638352503118), ('weather_snow_mean.csv', 0.028400304186527184), ('311_category_School_Maintenance_added_zeros.csv', 0.029656669683404007), ('311_category_electric_added_zeros.csv', 0.03163833265994381)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names_improvements = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time', \n",
    "                                                                            adjusted_r2_score=True, \n",
    "                                                                            model=linreg)\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. So now 43 out of 76 candidates are reducing the performance. It's not exactly a 'needle in a haystack' scenario when over 40% of the candidates bring gain, but let's see how our model works here. \n",
    "\n",
    "### First of all, we're gonna have to re-train our model to use ADJUSTED R2-SCORES. Note that the instances that compose the openml_training dataset use random forest, not linear regression! If it works well, it works as evidence that our model can adapt well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n"
     ]
    }
   ],
   "source": [
    "def derive_adj_r2_score_from_dataset(dataset):\n",
    "    '''\n",
    "    Given a dataset with instances that can be used to train our model \n",
    "    (or test instances for said model), this function derives adjusted \n",
    "    r2 scores and their corresponding gains for each instance.\n",
    "    '''\n",
    "    adjusted_r2_scores_before = []\n",
    "    adjusted_r2_scores_after = []\n",
    "    gains_in_adjusted_r2_scores = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        number_of_regressors_before = row['query_num_of_columns']\n",
    "        r2_score_before = row['r2_score_before']\n",
    "        \n",
    "        number_of_regressors_after = row['query_num_of_columns'] + row['candidate_num_of_columns']\n",
    "        r2_score_after = row['r2_score_after']\n",
    "\n",
    "        number_of_samples = row['query_num_of_rows'] #after augmentation, this is also the number of samples\n",
    "        \n",
    "        adj_r2_score_before = compute_adjusted_r2_score(number_of_regressors_before,\n",
    "                                                        r2_score_before, \n",
    "                                                        number_of_samples)\n",
    "        \n",
    "        adj_r2_score_after = compute_adjusted_r2_score(number_of_regressors_after, \n",
    "                                                       r2_score_after, \n",
    "                                                       number_of_samples)\n",
    "        \n",
    "        gain_in_adj_r2_score = (adj_r2_score_after - adj_r2_score_before)/np.fabs(adj_r2_score_before)\n",
    "        \n",
    "        adjusted_r2_scores_before.append(adj_r2_score_before)\n",
    "        adjusted_r2_scores_after.append(adj_r2_score_after)\n",
    "        gains_in_adjusted_r2_scores.append(gain_in_adj_r2_score)\n",
    "        \n",
    "    return adjusted_r2_scores_before, adjusted_r2_scores_after, gains_in_adjusted_r2_scores    \n",
    "\n",
    "adj_r2_scores_before, adj_r2_scores_after, gains_in_adj_r2_scores = derive_adj_r2_score_from_dataset(openml_training)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml_training['adj_r2_score_before'] = adj_r2_scores_before\n",
    "openml_training['adj_r2_scores_after'] = adj_r2_scores_after\n",
    "openml_training['gain_in_adj_r2_score'] = gains_in_adj_r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml_training['adj_class_pos_neg'] = ['gain' if row['gain_in_adj_r2_score'] > 0 else 'loss' \n",
    "                                        for index, row in openml_training.iterrows()]\n",
    "openml_training_high_containment = openml_training.loc[openml_training['containment_fraction'] >= THETA]\n",
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['adj_class_pos_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's remember what the current query/initial dataset is and what the candidates are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "      <th>persons_injured</th>\n",
       "      <th>motorist_injured</th>\n",
       "      <th>pedestrians_injured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "      <td>143</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "      <td>138</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "      <td>187</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "      <td>124</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints  \\\n",
       "0  2012-07-01      354746          443  17.902601                      37   \n",
       "1  2012-07-02      412337          475  16.507096                     410   \n",
       "2  2012-07-03      495375          577  15.954152                     300   \n",
       "3  2012-07-04      345717          353  17.808810                      31   \n",
       "4  2012-07-05      417036          517  16.603352                     290   \n",
       "\n",
       "   persons_injured  motorist_injured  pedestrians_injured  \n",
       "0              143               104                   30  \n",
       "1              138                91                   24  \n",
       "2              187               139                   34  \n",
       "3              125               100                   15  \n",
       "4              124                86                   30  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_many_predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal is to see what labels are predicted now with this 'adjusted' model, and finally check how similar they are to the true labels. Note that this is an experiment on EFFECTIVENESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep=SEPARATOR)\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    features = compute_features(crash_many_predictors.set_index('time'), \n",
    "                                candidate_dataset.set_index('time'), \n",
    "                                'time', \n",
    "                                'crash_count')\n",
    "    feature_vectors.append(features[:-1])\n",
    "    \n",
    "predictions = model.predict(normalize_features(np.array(feature_vectors)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'] 76\n"
     ]
    }
   ],
   "source": [
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'] 76\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep=SEPARATOR)\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                        candidate_dataset.set_index('time'), \n",
    "                                                                        'crash_count', \n",
    "                                                                        'time', \n",
    "                                                                        adjusted_r2_score=True, \n",
    "                                                                        model=linreg)\n",
    "    if improvement > 0:\n",
    "        true_labels.append('gain')\n",
    "    else:\n",
    "        true_labels.append('loss')\n",
    "print(np.array(true_labels), len(true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so our method is not pruning anything for this particular example. Let's go back to r2 scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])\n",
    "predictions = model.predict(normalize_features(np.array(feature_vectors))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.60      0.91      0.72        43\n",
      "        loss       0.64      0.21      0.32        33\n",
      "\n",
      "    accuracy                           0.61        76\n",
      "   macro avg       0.62      0.56      0.52        76\n",
      "weighted avg       0.62      0.61      0.55        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep=SEPARATOR)\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                        candidate_dataset.set_index('time'), \n",
    "                                                                        'crash_count', \n",
    "                                                                        'time', \n",
    "                                                                        adjusted_r2_score=False, \n",
    "                                                                        model=linreg)\n",
    "    if improvement > 0:\n",
    "        true_labels.append('gain')\n",
    "    else:\n",
    "        true_labels.append('loss')\n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. Note that, in this case, the minority corresponds to 'loss' -- not to 'gain', what would be ideal. But that's ok, we can fix that later. \n",
    "\n",
    "### Note that the effectiveness is pretty ok for class 'gain' but could be better for class 'loss': the low recall for the latter is to blame... \n",
    "\n",
    "### Before we try to understand EFFICIENCY, and see what kind of gains we would have if we used our model before using ARDA, let's see how a containment baseline would work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "containment_ratios = []\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep=SEPARATOR)\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    output = compute_features(crash_many_predictors.set_index('time'), \n",
    "                              candidate_dataset.set_index('time'), \n",
    "                              'time', \n",
    "                              'crash_count')\n",
    "    feature_vectors.append(output[:-1])\n",
    "    containment_ratios.append((name, output[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the containment ratios in order to figure out some kind of threshold for a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_Agency_Issues_added_zeros.csv', 1.0), ('311_category_SCRIE_added_zeros.csv', 1.0), ('cyclist_killed_sum.csv', 1.0), ('311_category_electric_added_zeros.csv', 1.0), ('311_category_Illegal_parking_added_zeros.csv', 1.0), ('311_category_Vacant_Lot_added_zeros.csv', 1.0), ('311_category_consumer_complaint_added_zeros.csv', 1.0), ('weather_temperature_mean.csv', 1.0), ('311_category_Litter_basket_added_zeros.csv', 1.0), ('311_category_dof_added_zeros.csv', 1.0), ('pedestrians_killed_sum.csv', 1.0), ('311_category_Construction_added_zeros.csv', 0.9397672826830937), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 1.0), ('311_category_Sidewalk_Condition_added_zeros.csv', 1.0), ('persons_killed_sum.csv', 1.0), ('311_category_Food_Establishment_added_zeros.csv', 1.0), ('311_category_Drinking_added_zeros.csv', 1.0), ('311_category_unsanitary_added_zeros.csv', 1.0), ('311_category_rodent_added_zeros.csv', 1.0), ('turnstile_count.csv', 1.0), ('311_category_flooring_stairs_added_zeros.csv', 0.5790554414784395), ('311_category_door_window_added_zeros.csv', 0.5790554414784395), ('311_category_Air_Quality_added_zeros.csv', 1.0), ('cyclist_injured_sum.csv', 1.0), ('311_category_Heating_added_zeros.csv', 1.0), ('311_category_derelict_added_zeros.csv', 1.0), ('311_category_Vending_added_zeros.csv', 1.0), ('weather_snow_mean.csv', 1.0), ('311_category_Plumbing_added_zeros.csv', 1.0), ('311_category_sewer_added_zeros.csv', 1.0), ('311_category_Asbestos_added_zeros.csv', 1.0), ('311_category_Lead_added_zeros.csv', 1.0), ('311_category_building_added_zeros.csv', 1.0), ('citibike_count.csv', 0.7474332648870636), ('311_category_Animal_in_a_Park_added_zeros.csv', 1.0), ('311_category_Street_Sign_added_zeros.csv', 1.0), ('nypd_count.csv', 0.8754277891854894), ('weather_windspeed_mean.csv', 1.0), ('311_category_nonconst_added_zeros.csv', 0.6228610540725531), ('311_category_Animal_Abuse_added_zeros.csv', 0.7097878165639973), ('311_category_maintenance_added_zeros.csv', 1.0), ('311_category_For_Hire_Vehicle_added_zeros.csv', 1.0), ('311_category_Housing_Options_added_zeros.csv', 1.0), ('311_category_Highway_condition_added_zeros.csv', 1.0), ('weather_pluviometry_mean.csv', 1.0), ('311_category_SPIT_added_zeros.csv', 1.0), ('311_category_Paint_added_zeros.csv', 1.0), ('311_category_broken_meter_added_zeros.csv', 1.0), ('311_category_Snow_added_zeros.csv', 1.0), ('311_category_Industrial_waste_added_zeros.csv', 1.0), ('311_category_Boilers_added_zeros.csv', 1.0), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 1.0), ('311_category_Blocked_Driveway_added_zeros.csv', 1.0), ('311_category_Water_added_zeros.csv', 1.0), ('311_category_Elevator_added_zeros.csv', 1.0), ('311_category_Food_Poisoning_added_zeros.csv', 1.0), ('311_category_Homeless_Encampment_added_zeros.csv', 1.0), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 1.0), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 1.0), ('311_category_appliance_added_zeros.csv', 1.0), ('311_category_School_Maintenance_added_zeros.csv', 1.0), ('311_category_DHS_Advantage_added_zeros.csv', 0.5598904859685148), ('311_category_Hazardous_Materials_added_zeros.csv', 1.0), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 1.0), ('311_category_Smoking_added_zeros.csv', 1.0), ('311_category_Housing_added_zeros.csv', 1.0), ('311_category_Noise_added_zeros.csv', 1.0), ('311_category_Literature_request_added_zeros.csv', 1.0), ('311_category_taxi_added_zeros.csv', 1.0), ('311_category_collection_added_zeros.csv', 1.0), ('311_category_homeless_person_assistance_added_zeros.csv', 0.7679671457905544), ('311_category_Traffic_added_zeros.csv', 1.0), ('311_category_Damaged_Tree_added_zeros.csv', 1.0), ('motorist_killed_sum.csv', 1.0), ('311_category_Enforcement_added_zeros.csv', 1.0), ('311_category_graffiti_added_zeros.csv', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(containment_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so most of them are basically equal to one. Let's do the following baseline: if a containment ratio is below \n",
    "### 0.6, let's assume the predicted class will be 'loss'; otherwise, it will be gain. Let's see how this compares to \n",
    "### our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.56      0.95      0.71        43\n",
      "        loss       0.33      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.55        76\n",
      "   macro avg       0.45      0.49      0.38        76\n",
      "weighted avg       0.46      0.55      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containment_based_predictions = ['loss' if elem[1] < 0.6 else 'gain' for elem in containment_ratios]\n",
    "print(classification_report(true_labels, containment_based_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results were significantly worse. This is beneficial for us. Now let's implement RIFS, the feature selection algorithm in ARDA, and start digging into efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsereg.model.base import STRidge # the stype of sparsereg they use is not clear in the ARDA paper\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_PREFIX = 'random_feature_'\n",
    "\n",
    "def augment_with_random_features(dataset, number_of_random_features):\n",
    "    '''\n",
    "    Given a dataset and a number of random features, this function derives\n",
    "    random features that are based on the original ones in the dataset\n",
    "    '''\n",
    "    mean = dataset.T.mean()\n",
    "   # print(dataset.T)\n",
    "    cov = dataset.T.cov()\n",
    "    features = np.random.multivariate_normal(mean, cov, number_of_random_features)\n",
    "    for i in range(number_of_random_features):\n",
    "        dataset[RANDOM_PREFIX + str(i)] = features[i,:]\n",
    "    return dataset\n",
    "\n",
    "def combine_rankings(rf_coefs, regression_coefs, feature_names, lin_comb_coef=0.5):\n",
    "    '''\n",
    "    Given feature coefficients computed with different methods (random forest and a regularized \n",
    "    regression), we scale their values, combine them linearly, rescale them and rank their  \n",
    "    corresponding features according to their values\n",
    "    '''\n",
    "    \n",
    "    # random forest coefficients are already in interval [0, 1]; let's pre-scale the regression \n",
    "    # coefficients, which are all over the place\n",
    "    min_value = min(regression_coefs)\n",
    "    shifted = np.array([elem + min_value*-1 for elem in regression_coefs])\n",
    "    normalized_regression_coefs = shifted/sum(shifted)\n",
    "    \n",
    "    combined_scores = lin_comb_coef*rf_coefs + (1-lin_comb_coef)*normalized_regression_coefs\n",
    "    ranked_features = sorted([(feat_name, score) for feat_name, score in zip(feature_names, combined_scores)], \n",
    "                             key=lambda x: x[1], \n",
    "                             reverse=True)\n",
    "    return [elem[0] for elem in ranked_features]\n",
    "\n",
    "def feature_in_front_of_random(feature_name, ranking):\n",
    "    '''\n",
    "    This function checks whether there are random features before \n",
    "    feature_name\n",
    "    '''\n",
    "    random_not_seen = True\n",
    "    for feat in ranking:\n",
    "        if RANDOM_PREFIX in feat:\n",
    "            return 0\n",
    "        if feat == feature_name:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def aggregate_features_by_quality(rankings):\n",
    "    '''\n",
    "    Given feature rankings, this function counts, for every non-random feature, \n",
    "    how many times it occured before every random feature\n",
    "    '''\n",
    "    feature_names = [elem for elem in rankings[0] if RANDOM_PREFIX not in elem]\n",
    "    \n",
    "    feats_quality = {}\n",
    "    for feature in feature_names:\n",
    "        for rank in rankings:\n",
    "            if feature in feats_quality:\n",
    "                feats_quality[feature] += feature_in_front_of_random(feature, rank)\n",
    "            else:\n",
    "                feats_quality[feature] = feature_in_front_of_random(feature, rank)\n",
    "    sorted_feats =  sorted(feats_quality.items(), \n",
    "                           key=lambda x: x[1], \n",
    "                           reverse=True)\n",
    "    return [(elem[0], elem[1]/len(rankings)) for elem in sorted_feats]\n",
    "    \n",
    "def random_injection_feature_selection(augmented_dataset_features, \n",
    "                                       target_column_data, \n",
    "                                       tau, \n",
    "                                       eta, \n",
    "                                       k_random_seeds):\n",
    "    '''\n",
    "    This is ARDA's feature selection algorithm RIFS. Given an augmented dataset, ideally \n",
    "    created through joins over sketches over the original datasets, a threshold tau for \n",
    "    quality of features, a fraction eta of random features to inject, and k random seeds to perform \n",
    "    k experiments in a reproducible way, it selects the features that should be used in the augmented \n",
    "    dataset\n",
    "    '''\n",
    "    number_of_random_features = int(np.ceil(eta*augmented_dataset_features.shape[1]))\n",
    "    augmented_dataset_with_random = augment_with_random_features(augmented_dataset_features, \n",
    "                                                                 number_of_random_features)\n",
    "    \n",
    "    # Now we obtain rankings using random forests and sparse regression models\n",
    "    ## the paper does not say what hyperparameters were used in the experiment\n",
    "    rankings = []\n",
    "    for seed in range(k_random_seeds):\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "        rf.fit(augmented_dataset_features, target_column_data)\n",
    "        rf_coefs = rf.feature_importances_\n",
    "        \n",
    "        ## coef = STRidge().fit(augmented_dataset_features, target_column_data).coef_\n",
    "        ## print(coef, augmented_dataset_features.columns)\n",
    "        ## print(len(rf.feature_importances_))\n",
    "        \n",
    "        ## This version of lasso is giving lower weights to random features, which is good\n",
    "        lasso = Lasso(random_state=seed)\n",
    "        lasso.fit(augmented_dataset_features, target_column_data)\n",
    "        lasso_coefs = lasso.coef_\n",
    "        rank = combine_rankings(rf_coefs, lasso_coefs, augmented_dataset_features.columns)\n",
    "        rankings.append(rank)\n",
    "    \n",
    "    # Now, for each non-random feature, we get the number of times it appeared in front of \n",
    "    ## all random features\n",
    "    sorted_features = aggregate_features_by_quality(rankings)\n",
    "    return [elem[0] for elem in sorted_features if elem[1] >= tau]\n",
    "    \n",
    "def wrapper_algorithm(augmented_dataset, target_name, key, thresholds_T, eta, k_random_seeds):\n",
    "    '''\n",
    "    This function searches for the best subset of features by doing an exponential search\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(target_name, axis=1), \n",
    "                                                        augmented_dataset[target_name], \n",
    "                                                        test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "    current_r2_score = float('-inf')\n",
    "    linreg = LinearRegression()\n",
    "    selected = []\n",
    "    for tau in thresholds_T:\n",
    "        selected = random_injection_feature_selection(augmented_dataset.drop([target_name], axis=1), \n",
    "                                                      augmented_dataset[target_name],\n",
    "                                                      tau, \n",
    "                                                      eta, \n",
    "                                                      k_random_seeds)\n",
    "        linreg.fit(X_train[selected], y_train)\n",
    "        y_pred = linreg.predict(X_test[selected])\n",
    "        new_r2_score = r2_score(y_test, y_pred)\n",
    "        if  new_r2_score > current_r2_score:\n",
    "            current_r2_score = new_r2_score\n",
    "        else:\n",
    "            break\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so now we have the code for RIFS. What we need to do now is check how it behaves when we use *all* available features, and see how things compare in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_efficiency_with_ida(base_dataset, \n",
    "                              dataset_directory, \n",
    "                              key, \n",
    "                              target_name, \n",
    "                              training_data, \n",
    "                              tau, \n",
    "                              eta, \n",
    "                              k_random_seeds, \n",
    "                              mean_data_imputation=True, \n",
    "                              rename_numerical=True, \n",
    "                              separator='|'):\n",
    "    '''\n",
    "    This function compares the time to run RIFS with and without pre-pruning with IDA\n",
    "    '''\n",
    "    \n",
    "    #Step 1: do the join with every candidate dataset in dataset_directory. \n",
    "    ## This has to be done both with and without IDA. \n",
    "    augmented_dataset = join_datasets(base_dataset, dataset_directory, key, rename_numerical=rename_numerical, separator=separator)\n",
    "    augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()] #removing duplicate columns\n",
    "    print('Done creating the augmented dataset')\n",
    "    \n",
    "    #Step 2: let's see how much time it takes to select features with RIFS, injecting 20% of random features\n",
    "    time1 = time.time()\n",
    "    selected_all = wrapper_algorithm(augmented_dataset, target_name, key, [0.2, 0.4, 0.6, 0.8], 0.2, 10)\n",
    "    time2 = time.time()\n",
    "    print('time to run RIFS', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 3: let's train our IDA model over the training dataset\n",
    "    time1 = time.time()\n",
    "    feature_scaler, model = train_rbf_svm(training_data[FEATURES], \n",
    "                                          training_data['class_pos_neg'])\n",
    "    time2 = time.time()\n",
    "    print('time to train our model', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 4: generate a label for every feature in the augmented dataset\n",
    "    time1 = time.time()\n",
    "    candidate_names = set(augmented_dataset.columns) - set(base_dataset.columns)\n",
    "    feature_vectors = []\n",
    "    for name in candidate_names:\n",
    "        candidate_dataset = augmented_dataset.reset_index()[[key, name]]\n",
    "        #print('getting features for candidate column', name)\n",
    "        #print(candidate_dataset.set_index(key))\n",
    "        features = compute_features(base_dataset.set_index(key), \n",
    "                                    candidate_dataset.set_index(key), \n",
    "                                    key, \n",
    "                                    target_name, \n",
    "                                    augmented_dataset=augmented_dataset)\n",
    "        feature_vectors.append(features[:-1])\n",
    "    predictions = model.predict(normalize_features(np.array(feature_vectors))) \n",
    "\n",
    "    candidates_to_keep = [name for name, pred in zip(candidate_names, predictions) if pred == 'gain']\n",
    "    time2 = time.time()\n",
    "    print('time to predict what candidates to keep', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 5: run RIFS only considering the features to keep   \n",
    "    pruned = augmented_dataset[base_dataset.set_index(key).columns.to_list() + candidates_to_keep]\n",
    "    \n",
    "    time1 = time.time()\n",
    "    selected_pruned = wrapper_algorithm(pruned, \n",
    "                                        target_name, \n",
    "                                        key, \n",
    "                                        [0.2, 0.4, 0.6, 0.8], \n",
    "                                        0.2, \n",
    "                                        10)\n",
    "    time2 = time.time()\n",
    "    print('time to run RIFS over features to keep', (time2-time1)*1000.0, 'ms')\n",
    "    print('size of entire dataset', augmented_dataset.shape[1], 'size of pruned', pruned.shape[1])\n",
    "    return selected_all, candidates_to_keep, selected_pruned\n",
    "\n",
    "#selected_all, candidates_to_keep, selected_pruned = check_efficiency_with_ida(crash_many_predictors, \n",
    "#                                                                            'nyc_indicators/', \n",
    "#                                                                            'time', \n",
    "#                                                                            'crash_count', \n",
    "#                                                                            openml_training_high_containment, \n",
    "#                                                                            0.7, \n",
    "#                                                                            0.2,\n",
    "#                                                                            [42, 17, 23, 2, 5, 19, 37, 41, 13, 33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "augmented_dataset = join_datasets(crash_many_predictors, 'nyc_indicators/', 'time')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(['crash_count'], axis=1), \n",
    "                                                    augmented_dataset['crash_count'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train[selected_all], y_train)\n",
    "y_pred = linreg.predict(X_test[selected_all])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "#print('r2_score for select_all (no pruning)', r2_score_initial)\n",
    "\n",
    "linreg.fit(X_train[selected_pruned], y_train)\n",
    "y_pred = linreg.predict(X_test[selected_pruned])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "#print('r2_score for select_pruned (pruning)', r2_score_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see how the IDA pruning works for a use case involving college columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_college_dataset = pd.read_csv('datasets_for_use_cases/companion-datasets/college-debt-v2.csv')\n",
    "initial_college_dataset = initial_college_dataset.fillna(initial_college_dataset.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>PCIP16</th>\n",
       "      <th>PPTUG_EF</th>\n",
       "      <th>UGDS_WHITE</th>\n",
       "      <th>UGDS_BLACK</th>\n",
       "      <th>UGDS_HISP</th>\n",
       "      <th>UGDS_ASIAN</th>\n",
       "      <th>SATMTMID</th>\n",
       "      <th>SATVRMID</th>\n",
       "      <th>SATWRMID</th>\n",
       "      <th>UGDS</th>\n",
       "      <th>DEBT_EARNINGS_RATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12268508</td>\n",
       "      <td>0.550002</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.187607</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.03452</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207564</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>2164.000000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420024</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164492</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.01230</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>1057.000000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234085</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.04200</td>\n",
       "      <td>575.0000</td>\n",
       "      <td>575.000000</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>1713.000000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UNITID  PCTFLOAN    PCIP16  PPTUG_EF  UGDS_WHITE  UGDS_BLACK  UGDS_HISP  \\\n",
       "0  12268508  0.550002  0.003175  0.232991    0.505208    0.187607   0.164215   \n",
       "1    207564  0.475000  0.000000  0.229700    0.295300    0.029100   0.064700   \n",
       "2    420024  0.812500  0.000000  0.231500    0.280800    0.566500   0.049300   \n",
       "3    164492  0.746500  0.000000  0.262100    0.651800    0.125800   0.102200   \n",
       "4    234085  0.458900  0.032100  0.000000    0.799200    0.060700   0.058400   \n",
       "\n",
       "   UGDS_ASIAN  SATMTMID    SATVRMID    SATWRMID         UGDS  \\\n",
       "0     0.03452  528.1225  521.880734  515.276297  3141.540889   \n",
       "1     0.00510  528.1225  521.880734  515.276297  2164.000000   \n",
       "2     0.00000  528.1225  521.880734  515.276297   203.000000   \n",
       "3     0.01230  528.1225  521.880734  515.276297  1057.000000   \n",
       "4     0.04200  575.0000  575.000000  515.276297  1713.000000   \n",
       "\n",
       "   DEBT_EARNINGS_RATIO  \n",
       "0                   49  \n",
       "1                   36  \n",
       "2                  127  \n",
       "3                   76  \n",
       "4                   53  "
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_college_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "time to run RIFS 1821234.0319156647 ms\n",
      "time to train our model 653.2838344573975 ms\n",
      "time to predict what candidates to keep 163886.07907295227 ms\n",
      "time to run RIFS over features to keep 537127.8910636902 ms\n",
      "size of entire dataset 768 size of pruned 299\n"
     ]
    }
   ],
   "source": [
    "selected_all, candidates_to_keep, selected_pruned = check_efficiency_with_ida(initial_college_dataset, \n",
    "                                                                              'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                                                              'UNITID', \n",
    "                                                                              'DEBT_EARNINGS_RATIO', \n",
    "                                                                              openml_training_high_containment, \n",
    "                                                                              0.7, \n",
    "                                                                              0.2, \n",
    "                                                                              [42, 17, 23, 2, 5, 19, 37, 41, 13, 33], \n",
    "                                                                              rename_numerical=False, \n",
    "                                                                              separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 20\n",
      "['CONTROL_y', 'HBCU_y', 'NPT41_PRIV_x', 'NPT4_PRIV_x', 'OPEID6_y', 'OPEID_x', 'OPEID_y', 'PCIP11_x', 'PCIP12_x', 'PCIP13_x', 'PCIP49_x', 'PCIP52_x', 'PCTFLOAN', 'PCTFLOAN_r_x', 'PCTPELL_x', 'PCTPELL_y', 'PPTUG_EF', 'PPTUG_EF_r_y', 'PREDDEG_y', 'RET_FT4_x', 'RET_FTL4_x', 'SAT_AVG_ALL_x', 'UG25ABV_x', 'UGDS', 'UGDS_ASIAN', 'UGDS_ASIAN_r_y', 'UGDS_BLACK', 'UGDS_BLACK_r_x', 'UGDS_HISP', 'UGDS_HISP_r_y', 'UGDS_NRA_x', 'UGDS_UNKN_x', 'UGDS_WHITE', 'UGDS_WHITE_r_x', 'UGDS_r_y']\n",
      "['CONTROL_y', 'HBCU_y', 'OPEID6_y', 'OPEID_x', 'OPEID_y', 'PCIP47_x', 'PCIP52_x', 'PCTFLOAN', 'PCTFLOAN_r_x', 'PCTPELL_x', 'PCTPELL_y', 'PPTUG_EF', 'PREDDEG_y', 'UGDS', 'UGDS_ASIAN', 'UGDS_BLACK', 'UGDS_BLACK_r_x', 'UGDS_HISP', 'UGDS_HISP_r_y', 'UGDS_WHITE']\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_all), len(selected_pruned))\n",
    "print(sorted(selected_all))\n",
    "print(sorted(selected_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pruning was far more substantial this time. Let's see the quality of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for select_all (no pruning) 0.701455005905175\n",
      "r2_score for select_pruned (pruning) 0.6696787586343466\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset = join_datasets(initial_college_dataset, \n",
    "                                  'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                  'UNITID',\n",
    "                                  rename_numerical=False,\n",
    "                                  separator=',')\n",
    "augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()] #removing duplicate columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(['DEBT_EARNINGS_RATIO'], axis=1), \n",
    "                                                    augmented_dataset['DEBT_EARNINGS_RATIO'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train[selected_all], y_train)\n",
    "y_pred = rf.predict(X_test[selected_all])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_all (no pruning)', r2_score_initial)\n",
    "\n",
    "rf.fit(X_train[selected_pruned], y_train)\n",
    "y_pred = rf.predict(X_test[selected_pruned])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_pruned (pruning)', r2_score_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok. So in this case the quality did drop a bit. Now let's see the concrete quality of this classifier considering the labels for each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_classifier_quality(classifier, \n",
    "                              base_dataset, \n",
    "                              dataset_directory, \n",
    "                              key, \n",
    "                              target_name, \n",
    "                              rename_numerical=True, \n",
    "                              separator='|'):\n",
    "    '''\n",
    "    This function generates true labels and predictions for a set of candidate datasets and \n",
    "    assesses the quality of the classifier\n",
    "    '''\n",
    "    \n",
    "    augmented_dataset = join_datasets(base_dataset, \n",
    "                                      dataset_directory, \n",
    "                                      key, \n",
    "                                      rename_numerical=rename_numerical, \n",
    "                                      separator=separator)\n",
    "    augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()]\n",
    "    \n",
    "    candidate_names = set(augmented_dataset.columns) - set(base_dataset.columns)\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    for name in candidate_names:\n",
    "        candidate_dataset = augmented_dataset.reset_index()[[key, name]]\n",
    "        \n",
    "        features = compute_features(base_dataset.set_index(key), \n",
    "                                    candidate_dataset.set_index(key), \n",
    "                                    key, \n",
    "                                    target_name, \n",
    "                                    augmented_dataset=augmented_dataset)\n",
    "        feature_vectors.append(features[:-1])\n",
    "        \n",
    "        initial, final, improvement = compute_model_performance_improvement(base_dataset.set_index(key), \n",
    "                                                                            candidate_dataset.set_index(key), \n",
    "                                                                            target_name, \n",
    "                                                                            key, \n",
    "                                                                            adjusted_r2_score=False)\n",
    "        if improvement > 0: \n",
    "            labels.append('gain')\n",
    "        else:\n",
    "            labels.append('loss')\n",
    "    predictions = classifier.predict(normalize_features(np.array(feature_vectors))) \n",
    "    print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.23      0.44      0.31       152\n",
      "        loss       0.82      0.64      0.72       603\n",
      "\n",
      "    accuracy                           0.60       755\n",
      "   macro avg       0.53      0.54      0.51       755\n",
      "weighted avg       0.70      0.60      0.63       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])\n",
    "\n",
    "assess_classifier_quality(model, \n",
    "                          initial_college_dataset, \n",
    "                          'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                          'UNITID', \n",
    "                          'DEBT_EARNINGS_RATIO', \n",
    "                              rename_numerical=False, \n",
    "                              separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
