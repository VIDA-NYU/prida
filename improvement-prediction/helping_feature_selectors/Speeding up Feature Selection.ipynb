{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we analyze whether the use of our method helps speed up feature selection as a preprocessing, pruning step. Naturally, it only makes sense to do so if the \"pre-pruned\" results are compatible in quality with what we would get with the pruners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's assume for now that the dataset in question involves using the number of taxi trips to predict the number  of accidents in traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "SEPARATOR = '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = pd.read_csv('nyc_indicators/taxi_count.csv', sep=SEPARATOR)\n",
    "taxi_trips = taxi_trips.rename(columns={'count': 'taxi_count'})\n",
    "traffic_accidents = pd.read_csv('nyc_indicators/crash_count.csv', sep=SEPARATOR)\n",
    "traffic_accidents = traffic_accidents.rename(columns={'count': 'crash_count'})\n",
    "taxi_crash = pd.merge(taxi_trips, traffic_accidents, on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_datasets(base_dataset, dataset_directory, key, mean_data_imputation=True):\n",
    "    '''\n",
    "    Given (1) a base dataset, (2) a directory with datasets that only have two \n",
    "    columns (one key and one numerical attribute), and (3) a key that is present \n",
    "    in all of them and helps for joining purposes, this function generates a big\n",
    "    table composed of all joined datasets.\n",
    "    '''\n",
    "    \n",
    "    augmented_dataset = base_dataset\n",
    "    dataset_names = [f for f in os.listdir(dataset_directory) if '.csv' in f]\n",
    "    for name in dataset_names:\n",
    "        try:\n",
    "            ### Step 1: read the dataset in the directory\n",
    "            dataset = pd.read_csv(os.path.join(dataset_directory, name), \n",
    "                                  sep=SEPARATOR)\n",
    "            \n",
    "            ### Step 2:  rename the numerical column in the dataset\n",
    "            numerical_column = [i for i in dataset.columns if i != key][0]\n",
    "            dataset = dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    \n",
    "            ### Step 3: augment the table\n",
    "            augmented_dataset = pd.merge(augmented_dataset, \n",
    "                                         dataset,\n",
    "                                         how='left',\n",
    "                                         on=key)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "    \n",
    "    augmented_dataset = augmented_dataset.set_index(key)\n",
    "\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_data.columns = augmented_dataset.columns\n",
    "        new_data.index = augmented_dataset.index\n",
    "        return new_data\n",
    "    \n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = join_datasets(taxi_crash, 'nyc_indicators/', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>311_category_Agency_Issues_added_zeros</th>\n",
       "      <th>taxispeed_speed_avg</th>\n",
       "      <th>311_category_SCRIE_added_zeros</th>\n",
       "      <th>cyclist_killed_sum</th>\n",
       "      <th>311_category_electric_added_zeros</th>\n",
       "      <th>311_category_Illegal_parking_added_zeros</th>\n",
       "      <th>311_category_Vacant_Lot_added_zeros</th>\n",
       "      <th>311_category_consumer_complaint_added_zeros</th>\n",
       "      <th>...</th>\n",
       "      <th>311_category_Noise_added_zeros</th>\n",
       "      <th>311_category_Literature_request_added_zeros</th>\n",
       "      <th>311_category_taxi_added_zeros</th>\n",
       "      <th>311_category_collection_added_zeros</th>\n",
       "      <th>311_category_homeless_person_assistance_added_zeros</th>\n",
       "      <th>311_category_Traffic_added_zeros</th>\n",
       "      <th>311_category_Damaged_Tree_added_zeros</th>\n",
       "      <th>motorist_killed_sum</th>\n",
       "      <th>311_category_Enforcement_added_zeros</th>\n",
       "      <th>311_category_graffiti_added_zeros</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-01</th>\n",
       "      <td>354746.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>934.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>61.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-02</th>\n",
       "      <td>412337.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>453.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>88.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-03</th>\n",
       "      <td>495375.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>487.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-04</th>\n",
       "      <td>345717.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>882.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>70.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-05</th>\n",
       "      <td>417036.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>617.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            taxi_count  crash_count  311_category_Agency_Issues_added_zeros  \\\n",
       "time                                                                          \n",
       "2012-07-01    354746.0        443.0                                     0.0   \n",
       "2012-07-02    412337.0        475.0                                     0.0   \n",
       "2012-07-03    495375.0        577.0                                     0.0   \n",
       "2012-07-04    345717.0        353.0                                     0.0   \n",
       "2012-07-05    417036.0        517.0                                     0.0   \n",
       "\n",
       "            taxispeed_speed_avg  311_category_SCRIE_added_zeros  \\\n",
       "time                                                              \n",
       "2012-07-01            17.902601                             1.0   \n",
       "2012-07-02            16.507096                            74.0   \n",
       "2012-07-03            15.954152                            63.0   \n",
       "2012-07-04            17.808810                             4.0   \n",
       "2012-07-05            16.603352                            52.0   \n",
       "\n",
       "            cyclist_killed_sum  311_category_electric_added_zeros  \\\n",
       "time                                                                \n",
       "2012-07-01                 0.0                               89.0   \n",
       "2012-07-02                 0.0                              177.0   \n",
       "2012-07-03                 0.0                              140.0   \n",
       "2012-07-04                 0.0                               61.0   \n",
       "2012-07-05                 0.0                              201.0   \n",
       "\n",
       "            311_category_Illegal_parking_added_zeros  \\\n",
       "time                                                   \n",
       "2012-07-01                                      72.0   \n",
       "2012-07-02                                      93.0   \n",
       "2012-07-03                                     102.0   \n",
       "2012-07-04                                      51.0   \n",
       "2012-07-05                                     108.0   \n",
       "\n",
       "            311_category_Vacant_Lot_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                  5.0   \n",
       "2012-07-02                                 15.0   \n",
       "2012-07-03                                 14.0   \n",
       "2012-07-04                                  6.0   \n",
       "2012-07-05                                 10.0   \n",
       "\n",
       "            311_category_consumer_complaint_added_zeros  ...  \\\n",
       "time                                                     ...   \n",
       "2012-07-01                                         25.0  ...   \n",
       "2012-07-02                                         72.0  ...   \n",
       "2012-07-03                                         57.0  ...   \n",
       "2012-07-04                                         18.0  ...   \n",
       "2012-07-05                                         66.0  ...   \n",
       "\n",
       "            311_category_Noise_added_zeros  \\\n",
       "time                                         \n",
       "2012-07-01                           934.0   \n",
       "2012-07-02                           453.0   \n",
       "2012-07-03                           487.0   \n",
       "2012-07-04                           882.0   \n",
       "2012-07-05                           617.0   \n",
       "\n",
       "            311_category_Literature_request_added_zeros  \\\n",
       "time                                                      \n",
       "2012-07-01                                          9.0   \n",
       "2012-07-02                                        112.0   \n",
       "2012-07-03                                         85.0   \n",
       "2012-07-04                                         10.0   \n",
       "2012-07-05                                         84.0   \n",
       "\n",
       "            311_category_taxi_added_zeros  \\\n",
       "time                                        \n",
       "2012-07-01                           25.0   \n",
       "2012-07-02                           40.0   \n",
       "2012-07-03                           28.0   \n",
       "2012-07-04                           31.0   \n",
       "2012-07-05                           46.0   \n",
       "\n",
       "            311_category_collection_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                 70.0   \n",
       "2012-07-02                                 24.0   \n",
       "2012-07-03                                 48.0   \n",
       "2012-07-04                                 19.0   \n",
       "2012-07-05                                  2.0   \n",
       "\n",
       "            311_category_homeless_person_assistance_added_zeros  \\\n",
       "time                                                              \n",
       "2012-07-01                                          26.341051     \n",
       "2012-07-02                                          26.341051     \n",
       "2012-07-03                                          26.341051     \n",
       "2012-07-04                                          26.341051     \n",
       "2012-07-05                                          26.341051     \n",
       "\n",
       "            311_category_Traffic_added_zeros  \\\n",
       "time                                           \n",
       "2012-07-01                              61.0   \n",
       "2012-07-02                              88.0   \n",
       "2012-07-03                             121.0   \n",
       "2012-07-04                              70.0   \n",
       "2012-07-05                             100.0   \n",
       "\n",
       "            311_category_Damaged_Tree_added_zeros  motorist_killed_sum  \\\n",
       "time                                                                     \n",
       "2012-07-01                                   32.0                  0.0   \n",
       "2012-07-02                                   80.0                  1.0   \n",
       "2012-07-03                                   73.0                  1.0   \n",
       "2012-07-04                                   25.0                  0.0   \n",
       "2012-07-05                                   60.0                  0.0   \n",
       "\n",
       "            311_category_Enforcement_added_zeros  \\\n",
       "time                                               \n",
       "2012-07-01                                  13.0   \n",
       "2012-07-02                                 123.0   \n",
       "2012-07-03                                  58.0   \n",
       "2012-07-04                                   8.0   \n",
       "2012-07-05                                  71.0   \n",
       "\n",
       "            311_category_graffiti_added_zeros  \n",
       "time                                           \n",
       "2012-07-01                                6.0  \n",
       "2012-07-02                              169.0  \n",
       "2012-07-03                               38.0  \n",
       "2012-07-04                                5.0  \n",
       "2012-07-05                               96.0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see which of these features would be classified by our approach as bad for augmentation. To this end, let's build our model over openml-training instances with $\\theta = 1$, svm-rbf, and one candidate per query in the training examples. NOTE THAT THESE TRAINING INSTANCES REFER TO REGRESSION PROBLEMS.\n",
    "\n",
    "### For now we are going to use the following class policy: if the gain in R2-score is predicted as \"above zero\", we consider that the feature should not be pruned. Otherwise it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "FEATURES = ['query_num_of_columns', 'query_num_of_rows', 'query_row_column_ratio',\n",
    "            'query_max_skewness', 'query_max_kurtosis', 'query_max_unique', \n",
    "            'candidate_num_rows', 'candidate_max_skewness', 'candidate_max_kurtosis',\n",
    "            'candidate_max_unique', 'query_target_max_pearson', \n",
    "            'query_target_max_spearman', 'query_target_max_covariance', \n",
    "            'query_target_max_mutual_info', 'candidate_target_max_pearson', \n",
    "            'candidate_target_max_spearman', 'candidate_target_max_covariance', \n",
    "            'candidate_target_max_mutual_info']\n",
    "THETA = 1\n",
    "\n",
    "def train_rbf_svm(features, classes):\n",
    "    '''\n",
    "    Builds a model using features to predict associated classes,\n",
    "    '''\n",
    "\n",
    "    feature_scaler = MinMaxScaler().fit(features)\n",
    "    features_train = feature_scaler.transform(features)\n",
    "    clf = SVC(max_iter=1000, gamma='auto')\n",
    "    clf.fit(features_train, classes)\n",
    "\n",
    "    return feature_scaler, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7566, 36)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml_training = pd.read_csv('../classification/training-simplified-data-generation.csv')\n",
    "openml_training['class_pos_neg'] = ['gain' if row['gain_in_r2_score'] > 0 else 'loss' \n",
    "                                    for index, row in openml_training.iterrows()]\n",
    "openml_training_high_containment = openml_training.loc[openml_training['containment_fraction'] >= THETA]\n",
    "openml_training_high_containment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, for each single-feature dataset in nyc_indicators/, we will join it with the base table, generate all features and classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two lines are important for importing files that are in the parent directory, \n",
    "# necessary to generate the features\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from feature_factory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(query_dataset, candidate_dataset, key, target_name, mean_data_imputation=True):\n",
    "    '''\n",
    "    This function generates all the features required to determine, through classification, \n",
    "    whether an augmentation with the candidate_dataset (which is single-feature) is likely to \n",
    "    hamper the model (or simply bring no gain)\n",
    "    '''\n",
    "    \n",
    "    # Step 1: individual query features\n",
    "    feature_factory_query = FeatureFactory(query_dataset.drop([target_name], axis=1))\n",
    "    query_dataset_individual_features = feature_factory_query.get_individual_features(func=max_in_modulus)\n",
    "    ## In order, the returned features are number_of_columns, number_of_rows, row_to_column_ratio,\n",
    "    ## max_mean, max_outlier_percentage, max_skewness, max_kurtosis, max_number_of_unique_values.\n",
    "    ## For now, we're only using number_of_columns, number_of_rows, row_to_column_ratio, \n",
    "    ## max_skewness, max_kurtosis, max_number_of_unique_values, so we remove the unnecessary elements \n",
    "    ## in the lines below\n",
    "    query_dataset_individual_features = [query_dataset_individual_features[index] for index in [0, 1, 2, 5, 6, 7]]\n",
    "    \n",
    "    # Step 2: individual candidate features\n",
    "    feature_factory_candidate = FeatureFactory(candidate_dataset)\n",
    "    candidate_dataset_individual_features = feature_factory_candidate.get_individual_features(func=max_in_modulus)\n",
    "    ## For now, we're only using number_of_rows, max_skewness, max_kurtosis, max_number_of_unique_values, \n",
    "    ## so we remove the unnecessary elements in the lines below \n",
    "    candidate_dataset_individual_features = [candidate_dataset_individual_features[index] for index in [1, 5, 6, 7]]\n",
    "    \n",
    "    # Step 3: join the datasets and compute pairwise features\n",
    "    augmented_dataset = pd.merge(query_dataset, \n",
    "                                 candidate_dataset,\n",
    "                                 how='left',\n",
    "                                 on=key)\n",
    "    #augmented_dataset = augmented_dataset.set_index(key)\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_dataset.columns = augmented_dataset.columns\n",
    "        new_dataset.index = augmented_dataset.index\n",
    "        augmented_dataset = new_dataset\n",
    "    \n",
    "    # Step 3.1: get query-target features \n",
    "    ## The features are, in order: max_query_target_pearson, max_query_target_spearman, \n",
    "    ## max_query_target_covariance, max_query_target_mutual_info\n",
    "    feature_factory_full_query = FeatureFactory(query_dataset)\n",
    "    query_features_target = feature_factory_full_query.get_pairwise_features_with_target(target_name,\n",
    "                                                                                         func=max_in_modulus)\n",
    "    # Step 3.2: get candidate-target features\n",
    "    ## The features are, in order: max_query_candidate_pearson, max_query_candidate_spearman, \n",
    "    ## max_query_candidate_covariance, max_query_candidate_mutual_info\n",
    "    column_names = candidate_dataset.columns.tolist() + [target_name]\n",
    "    feature_factory_candidate_target = FeatureFactory(augmented_dataset[column_names])\n",
    "    candidate_features_target = feature_factory_candidate_target.get_pairwise_features_with_target(target_name,\n",
    "                                                                                                   func=max_in_modulus)\n",
    "    return np.array(query_dataset_individual_features + \n",
    "                    candidate_dataset_individual_features + \n",
    "                    query_features_target + \n",
    "                    candidate_features_target)\n",
    "\n",
    "candidate_dataset = pd.read_csv('nyc_indicators/citibike_count.csv', sep=SEPARATOR)\n",
    "candidate_dataset = candidate_dataset.rename(columns={'count':'citibike_count'})\n",
    "features = compute_features(taxi_crash.set_index('time'), \n",
    "                            candidate_dataset.set_index('time'), \n",
    "                            'time',\n",
    "                            'crash_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.46200000e+03 1.46200000e+03 3.86883428e-01\n",
      " 1.47702153e+00 1.45600000e+03 1.63300000e+03 3.02000052e-01\n",
      " 5.28414244e-01 1.60300000e+03 1.60072219e-01 1.24021640e-01\n",
      " 9.50707731e+05 8.68223838e-01 2.62378575e-01 2.87505489e-01\n",
      " 2.27171493e+05 7.52732960e-01]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quickly sanity-check these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query number of rows 1462 query skewness -0.38688342830440225 query kurtosis 1.4770215280107495 candidate number of unique values 1456\n",
      "candidate number of rows 1633 candidate skewness 0.30200005209988534 candidate kurtosis -0.5284142437311292 candidate number of unique values 1603\n",
      "query target pearson (0.160072218826492, 7.506211089340906e-10) query target spearman SpearmanrResult(correlation=0.12402163995232865, pvalue=1.970536577824118e-06) query target covariance 950707.7308413646 query target mutual info 0.8682238377526429\n",
      "candidate target pearson (0.2623785747241163, 1.9218632466061835e-24) candidate target spearman SpearmanrResult(correlation=0.2875054890817096, pvalue=3.2000264303199555e-29) candidate target covariance 227171.49311560777 candidate target mutual info 0.7527329601416918\n"
     ]
    }
   ],
   "source": [
    "print('query number of rows', taxi_crash.shape[0], \n",
    "      'query skewness', taxi_crash['taxi_count'].skew(), \n",
    "      'query kurtosis', taxi_crash['taxi_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(taxi_crash['taxi_count'])))\n",
    "\n",
    "print('candidate number of rows', candidate_dataset.shape[0],\n",
    "      'candidate skewness', candidate_dataset['citibike_count'].skew(), \n",
    "      'candidate kurtosis', candidate_dataset['citibike_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(candidate_dataset['citibike_count'])))\n",
    "\n",
    "augmented_dataset = pd.merge(taxi_crash,\n",
    "                             candidate_dataset,\n",
    "                             how='left',\n",
    "                             on='time').set_index('time')\n",
    "fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "new_dataset.columns = augmented_dataset.columns\n",
    "new_dataset.index = augmented_dataset.index\n",
    "augmented_dataset = new_dataset\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import cov\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "print('query target pearson', pearsonr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target spearman', spearmanr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target covariance', cov(augmented_dataset['taxi_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'query target mutual info', normalized_mutual_info_score(augmented_dataset['taxi_count'], augmented_dataset['crash_count']))\n",
    "\n",
    "print('candidate target pearson', pearsonr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target spearman', spearmanr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target covariance', cov(augmented_dataset['citibike_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'candidate target mutual info', normalized_mutual_info_score(augmented_dataset['citibike_count'], augmented_dataset['crash_count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok! So let's see what label is predicted once we present these features to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss']\n"
     ]
    }
   ],
   "source": [
    "def normalize_features(features, scaler=None):\n",
    "    '''\n",
    "    This function normalizes features using sklearn's StandardScaler\n",
    "    '''\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler().fit(features)\n",
    "    return scaler.transform(features)\n",
    "\n",
    "print(model.predict(normalize_features([features], feature_scaler)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create predictions for all candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_label = {}\n",
    "candidate_names = [f for f in os.listdir('nyc_indicators/') if '.csv' in f]\n",
    "feature_vectors = []\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep=SEPARATOR)\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        features = compute_features(taxi_crash.set_index('time'), \n",
    "                                    candidate_dataset.set_index('time'), \n",
    "                                    'time',\n",
    "                                    'crash_count')\n",
    "        feature_vectors.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(normalize_features(np.array(feature_vectors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'311_category_Agency_Issues_added_zeros.csv': 'gain',\n",
       " 'taxispeed_speed_avg.csv': 'gain',\n",
       " '311_category_SCRIE_added_zeros.csv': 'gain',\n",
       " 'cyclist_killed_sum.csv': 'gain',\n",
       " '311_category_electric_added_zeros.csv': 'gain',\n",
       " '311_category_Illegal_parking_added_zeros.csv': 'gain',\n",
       " '311_category_Vacant_Lot_added_zeros.csv': 'gain',\n",
       " '311_category_consumer_complaint_added_zeros.csv': 'gain',\n",
       " 'weather_temperature_mean.csv': 'gain',\n",
       " '311_category_Litter_basket_added_zeros.csv': 'gain',\n",
       " '311_category_dof_added_zeros.csv': 'gain',\n",
       " 'pedestrians_killed_sum.csv': 'loss',\n",
       " '311_category_Construction_added_zeros.csv': 'loss',\n",
       " '311_category_DOH_New_License_Application_Request_added_zeros.csv': 'gain',\n",
       " '311_category_Sidewalk_Condition_added_zeros.csv': 'gain',\n",
       " 'persons_killed_sum.csv': 'loss',\n",
       " '311_category_Food_Establishment_added_zeros.csv': 'gain',\n",
       " '311_category_Drinking_added_zeros.csv': 'loss',\n",
       " '311_category_unsanitary_added_zeros.csv': 'gain',\n",
       " '311_category_rodent_added_zeros.csv': 'gain',\n",
       " 'turnstile_count.csv': 'gain',\n",
       " '311_category_flooring_stairs_added_zeros.csv': 'gain',\n",
       " '311_category_door_window_added_zeros.csv': 'gain',\n",
       " '311_category_Air_Quality_added_zeros.csv': 'gain',\n",
       " 'cyclist_injured_sum.csv': 'gain',\n",
       " '311_category_Heating_added_zeros.csv': 'gain',\n",
       " '311_category_derelict_added_zeros.csv': 'gain',\n",
       " '311_category_Vending_added_zeros.csv': 'gain',\n",
       " 'weather_snow_mean.csv': 'gain',\n",
       " '311_category_Plumbing_added_zeros.csv': 'gain',\n",
       " '311_category_sewer_added_zeros.csv': 'gain',\n",
       " '311_category_Asbestos_added_zeros.csv': 'gain',\n",
       " '311_category_Lead_added_zeros.csv': 'gain',\n",
       " '311_category_building_added_zeros.csv': 'gain',\n",
       " 'citibike_count.csv': 'gain',\n",
       " '311_category_Animal_in_a_Park_added_zeros.csv': 'loss',\n",
       " 'persons_injured_sum.csv': 'gain',\n",
       " '311_category_Street_Sign_added_zeros.csv': 'gain',\n",
       " 'nypd_count.csv': 'gain',\n",
       " 'weather_windspeed_mean.csv': 'gain',\n",
       " 'pedestrians_injured_sum.csv': 'gain',\n",
       " '311_category_nonconst_added_zeros.csv': 'loss',\n",
       " '311_category_Animal_Abuse_added_zeros.csv': 'loss',\n",
       " '311_category_maintenance_added_zeros.csv': 'gain',\n",
       " '311_category_For_Hire_Vehicle_added_zeros.csv': 'gain',\n",
       " '311_category_Housing_Options_added_zeros.csv': 'gain',\n",
       " '311_category_Highway_condition_added_zeros.csv': 'gain',\n",
       " 'weather_pluviometry_mean.csv': 'gain',\n",
       " '311_category_SPIT_added_zeros.csv': 'gain',\n",
       " '311_category_Paint_added_zeros.csv': 'gain',\n",
       " '311_category_broken_meter_added_zeros.csv': 'gain',\n",
       " '311_category_Snow_added_zeros.csv': 'gain',\n",
       " '311_category_Industrial_waste_added_zeros.csv': 'gain',\n",
       " '311_category_Boilers_added_zeros.csv': 'loss',\n",
       " '311_category_Fire_Safety_Director_-_F58_added_zeros.csv': 'gain',\n",
       " '311_category_Blocked_Driveway_added_zeros.csv': 'gain',\n",
       " '311_category_Water_added_zeros.csv': 'gain',\n",
       " '311_category_Elevator_added_zeros.csv': 'gain',\n",
       " '311_category_Food_Poisoning_added_zeros.csv': 'gain',\n",
       " '311_category_Homeless_Encampment_added_zeros.csv': 'gain',\n",
       " '311_category_Street_light_condition_added_zeros.csv': 'gain',\n",
       " '311_category_Non-Emergency_Police_Matter_added_zeros.csv': 'gain',\n",
       " '311_category_Violation_of_Park_Rules_added_zeros.csv': 'loss',\n",
       " '311_category_appliance_added_zeros.csv': 'gain',\n",
       " '311_category_School_Maintenance_added_zeros.csv': 'gain',\n",
       " '311_category_DHS_Advantage_added_zeros.csv': 'gain',\n",
       " 'motorist_injured_sum.csv': 'gain',\n",
       " '311_category_Hazardous_Materials_added_zeros.csv': 'gain',\n",
       " '311_category_Benefit_Card_Replacement_added_zeros.csv': 'gain',\n",
       " '311_category_Smoking_added_zeros.csv': 'gain',\n",
       " '311_category_Housing_added_zeros.csv': 'gain',\n",
       " '311_category_Noise_added_zeros.csv': 'gain',\n",
       " '311_category_Literature_request_added_zeros.csv': 'gain',\n",
       " '311_category_taxi_added_zeros.csv': 'gain',\n",
       " '311_category_collection_added_zeros.csv': 'gain',\n",
       " '311_category_homeless_person_assistance_added_zeros.csv': 'loss',\n",
       " '311_category_Traffic_added_zeros.csv': 'gain',\n",
       " '311_category_Damaged_Tree_added_zeros.csv': 'gain',\n",
       " 'motorist_killed_sum.csv': 'loss',\n",
       " '311_category_Enforcement_added_zeros.csv': 'gain',\n",
       " '311_category_graffiti_added_zeros.csv': 'gain'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, pred in zip(candidate_names, predictions):\n",
    "    candidate_label[name] = pred\n",
    "candidate_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
