{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we analyze whether the use of our method helps speed up feature selection as a preprocessing, pruning step. Naturally, it only makes sense to do so if the \"pre-pruned\" results are compatible in quality with what we would get with the pruners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's assume for now that the dataset in question involves using the number of taxi trips to predict the number  of accidents in traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "SEPARATOR = '|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_trips = pd.read_csv('taxi_count.csv', sep=SEPARATOR)\n",
    "taxi_trips = taxi_trips.rename(columns={'count': 'taxi_count'})\n",
    "traffic_accidents = pd.read_csv('crash_count.csv', sep=SEPARATOR)\n",
    "traffic_accidents = traffic_accidents.rename(columns={'count': 'crash_count'})\n",
    "taxi_crash = pd.merge(taxi_trips, traffic_accidents, on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_datasets(base_dataset, dataset_directory, key, mean_data_imputation=True, rename_numerical=True, separator='|'):\n",
    "    '''\n",
    "    Given (1) a base dataset, (2) a directory with datasets that only have two \n",
    "    columns (one key and one numerical attribute), and (3) a key that is present \n",
    "    in all of them and helps for joining purposes, this function generates a big\n",
    "    table composed of all joined datasets.\n",
    "    '''\n",
    "    \n",
    "    augmented_dataset = base_dataset\n",
    "    dataset_names = [f for f in os.listdir(dataset_directory) if '.csv' in f]\n",
    "    for name in dataset_names:\n",
    "        try:\n",
    "            ### Step 1: read the dataset in the directory\n",
    "            dataset = pd.read_csv(os.path.join(dataset_directory, name), \n",
    "                                  sep=separator)\n",
    "            \n",
    "            ### Step 2 (optional):  rename the numerical column in the dataset\n",
    "            if rename_numerical:\n",
    "                numerical_column = [i for i in dataset.columns if i != key][0]\n",
    "                dataset = dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    \n",
    "            ### Step 3: augment the table\n",
    "            #print('NAME', name)\n",
    "            augmented_dataset = pd.merge(augmented_dataset, \n",
    "                                         dataset,\n",
    "                                         how='left',\n",
    "                                         on=key)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "    \n",
    "    augmented_dataset = augmented_dataset.set_index(key)\n",
    "    augmented_dataset = augmented_dataset.select_dtypes(include=['int64', 'float64'])\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_data.index = augmented_dataset.index\n",
    "        new_data.columns = augmented_dataset.columns\n",
    "        return new_data\n",
    "    \n",
    "    return augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = join_datasets(taxi_crash, 'nyc_indicators/', 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>311_category_Agency_Issues_added_zeros</th>\n",
       "      <th>311_category_SCRIE_added_zeros</th>\n",
       "      <th>cyclist_killed_sum</th>\n",
       "      <th>311_category_electric_added_zeros</th>\n",
       "      <th>311_category_Illegal_parking_added_zeros</th>\n",
       "      <th>311_category_Vacant_Lot_added_zeros</th>\n",
       "      <th>311_category_consumer_complaint_added_zeros</th>\n",
       "      <th>weather_temperature_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>311_category_Noise_added_zeros</th>\n",
       "      <th>311_category_Literature_request_added_zeros</th>\n",
       "      <th>311_category_taxi_added_zeros</th>\n",
       "      <th>311_category_collection_added_zeros</th>\n",
       "      <th>311_category_homeless_person_assistance_added_zeros</th>\n",
       "      <th>311_category_Traffic_added_zeros</th>\n",
       "      <th>311_category_Damaged_Tree_added_zeros</th>\n",
       "      <th>motorist_killed_sum</th>\n",
       "      <th>311_category_Enforcement_added_zeros</th>\n",
       "      <th>311_category_graffiti_added_zeros</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07-01</th>\n",
       "      <td>354746.0</td>\n",
       "      <td>443.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.923529</td>\n",
       "      <td>...</td>\n",
       "      <td>934.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>61.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-02</th>\n",
       "      <td>412337.0</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>27.681250</td>\n",
       "      <td>...</td>\n",
       "      <td>453.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>88.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-03</th>\n",
       "      <td>495375.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>26.240625</td>\n",
       "      <td>...</td>\n",
       "      <td>487.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>121.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-04</th>\n",
       "      <td>345717.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>26.196969</td>\n",
       "      <td>...</td>\n",
       "      <td>882.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>70.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-07-05</th>\n",
       "      <td>417036.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.859375</td>\n",
       "      <td>...</td>\n",
       "      <td>617.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.341051</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            taxi_count  crash_count  311_category_Agency_Issues_added_zeros  \\\n",
       "time                                                                          \n",
       "2012-07-01    354746.0        443.0                                     0.0   \n",
       "2012-07-02    412337.0        475.0                                     0.0   \n",
       "2012-07-03    495375.0        577.0                                     0.0   \n",
       "2012-07-04    345717.0        353.0                                     0.0   \n",
       "2012-07-05    417036.0        517.0                                     0.0   \n",
       "\n",
       "            311_category_SCRIE_added_zeros  cyclist_killed_sum  \\\n",
       "time                                                             \n",
       "2012-07-01                             1.0                 0.0   \n",
       "2012-07-02                            74.0                 0.0   \n",
       "2012-07-03                            63.0                 0.0   \n",
       "2012-07-04                             4.0                 0.0   \n",
       "2012-07-05                            52.0                 0.0   \n",
       "\n",
       "            311_category_electric_added_zeros  \\\n",
       "time                                            \n",
       "2012-07-01                               89.0   \n",
       "2012-07-02                              177.0   \n",
       "2012-07-03                              140.0   \n",
       "2012-07-04                               61.0   \n",
       "2012-07-05                              201.0   \n",
       "\n",
       "            311_category_Illegal_parking_added_zeros  \\\n",
       "time                                                   \n",
       "2012-07-01                                      72.0   \n",
       "2012-07-02                                      93.0   \n",
       "2012-07-03                                     102.0   \n",
       "2012-07-04                                      51.0   \n",
       "2012-07-05                                     108.0   \n",
       "\n",
       "            311_category_Vacant_Lot_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                  5.0   \n",
       "2012-07-02                                 15.0   \n",
       "2012-07-03                                 14.0   \n",
       "2012-07-04                                  6.0   \n",
       "2012-07-05                                 10.0   \n",
       "\n",
       "            311_category_consumer_complaint_added_zeros  \\\n",
       "time                                                      \n",
       "2012-07-01                                         25.0   \n",
       "2012-07-02                                         72.0   \n",
       "2012-07-03                                         57.0   \n",
       "2012-07-04                                         18.0   \n",
       "2012-07-05                                         66.0   \n",
       "\n",
       "            weather_temperature_mean  ...  311_category_Noise_added_zeros  \\\n",
       "time                                  ...                                   \n",
       "2012-07-01                 27.923529  ...                           934.0   \n",
       "2012-07-02                 27.681250  ...                           453.0   \n",
       "2012-07-03                 26.240625  ...                           487.0   \n",
       "2012-07-04                 26.196969  ...                           882.0   \n",
       "2012-07-05                 29.859375  ...                           617.0   \n",
       "\n",
       "            311_category_Literature_request_added_zeros  \\\n",
       "time                                                      \n",
       "2012-07-01                                          9.0   \n",
       "2012-07-02                                        112.0   \n",
       "2012-07-03                                         85.0   \n",
       "2012-07-04                                         10.0   \n",
       "2012-07-05                                         84.0   \n",
       "\n",
       "            311_category_taxi_added_zeros  \\\n",
       "time                                        \n",
       "2012-07-01                           25.0   \n",
       "2012-07-02                           40.0   \n",
       "2012-07-03                           28.0   \n",
       "2012-07-04                           31.0   \n",
       "2012-07-05                           46.0   \n",
       "\n",
       "            311_category_collection_added_zeros  \\\n",
       "time                                              \n",
       "2012-07-01                                 70.0   \n",
       "2012-07-02                                 24.0   \n",
       "2012-07-03                                 48.0   \n",
       "2012-07-04                                 19.0   \n",
       "2012-07-05                                  2.0   \n",
       "\n",
       "            311_category_homeless_person_assistance_added_zeros  \\\n",
       "time                                                              \n",
       "2012-07-01                                          26.341051     \n",
       "2012-07-02                                          26.341051     \n",
       "2012-07-03                                          26.341051     \n",
       "2012-07-04                                          26.341051     \n",
       "2012-07-05                                          26.341051     \n",
       "\n",
       "            311_category_Traffic_added_zeros  \\\n",
       "time                                           \n",
       "2012-07-01                              61.0   \n",
       "2012-07-02                              88.0   \n",
       "2012-07-03                             121.0   \n",
       "2012-07-04                              70.0   \n",
       "2012-07-05                             100.0   \n",
       "\n",
       "            311_category_Damaged_Tree_added_zeros  motorist_killed_sum  \\\n",
       "time                                                                     \n",
       "2012-07-01                                   32.0                  0.0   \n",
       "2012-07-02                                   80.0                  1.0   \n",
       "2012-07-03                                   73.0                  1.0   \n",
       "2012-07-04                                   25.0                  0.0   \n",
       "2012-07-05                                   60.0                  0.0   \n",
       "\n",
       "            311_category_Enforcement_added_zeros  \\\n",
       "time                                               \n",
       "2012-07-01                                  13.0   \n",
       "2012-07-02                                 123.0   \n",
       "2012-07-03                                  58.0   \n",
       "2012-07-04                                   8.0   \n",
       "2012-07-05                                  71.0   \n",
       "\n",
       "            311_category_graffiti_added_zeros  \n",
       "time                                           \n",
       "2012-07-01                                6.0  \n",
       "2012-07-02                              169.0  \n",
       "2012-07-03                               38.0  \n",
       "2012-07-04                                5.0  \n",
       "2012-07-05                               96.0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see which of these features would be classified by our approach as bad for augmentation. To this end, let's build our model over openml-training instances with $\\theta = 1$, svm-rbf, and one candidate per query in the training examples. NOTE THAT THESE TRAINING INSTANCES REFER TO REGRESSION PROBLEMS.\n",
    "\n",
    "### For now we are going to use the following class policy: if the gain in R2-score is predicted as \"above zero\", we consider that the feature should not be pruned. Otherwise it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "FEATURES = ['query_num_of_columns', 'query_num_of_rows', 'query_row_column_ratio',\n",
    "            'query_max_skewness', 'query_max_kurtosis', 'query_max_unique', \n",
    "            'candidate_num_rows', 'candidate_max_skewness', 'candidate_max_kurtosis',\n",
    "            'candidate_max_unique', 'query_target_max_pearson', \n",
    "            'query_target_max_spearman', 'query_target_max_covariance', \n",
    "            'query_target_max_mutual_info', 'candidate_target_max_pearson', \n",
    "            'candidate_target_max_spearman', 'candidate_target_max_covariance', \n",
    "            'candidate_target_max_mutual_info']\n",
    "THETA = 1\n",
    "\n",
    "def train_rbf_svm(features, classes):\n",
    "    '''\n",
    "    Builds a model using features to predict associated classes,\n",
    "    '''\n",
    "\n",
    "    feature_scaler = MinMaxScaler().fit(features)\n",
    "    features_train = feature_scaler.transform(features)\n",
    "    clf = SVC(max_iter=1000, gamma='auto')\n",
    "    clf.fit(features_train, classes)\n",
    "\n",
    "    return feature_scaler, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7566, 36)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openml_training = pd.read_csv('../classification/training-simplified-data-generation.csv')\n",
    "openml_training['class_pos_neg'] = ['gain' if row['gain_in_r2_score'] > 0 else 'loss' \n",
    "                                    for index, row in openml_training.iterrows()]\n",
    "openml_training_high_containment = openml_training.loc[openml_training['containment_fraction'] >= THETA]\n",
    "openml_training_high_containment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, for each single-feature dataset in nyc_indicators/, we will join it with the base table, generate all features and classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two lines are important for importing files that are in the parent directory, \n",
    "# necessary to generate the features\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from feature_factory import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(query_dataset, \n",
    "                     candidate_dataset, \n",
    "                     key, \n",
    "                     target_name, \n",
    "                     augmented_dataset=pd.DataFrame([]),\n",
    "                     mean_data_imputation=True):\n",
    "    '''\n",
    "    This function generates all the features required to determine, through classification, \n",
    "    whether an augmentation with the candidate_dataset (which is single-feature) is likely to \n",
    "    hamper the model (or simply bring no gain)\n",
    "    '''\n",
    "    \n",
    "    # Step 1: individual query features\n",
    "    feature_factory_query = FeatureFactory(query_dataset.drop([target_name], axis=1))\n",
    "    query_dataset_individual_features = feature_factory_query.get_individual_features(func=max_in_modulus)\n",
    "    ## In order, the returned features are number_of_columns, number_of_rows, row_to_column_ratio,\n",
    "    ## max_mean, max_outlier_percentage, max_skewness, max_kurtosis, max_number_of_unique_values.\n",
    "    ## For now, we're only using number_of_columns, number_of_rows, row_to_column_ratio, \n",
    "    ## max_skewness, max_kurtosis, max_number_of_unique_values, so we remove the unnecessary elements \n",
    "    ## in the lines below\n",
    "    query_dataset_individual_features = [query_dataset_individual_features[index] for index in [0, 1, 2, 5, 6, 7]]\n",
    "    \n",
    "    # Step 2: individual candidate features\n",
    "    feature_factory_candidate = FeatureFactory(candidate_dataset)\n",
    "    candidate_dataset_individual_features = feature_factory_candidate.get_individual_features(func=max_in_modulus)\n",
    "    ## For now, we're only using number_of_rows, max_skewness, max_kurtosis, max_number_of_unique_values, \n",
    "    ## so we remove the unnecessary elements in the lines below \n",
    "    candidate_dataset_individual_features = [candidate_dataset_individual_features[index] for index in [1, 5, 6, 7]]\n",
    "    \n",
    "    # Step 3: join the datasets and compute pairwise features\n",
    "    if augmented_dataset.empty:\n",
    "        augmented_dataset = pd.merge(query_dataset, \n",
    "                                     candidate_dataset,\n",
    "                                     how='left',\n",
    "                                     on=key)\n",
    "    #augmented_dataset = augmented_dataset.set_index(key)\n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_dataset.columns = augmented_dataset.columns\n",
    "        new_dataset.index = augmented_dataset.index\n",
    "        augmented_dataset = new_dataset\n",
    "    \n",
    "    # Step 3.1: get query-target features \n",
    "    ## The features are, in order: max_query_target_pearson, max_query_target_spearman, \n",
    "    ## max_query_target_covariance, max_query_target_mutual_info\n",
    "    feature_factory_full_query = FeatureFactory(query_dataset)\n",
    "    query_features_target = feature_factory_full_query.get_pairwise_features_with_target(target_name,\n",
    "                                                                                         func=max_in_modulus)\n",
    "    # Step 3.2: get candidate-target features\n",
    "    ## The features are, in order: max_query_candidate_pearson, max_query_candidate_spearman, \n",
    "    ## max_query_candidate_covariance, max_query_candidate_mutual_info\n",
    "    column_names = candidate_dataset.columns.tolist() + [target_name]\n",
    "    feature_factory_candidate_target = FeatureFactory(augmented_dataset[column_names])\n",
    "    candidate_features_target = feature_factory_candidate_target.get_pairwise_features_with_target(target_name,\n",
    "                                                                                                   func=max_in_modulus)\n",
    "    \n",
    "    # Step 4: get query-candidate feature \"containment ratio\". We may not use it in models, but it's \n",
    "    ## important to have this value in order to filter candidates in baselines, for example.\n",
    "    query_key_values = query_dataset.index.values\n",
    "    candidate_key_values = candidate_dataset.index.values\n",
    "    intersection_size = len(set(query_key_values) & set(candidate_key_values))\n",
    "    containment_ratio = [intersection_size/len(query_key_values)]\n",
    "\n",
    "    return np.array(query_dataset_individual_features + \n",
    "                    candidate_dataset_individual_features + \n",
    "                    query_features_target + \n",
    "                    candidate_features_target + \n",
    "                    containment_ratio)\n",
    "\n",
    "candidate_dataset = pd.read_csv('nyc_indicators/citibike_count.csv', sep='|')\n",
    "candidate_dataset = candidate_dataset.rename(columns={'count':'citibike_count'})\n",
    "features = compute_features(taxi_crash.set_index('time'), \n",
    "                            candidate_dataset.set_index('time'), \n",
    "                            'time',\n",
    "                            'crash_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quickly sanity-check these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query number of rows 1462 query skewness -0.38688342830440225 query kurtosis 1.4770215280107495 candidate number of unique values 1456\n",
      "candidate number of rows 1633 candidate skewness 0.30200005209988534 candidate kurtosis -0.5284142437311292 candidate number of unique values 1603\n",
      "query target pearson (0.160072218826492, 7.506211089340906e-10) query target spearman SpearmanrResult(correlation=0.12402163995232865, pvalue=1.970536577824118e-06) query target covariance 950707.7308413646 query target mutual info 0.8682238377526429\n",
      "candidate target pearson (0.2623785747241163, 1.9218632466061835e-24) candidate target spearman SpearmanrResult(correlation=0.2875054890817096, pvalue=3.2000264303199555e-29) candidate target covariance 227171.49311560777 candidate target mutual info 0.7527329601416918\n"
     ]
    }
   ],
   "source": [
    "print('query number of rows', taxi_crash.shape[0], \n",
    "      'query skewness', taxi_crash['taxi_count'].skew(), \n",
    "      'query kurtosis', taxi_crash['taxi_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(taxi_crash['taxi_count'])))\n",
    "\n",
    "print('candidate number of rows', candidate_dataset.shape[0],\n",
    "      'candidate skewness', candidate_dataset['citibike_count'].skew(), \n",
    "      'candidate kurtosis', candidate_dataset['citibike_count'].kurtosis(),\n",
    "      'candidate number of unique values', len(set(candidate_dataset['citibike_count'])))\n",
    "\n",
    "augmented_dataset = pd.merge(taxi_crash,\n",
    "                             candidate_dataset,\n",
    "                             how='left',\n",
    "                             on='time').set_index('time')\n",
    "fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "new_dataset.columns = augmented_dataset.columns\n",
    "new_dataset.index = augmented_dataset.index\n",
    "augmented_dataset = new_dataset\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import cov\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "print('query target pearson', pearsonr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target spearman', spearmanr(augmented_dataset['taxi_count'], augmented_dataset['crash_count']),\n",
    "      'query target covariance', cov(augmented_dataset['taxi_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'query target mutual info', normalized_mutual_info_score(augmented_dataset['taxi_count'], augmented_dataset['crash_count']))\n",
    "\n",
    "print('candidate target pearson', pearsonr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target spearman', spearmanr(augmented_dataset['citibike_count'], augmented_dataset['crash_count']),\n",
    "      'candidate target covariance', cov(augmented_dataset['citibike_count'], augmented_dataset['crash_count'])[0, 1],\n",
    "      'candidate target mutual info', normalized_mutual_info_score(augmented_dataset['citibike_count'], augmented_dataset['crash_count']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok! So let's see what label is predicted once we present these features to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features, scaler=None):\n",
    "    '''\n",
    "    This function normalizes features using sklearn's StandardScaler\n",
    "    '''\n",
    "    if not scaler:\n",
    "        scaler = MinMaxScaler().fit(features)\n",
    "    return scaler.transform(features)\n",
    "\n",
    "#print(model.predict(normalize_features([features], feature_scaler)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create predictions for all candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_names = [f for f in os.listdir('nyc_indicators/') if '.csv' in f]\n",
    "feature_vectors = []\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        features = compute_features(taxi_crash.set_index('time'), \n",
    "                                    candidate_dataset.set_index('time'), \n",
    "                                    'time',\n",
    "                                    'crash_count')\n",
    "        feature_vectors.append(features[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(normalize_features(np.array(feature_vectors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, let's see what gain is brought by each feature individually to an initial prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def  compute_adjusted_r2_score(number_of_regressors, r2_score, number_of_samples):\n",
    "    '''\n",
    "    This function adjusts the value of an r2 score based on its numbers \n",
    "    of regressors and samples.\n",
    "    '''\n",
    "    try:\n",
    "        num = (1.0 - r2_score) * (number_of_samples - 1.0)\n",
    "        den = number_of_samples - number_of_regressors - 1.0\n",
    "        return 1.0 - (num/den)  \n",
    "    except ZeroDivisionError:\n",
    "        print('**** ERROR', number_of_samples, number_of_regressors)\n",
    "        return r2_score\n",
    "    \n",
    "def compute_model_performance_improvement(query_dataset, \n",
    "                                          candidate_dataset, \n",
    "                                          target_name, \n",
    "                                          key, \n",
    "                                          mean_data_imputation=True, \n",
    "                                          adjusted_r2_score=False, \n",
    "                                          model=None):\n",
    "    '''\n",
    "    This function computes the change in (adjusted) R2 score obtained when we try to predict 'target_name' with an \n",
    "    augmented dataset (query_dataset + candidate_dataset)\n",
    "    '''\n",
    "    \n",
    "    # To make sure that we compare apples to apples, let's perform the augmentation and any \n",
    "    # necessary missing data imputation first\n",
    "    augmented_dataset = pd.merge(query_dataset, \n",
    "                                 candidate_dataset,\n",
    "                                 how='left',\n",
    "                                 on=key)\n",
    "    \n",
    "    if mean_data_imputation:\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_dataset = pd.DataFrame(fill_NaN.fit_transform(augmented_dataset))\n",
    "        new_dataset.columns = augmented_dataset.columns\n",
    "        new_dataset.index = augmented_dataset.index\n",
    "        augmented_dataset = new_dataset\n",
    "    \n",
    "    # Now let's split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop([target_name], axis=1), \n",
    "                                                        augmented_dataset[target_name], \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Computing the initial and final r-squared scores\n",
    "    if not model:\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    model.fit(X_train[query_dataset.drop([target_name], axis=1).columns], y_train.ravel())\n",
    "    y_pred_initial = model.predict(X_test[query_dataset.drop([target_name], axis=1).columns])\n",
    "    r2_score_initial = r2_score(y_test, y_pred_initial)\n",
    "    if adjusted_r2_score:\n",
    "        r2_score_initial = compute_adjusted_r2_score(len(query_dataset.drop([target_name], axis=1).columns),\n",
    "                                                     r2_score_initial, \n",
    "                                                     len(y_test))\n",
    "        \n",
    "    model.fit(X_train[augmented_dataset.drop([target_name], axis=1).columns], y_train.ravel())\n",
    "    y_pred_final = model.predict(X_test[augmented_dataset.drop([target_name], axis=1).columns])\n",
    "    r2_score_final = r2_score(y_test, y_pred_final)\n",
    "    if adjusted_r2_score:\n",
    "        r2_score_final = compute_adjusted_r2_score(len(augmented_dataset.drop([target_name], axis=1).columns),\n",
    "                                                   r2_score_final,\n",
    "                                                   len(y_test))\n",
    "    \n",
    "    performance_difference = (r2_score_final - r2_score_initial)/np.fabs(r2_score_initial)\n",
    "    \n",
    "    return r2_score_initial, r2_score_final, performance_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "gains_in_r2_score = []\n",
    "\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(taxi_crash.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "        \n",
    "        \n",
    "candidates_info = {}\n",
    "for name, pred, feat_vector, gain, true_label in zip(candidate_names, predictions, feature_vectors, gains_in_r2_score, true_labels):\n",
    "    candidates_info[name] = (feat_vector, gain, pred, true_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how well our model works for this example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.98      0.85      0.91        75\n",
      "        loss       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84        76\n",
      "   macro avg       0.49      0.43      0.46        76\n",
      "weighted avg       0.97      0.84      0.90        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example is too easy in the sense that most candidates do lead to good augmentations. I need to create an example where most candidates do not help. \n",
    "\n",
    "### Let me first see which candidates brought the highest gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cyclist_killed_sum.csv', -0.013631028026993834), ('311_category_Boilers_added_zeros.csv', 0.07997961729100868), ('motorist_killed_sum.csv', 0.08119269554132075), ('persons_killed_sum.csv', 0.14147012684452737), ('311_category_Snow_added_zeros.csv', 0.1472460737106083), ('weather_snow_mean.csv', 0.16316251153613825), ('pedestrians_killed_sum.csv', 0.195407026426819), ('311_category_Animal_in_a_Park_added_zeros.csv', 0.21614843896030195), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.24732513214012192), ('311_category_Smoking_added_zeros.csv', 0.3475493857864856), ('311_category_Vacant_Lot_added_zeros.csv', 0.4318863599771732), ('311_category_Drinking_added_zeros.csv', 0.43968255649418525), ('311_category_Lead_added_zeros.csv', 0.4920252978669224), ('311_category_Street_Sign_added_zeros.csv', 0.5369216015199827), ('cyclist_injured_sum.csv', 0.5549550374398279), ('311_category_Heating_added_zeros.csv', 0.5638690035157186), ('311_category_collection_added_zeros.csv', 0.5661264782155653), ('weather_pluviometry_mean.csv', 0.5950838992563379), ('nypd_count.csv', 0.5991260351199055), ('311_category_Industrial_waste_added_zeros.csv', 0.599126351767138), ('311_category_DHS_Advantage_added_zeros.csv', 0.6109504921675661), ('311_category_Vending_added_zeros.csv', 0.6320437774555394), ('311_category_maintenance_added_zeros.csv', 0.6899335642426384), ('weather_windspeed_mean.csv', 0.7088692286845254), ('311_category_Food_Poisoning_added_zeros.csv', 0.7301406104227237), ('311_category_nonconst_added_zeros.csv', 0.8052009161437185), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.8065169158690201), ('311_category_Hazardous_Materials_added_zeros.csv', 0.8207777471768148), ('311_category_Animal_Abuse_added_zeros.csv', 0.840459330927071), ('311_category_School_Maintenance_added_zeros.csv', 0.8454233615351752), ('citibike_count.csv', 0.8523515896211515), ('311_category_Litter_basket_added_zeros.csv', 0.8541937006530771), ('311_category_homeless_person_assistance_added_zeros.csv', 0.8717705612514153), ('311_category_SPIT_added_zeros.csv', 0.8733940014450833), ('311_category_Food_Establishment_added_zeros.csv', 0.8839546773867323), ('311_category_rodent_added_zeros.csv', 0.9187816767207636), ('311_category_graffiti_added_zeros.csv', 0.9298341832308393), ('311_category_Homeless_Encampment_added_zeros.csv', 0.932207279837008), ('weather_temperature_mean.csv', 0.9499582773535991), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 0.9615561590716805), ('311_category_taxi_added_zeros.csv', 0.9622712365284978), ('311_category_Construction_added_zeros.csv', 0.9646974598449585), ('311_category_Highway_condition_added_zeros.csv', 0.9658468771789152), ('turnstile_count.csv', 0.9911351535388618), ('311_category_sewer_added_zeros.csv', 0.9959965997970991), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 1.0018936252881714), ('311_category_Air_Quality_added_zeros.csv', 1.0195214905589256), ('311_category_Elevator_added_zeros.csv', 1.0220695772034494), ('311_category_Noise_added_zeros.csv', 1.075975682736832), ('311_category_Illegal_parking_added_zeros.csv', 1.0775990545140037), ('311_category_Water_added_zeros.csv', 1.081080563597186), ('311_category_Damaged_Tree_added_zeros.csv', 1.1918647722024667), ('311_category_derelict_added_zeros.csv', 1.19863582225308), ('311_category_Housing_Options_added_zeros.csv', 1.1988793933726742), ('311_category_Traffic_added_zeros.csv', 1.2118312820420336), ('311_category_Agency_Issues_added_zeros.csv', 1.2189405575371879), ('311_category_Asbestos_added_zeros.csv', 1.236906384516526), ('311_category_broken_meter_added_zeros.csv', 1.2451851684468938), ('311_category_Sidewalk_Condition_added_zeros.csv', 1.250993633300947), ('311_category_Blocked_Driveway_added_zeros.csv', 1.2510445024716812), ('311_category_unsanitary_added_zeros.csv', 1.2964248399396994), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 1.2987499695196114), ('311_category_appliance_added_zeros.csv', 1.3172186110598556), ('311_category_electric_added_zeros.csv', 1.3352904065786741), ('311_category_Enforcement_added_zeros.csv', 1.3801474860508403), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 1.4120142698550413), ('311_category_flooring_stairs_added_zeros.csv', 1.4129537141059982), ('311_category_consumer_complaint_added_zeros.csv', 1.413932820334395), ('311_category_Plumbing_added_zeros.csv', 1.420473410065331), ('311_category_door_window_added_zeros.csv', 1.4607207810337388), ('311_category_Literature_request_added_zeros.csv', 1.463920667746351), ('311_category_Housing_added_zeros.csv', 1.4965957993945693), ('311_category_Paint_added_zeros.csv', 1.5064213337234849), ('311_category_SCRIE_added_zeros.csv', 1.558712778274975), ('311_category_building_added_zeros.csv', 1.6989229020921517), ('311_category_dof_added_zeros.csv', 1.8230098443216247)]\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "for name in candidate_names:\n",
    "    tmp.append((name, candidates_info[name][1]))\n",
    "print(sorted(tmp, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few of them seem to bring gains by accident, but let's see what happens if we start with a bigger table including taxispeed_speed_avg.csv and 311_category_Street_light_condition_added_zeros.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints\n",
       "0  2012-07-01      354746          443  17.902601                      37\n",
       "1  2012-07-02      412337          475  16.507096                     410\n",
       "2  2012-07-03      495375          577  15.954152                     300\n",
       "3  2012-07-04      345717          353  17.808810                      31\n",
       "4  2012-07-05      417036          517  16.603352                     290"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxispeed = pd.read_csv('taxispeed_speed_avg.csv', sep='|')\n",
    "taxispeed = taxispeed.rename(columns={'mean': 'taxispeed'})\n",
    "streetlight_complaints = pd.read_csv('311_category_Street_light_condition_added_zeros.csv', sep='|')\n",
    "streetlight_complaints = streetlight_complaints.rename(columns={'count':'streetlight_complaints'})\n",
    "taxi_crash_taxispeed = pd.merge(taxi_crash, taxispeed, on='time', how='inner')\n",
    "taxi_crash_taxispeed_streetlight = pd.merge(taxi_crash_taxispeed, streetlight_complaints, on='time', how='inner')\n",
    "\n",
    "taxi_crash_taxispeed_streetlight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_candidate_names = list(set(candidate_names) - set(['nyc_indicators/taxispeed_speed_avg.csv', \n",
    "                                         'nyc_indicators/311_category_Street_light_condition_added_zeros.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('turnstile_count.csv', -0.04203859621234797), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', -0.038392664194468615), ('311_category_Smoking_added_zeros.csv', -0.032251622626679396), ('311_category_Vending_added_zeros.csv', -0.026410682357405905), ('311_category_Elevator_added_zeros.csv', -0.021587380873384954), ('311_category_Food_Poisoning_added_zeros.csv', -0.020836944765305725), ('311_category_Housing_added_zeros.csv', -0.017828113175924074), ('311_category_Snow_added_zeros.csv', -0.010012004624826565), ('311_category_graffiti_added_zeros.csv', -0.009935261814290716), ('311_category_Housing_Options_added_zeros.csv', -0.007073414864474615), ('311_category_door_window_added_zeros.csv', -0.001869179999049201), ('motorist_killed_sum.csv', 0.0009208130232868357), ('cyclist_killed_sum.csv', 0.0025005275485507307), ('311_category_building_added_zeros.csv', 0.006593334845260233), ('weather_windspeed_mean.csv', 0.007421391176620442), ('persons_killed_sum.csv', 0.007465257650047305), ('311_category_Asbestos_added_zeros.csv', 0.00793960121428493), ('311_category_Enforcement_added_zeros.csv', 0.008678921808321135), ('311_category_consumer_complaint_added_zeros.csv', 0.012274415894491172), ('311_category_Litter_basket_added_zeros.csv', 0.01359778987950106), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.01464496275250513), ('311_category_unsanitary_added_zeros.csv', 0.016913401560116326), ('311_category_Highway_condition_added_zeros.csv', 0.02109606705634574), ('citibike_count.csv', 0.02182387510687942), ('311_category_Literature_request_added_zeros.csv', 0.02243339143364649), ('311_category_Industrial_waste_added_zeros.csv', 0.023252781049097556), ('311_category_Boilers_added_zeros.csv', 0.023744772723192753), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 0.02498469935085285), ('311_category_appliance_added_zeros.csv', 0.02669463791972731), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 0.02760667188064529), ('311_category_SCRIE_added_zeros.csv', 0.02768409432172275), ('311_category_Animal_Abuse_added_zeros.csv', 0.03011491193272067), ('311_category_flooring_stairs_added_zeros.csv', 0.03365579885375608), ('311_category_Drinking_added_zeros.csv', 0.03511571218764594), ('311_category_Noise_added_zeros.csv', 0.03538602424780189), ('pedestrians_killed_sum.csv', 0.03777745667516245), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.04316219012390527), ('311_category_dof_added_zeros.csv', 0.04546485304060655), ('311_category_sewer_added_zeros.csv', 0.045809683497652395), ('311_category_Animal_in_a_Park_added_zeros.csv', 0.04669015496589134), ('311_category_broken_meter_added_zeros.csv', 0.04959735657491392), ('weather_snow_mean.csv', 0.05034073883661519), ('311_category_taxi_added_zeros.csv', 0.052583442285616705), ('311_category_DHS_Advantage_added_zeros.csv', 0.052695863694794245), ('weather_pluviometry_mean.csv', 0.05442283898362796), ('311_category_SPIT_added_zeros.csv', 0.05588810739597394), ('311_category_homeless_person_assistance_added_zeros.csv', 0.05642144565172061), ('311_category_Lead_added_zeros.csv', 0.06182422200364748), ('311_category_electric_added_zeros.csv', 0.06310784716335481), ('311_category_Food_Establishment_added_zeros.csv', 0.063685660969185), ('311_category_Air_Quality_added_zeros.csv', 0.06717197465263751), ('311_category_Paint_added_zeros.csv', 0.0707988854972309), ('311_category_School_Maintenance_added_zeros.csv', 0.07107965163839249), ('311_category_Street_Sign_added_zeros.csv', 0.07262788787299182), ('311_category_Blocked_Driveway_added_zeros.csv', 0.0726921278410629), ('311_category_nonconst_added_zeros.csv', 0.07446477747916748), ('311_category_Vacant_Lot_added_zeros.csv', 0.07558844695663454), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.08601418465455163), ('nypd_count.csv', 0.08661085724406982), ('311_category_Hazardous_Materials_added_zeros.csv', 0.09283999979553813), ('311_category_derelict_added_zeros.csv', 0.09397457358804051), ('311_category_Plumbing_added_zeros.csv', 0.09705511506702465), ('311_category_Heating_added_zeros.csv', 0.10057327561772338), ('311_category_collection_added_zeros.csv', 0.10103256244306102), ('311_category_Construction_added_zeros.csv', 0.10308428986399404), ('311_category_maintenance_added_zeros.csv', 0.10587122948380777), ('311_category_Water_added_zeros.csv', 0.11149994536219014), ('311_category_Illegal_parking_added_zeros.csv', 0.11896038977220035), ('311_category_Homeless_Encampment_added_zeros.csv', 0.11900133716536598), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.12000633978829857), ('311_category_Agency_Issues_added_zeros.csv', 0.12934216653930272), ('311_category_Traffic_added_zeros.csv', 0.13093605183172538), ('311_category_Damaged_Tree_added_zeros.csv', 0.13866688548707104), ('311_category_rodent_added_zeros.csv', 0.1555682755373543), ('cyclist_injured_sum.csv', 0.16522206505476497), ('weather_temperature_mean.csv', 0.18334116543226417)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "for name in new_candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(taxi_crash_taxispeed_streetlight.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now there are more candidates that are bringing no gains, but a bunch of them still do. Let's add a few more features to the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_injured = pd.read_csv('persons_injured_sum.csv', sep='|')\n",
    "persons_injured = persons_injured.rename(columns={'sum': 'persons_injured'})\n",
    "motorist_injured = pd.read_csv('motorist_injured_sum.csv', sep='|')\n",
    "motorist_injured = motorist_injured.rename(columns={'sum':'motorist_injured'})\n",
    "pedestrians_injured = pd.read_csv('pedestrians_injured_sum.csv', sep='|')\n",
    "pedestrians_injured = pedestrians_injured.rename(columns={'sum': 'pedestrians_injured'})\n",
    "\n",
    "crash_many_predictors = pd.merge(taxi_crash_taxispeed_streetlight, persons_injured, on='time', how='inner')\n",
    "crash_many_predictors = pd.merge(crash_many_predictors, motorist_injured, on='time', how='inner')\n",
    "crash_many_predictors = pd.merge(crash_many_predictors, pedestrians_injured, on='time', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "      <th>persons_injured</th>\n",
       "      <th>motorist_injured</th>\n",
       "      <th>pedestrians_injured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "      <td>143</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "      <td>138</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "      <td>187</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "      <td>124</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints  \\\n",
       "0  2012-07-01      354746          443  17.902601                      37   \n",
       "1  2012-07-02      412337          475  16.507096                     410   \n",
       "2  2012-07-03      495375          577  15.954152                     300   \n",
       "3  2012-07-04      345717          353  17.808810                      31   \n",
       "4  2012-07-05      417036          517  16.603352                     290   \n",
       "\n",
       "   persons_injured  motorist_injured  pedestrians_injured  \n",
       "0              143               104                   30  \n",
       "1              138                91                   24  \n",
       "2              187               139                   34  \n",
       "3              125               100                   15  \n",
       "4              124                86                   30  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_many_predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_SCRIE_added_zeros.csv', -0.014791401791830274), ('311_category_Elevator_added_zeros.csv', -0.011451887625023294), ('311_category_graffiti_added_zeros.csv', -0.010378240905085438), ('311_category_Drinking_added_zeros.csv', -0.009650693505290275), ('311_category_Housing_Options_added_zeros.csv', -0.009210933861856407), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.009008898241276646), ('311_category_Agency_Issues_added_zeros.csv', -0.006840305893750853), ('311_category_Enforcement_added_zeros.csv', -0.006730938137458545), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.006630506830875576), ('weather_windspeed_mean.csv', -0.00590437224095922), ('311_category_taxi_added_zeros.csv', -0.005281794934152415), ('311_category_School_Maintenance_added_zeros.csv', -0.004932540289080163), ('cyclist_killed_sum.csv', -0.0041670723365609405), ('311_category_Smoking_added_zeros.csv', -0.003993332663934144), ('311_category_Boilers_added_zeros.csv', -0.0038589620177934612), ('311_category_Blocked_Driveway_added_zeros.csv', -0.0036319252255472907), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.0031571402587326747), ('311_category_Food_Establishment_added_zeros.csv', -0.003141459527207121), ('311_category_dof_added_zeros.csv', -0.0024019264621132196), ('311_category_building_added_zeros.csv', -0.001798518771233265), ('311_category_Asbestos_added_zeros.csv', -0.0017957655473006397), ('311_category_appliance_added_zeros.csv', -0.0008778663494616262), ('311_category_DHS_Advantage_added_zeros.csv', -0.0006985230355998947), ('311_category_Highway_condition_added_zeros.csv', -0.0005974348727275575), ('311_category_Housing_added_zeros.csv', -0.00039883120436045856), ('311_category_Water_added_zeros.csv', 0.00021604200708175448), ('311_category_Industrial_waste_added_zeros.csv', 0.0004621703540728183), ('weather_pluviometry_mean.csv', 0.0006030486837413879), ('pedestrians_killed_sum.csv', 0.0006258162654009127), ('311_category_Hazardous_Materials_added_zeros.csv', 0.0006541475057382856), ('311_category_SPIT_added_zeros.csv', 0.0010443652785830733), ('311_category_Damaged_Tree_added_zeros.csv', 0.002192186421106882), ('311_category_consumer_complaint_added_zeros.csv', 0.002471160508107418), ('motorist_killed_sum.csv', 0.0026835306676361507), ('turnstile_count.csv', 0.0028556589642834934), ('311_category_Vending_added_zeros.csv', 0.0028784381088425605), ('persons_killed_sum.csv', 0.0030835822455052876), ('311_category_Lead_added_zeros.csv', 0.0032483980993816153), ('311_category_broken_meter_added_zeros.csv', 0.0033480280366483648), ('311_category_Literature_request_added_zeros.csv', 0.003592970002269886), ('311_category_Air_Quality_added_zeros.csv', 0.0037123114328787353), ('311_category_Illegal_parking_added_zeros.csv', 0.003758432867717673), ('311_category_maintenance_added_zeros.csv', 0.003874908138763393), ('311_category_flooring_stairs_added_zeros.csv', 0.004199266281952063), ('311_category_sewer_added_zeros.csv', 0.004272661485836762), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.004730929338048661), ('cyclist_injured_sum.csv', 0.004816743096191344), ('311_category_Food_Poisoning_added_zeros.csv', 0.00493122527625582), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.004996833029735841), ('311_category_Construction_added_zeros.csv', 0.00515876233805323), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.006065647386353659), ('311_category_unsanitary_added_zeros.csv', 0.006459966505784307), ('311_category_Noise_added_zeros.csv', 0.006549679747360505), ('311_category_Snow_added_zeros.csv', 0.007318413917196507), ('311_category_nonconst_added_zeros.csv', 0.007687480081659194), ('311_category_Litter_basket_added_zeros.csv', 0.007770553525451754), ('311_category_Plumbing_added_zeros.csv', 0.008011444807634467), ('311_category_Paint_added_zeros.csv', 0.008726971667682682), ('311_category_door_window_added_zeros.csv', 0.008809899483951705), ('311_category_homeless_person_assistance_added_zeros.csv', 0.00900074607193958), ('nypd_count.csv', 0.009228980808455335), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.010382793483382908), ('311_category_collection_added_zeros.csv', 0.011854039006899285), ('311_category_Vacant_Lot_added_zeros.csv', 0.012090358207098131), ('311_category_Heating_added_zeros.csv', 0.012837605258711889), ('311_category_Animal_Abuse_added_zeros.csv', 0.013047080978332345), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.014827222354797226), ('weather_snow_mean.csv', 0.014994338714899547), ('311_category_Street_Sign_added_zeros.csv', 0.015311514857828178), ('311_category_Traffic_added_zeros.csv', 0.016984392739993175), ('311_category_rodent_added_zeros.csv', 0.017013324275929623), ('311_category_Homeless_Encampment_added_zeros.csv', 0.01776707408871248), ('weather_temperature_mean.csv', 0.01827550602554089), ('citibike_count.csv', 0.020876005042997744), ('311_category_electric_added_zeros.csv', 0.02413075614310067), ('311_category_derelict_added_zeros.csv', 0.02508041500425141)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "candidate_names = [f for f in os.listdir('nyc_indicators/') if '.csv' in f]\n",
    "\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time')\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the gains are considerably lower and much closer to zero, but many are still slightly above zero. What would happen if we used 'gains in adjusted R2-score' instead of 'gains in R2-score'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_SCRIE_added_zeros.csv', -0.0161776045624857), ('311_category_Elevator_added_zeros.csv', -0.012767376762512255), ('311_category_graffiti_added_zeros.csv', -0.011670995759528338), ('311_category_Drinking_added_zeros.csv', -0.01092804266975232), ('311_category_Housing_Options_added_zeros.csv', -0.01047897119215519), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.010272657502357395), ('311_category_Agency_Issues_added_zeros.csv', -0.0080581455880649), ('311_category_Enforcement_added_zeros.csv', -0.007946461988505446), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.00784390406642779), ('weather_windspeed_mean.csv', -0.0071023937025346505), ('311_category_taxi_added_zeros.csv', -0.006466633429256527), ('311_category_School_Maintenance_added_zeros.csv', -0.0061099833776611665), ('cyclist_killed_sum.csv', -0.005328306774035426), ('311_category_Smoking_added_zeros.csv', -0.005150888193979814), ('311_category_Boilers_added_zeros.csv', -0.005013672272716708), ('311_category_Blocked_Driveway_added_zeros.csv', -0.0047818280157795705), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.004296989559655137), ('311_category_Food_Establishment_added_zeros.csv', -0.004280976791359712), ('311_category_dof_added_zeros.csv', -0.003525784241901324), ('311_category_building_added_zeros.csv', -0.0029095994978419882), ('311_category_Asbestos_added_zeros.csv', -0.002906787974870067), ('311_category_appliance_added_zeros.csv', -0.0019694524207475896), ('311_category_DHS_Advantage_added_zeros.csv', -0.0017863115433261838), ('311_category_Highway_condition_added_zeros.csv', -0.001683082856141069), ('311_category_Housing_added_zeros.csv', -0.0014802737895487474), ('311_category_Water_added_zeros.csv', -0.000852380744518115), ('311_category_Industrial_waste_added_zeros.csv', -0.0006010406724762077), ('weather_pluviometry_mean.csv', -0.0004571792686152927), ('pedestrians_killed_sum.csv', -0.00043392958736371924), ('311_category_Hazardous_Materials_added_zeros.csv', -0.0004049984379270327), ('311_category_SPIT_added_zeros.csv', -6.517871434398492e-06), ('311_category_Damaged_Tree_added_zeros.csv', 0.0011656081846663298), ('311_category_consumer_complaint_added_zeros.csv', 0.0014504894995646335), ('motorist_killed_sum.csv', 0.0016673565603809578), ('turnstile_count.csv', 0.001843129643850201), ('311_category_Vending_added_zeros.csv', 0.0018663911328436833), ('persons_killed_sum.csv', 0.0020758791610132003), ('311_category_Lead_added_zeros.csv', 0.0022441849620042833), ('311_category_broken_meter_added_zeros.csv', 0.0023459245459095317), ('311_category_Literature_request_added_zeros.csv', 0.0025960531151623923), ('311_category_Air_Quality_added_zeros.csv', 0.0027179215798619036), ('311_category_Illegal_parking_added_zeros.csv', 0.002765019628082665), ('311_category_maintenance_added_zeros.csv', 0.0028839612427874754), ('311_category_flooring_stairs_added_zeros.csv', 0.003215187613390622), ('311_category_sewer_added_zeros.csv', 0.0032901369479856717), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 0.0037581085424709835), ('cyclist_injured_sum.csv', 0.003845739392055974), ('311_category_Food_Poisoning_added_zeros.csv', 0.003962645712423701), ('311_category_For_Hire_Vehicle_added_zeros.csv', 0.004029642698702623), ('311_category_Construction_added_zeros.csv', 0.004195000832033177), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.0051210890139112486), ('311_category_unsanitary_added_zeros.csv', 0.005523757772292231), ('311_category_Noise_added_zeros.csv', 0.005615370676195818), ('311_category_Snow_added_zeros.csv', 0.0064003826587236004), ('311_category_nonconst_added_zeros.csv', 0.0067772637351956894), ('311_category_Litter_basket_added_zeros.csv', 0.006862096244749576), ('311_category_Plumbing_added_zeros.csv', 0.007108088358044354), ('311_category_Paint_added_zeros.csv', 0.007838766375229572), ('311_category_door_window_added_zeros.csv', 0.007923450173622308), ('311_category_homeless_person_assistance_added_zeros.csv', 0.008118337905004122), ('nypd_count.csv', 0.008351405472473053), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 0.009529649930636902), ('311_category_collection_added_zeros.csv', 0.011032048822892259), ('311_category_Vacant_Lot_added_zeros.csv', 0.01127337204116032), ('311_category_Heating_added_zeros.csv', 0.012036441919464808), ('311_category_Animal_Abuse_added_zeros.csv', 0.012250353251109852), ('311_category_Sidewalk_Condition_added_zeros.csv', 0.014068188812414993), ('weather_snow_mean.csv', 0.014238843832452367), ('311_category_Street_Sign_added_zeros.csv', 0.014562736125185946), ('311_category_Traffic_added_zeros.csv', 0.016271036906306515), ('311_category_rodent_added_zeros.csv', 0.016300581062497668), ('311_category_Homeless_Encampment_added_zeros.csv', 0.017070291396810912), ('weather_temperature_mean.csv', 0.01758948929170246), ('citibike_count.csv', 0.020245053424737525), ('311_category_electric_added_zeros.csv', 0.023568723314773832), ('311_category_derelict_added_zeros.csv', 0.02453849103756917)]\n"
     ]
    }
   ],
   "source": [
    "names_improvements = []\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time', \n",
    "                                                                            adjusted_r2_score=True)\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the model used by the user were a linear regression instead of a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_Illegal_parking_added_zeros.csv', -0.03130540327588831), ('311_category_Agency_Issues_added_zeros.csv', -0.03073419118268377), ('311_category_Benefit_Card_Replacement_added_zeros.csv', -0.03046877116163275), ('311_category_derelict_added_zeros.csv', -0.02452031173579836), ('311_category_unsanitary_added_zeros.csv', -0.022454693474382153), ('311_category_Food_Establishment_added_zeros.csv', -0.015877620756254485), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', -0.01332642089887011), ('311_category_Air_Quality_added_zeros.csv', -0.012813742017743296), ('311_category_Smoking_added_zeros.csv', -0.010698977550442764), ('311_category_Blocked_Driveway_added_zeros.csv', -0.010537410309341412), ('turnstile_count.csv', -0.010501254477746812), ('311_category_taxi_added_zeros.csv', -0.007892035639892766), ('311_category_Construction_added_zeros.csv', -0.007677094577480214), ('311_category_Litter_basket_added_zeros.csv', -0.007642806104633979), ('311_category_rodent_added_zeros.csv', -0.006758328296166309), ('311_category_Hazardous_Materials_added_zeros.csv', -0.006298648111865432), ('311_category_graffiti_added_zeros.csv', -0.0056803414199031), ('311_category_Animal_in_a_Park_added_zeros.csv', -0.005071020027254036), ('persons_killed_sum.csv', -0.0034417340882873762), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', -0.0034415249219944236), ('311_category_Elevator_added_zeros.csv', -0.0033545023890863977), ('311_category_Street_Sign_added_zeros.csv', -0.0033170724279678668), ('311_category_Water_added_zeros.csv', -0.0032112828246019976), ('311_category_For_Hire_Vehicle_added_zeros.csv', -0.003076968713147718), ('pedestrians_killed_sum.csv', -0.00298610283627221), ('311_category_Homeless_Encampment_added_zeros.csv', -0.0024465557894288283), ('nypd_count.csv', -0.002106107230052795), ('311_category_building_added_zeros.csv', -0.0016952003826407813), ('motorist_killed_sum.csv', -0.0016907710190441616), ('311_category_Violation_of_Park_Rules_added_zeros.csv', -0.0016172150882183768), ('311_category_DHS_Advantage_added_zeros.csv', -0.001310656105277468), ('311_category_Drinking_added_zeros.csv', -0.0012425122796462789), ('311_category_Industrial_waste_added_zeros.csv', -0.0011539528905676203), ('311_category_Vacant_Lot_added_zeros.csv', -0.0011261072797019056), ('311_category_Enforcement_added_zeros.csv', -0.0010440726982364561), ('311_category_Vending_added_zeros.csv', -0.00099752654592355), ('311_category_Damaged_Tree_added_zeros.csv', -0.0009784121591290603), ('cyclist_killed_sum.csv', -0.0009399737140242625), ('311_category_broken_meter_added_zeros.csv', -0.0007178776067450228), ('311_category_Lead_added_zeros.csv', -0.000712617665547594), ('311_category_Sidewalk_Condition_added_zeros.csv', -0.0002616933065977665), ('citibike_count.csv', -0.0002580066328550412), ('311_category_nonconst_added_zeros.csv', -0.00019862912958005038), ('weather_windspeed_mean.csv', 8.960862845477669e-05), ('311_category_Asbestos_added_zeros.csv', 0.00017501172471854137), ('cyclist_injured_sum.csv', 0.0003792856583222003), ('311_category_consumer_complaint_added_zeros.csv', 0.0006353570879507665), ('311_category_Food_Poisoning_added_zeros.csv', 0.0006999371528479923), ('weather_pluviometry_mean.csv', 0.0007624690978327997), ('weather_temperature_mean.csv', 0.0008923555433203629), ('311_category_Heating_added_zeros.csv', 0.0009716000302208997), ('311_category_homeless_person_assistance_added_zeros.csv', 0.001034205551332079), ('311_category_SPIT_added_zeros.csv', 0.0013346953219257483), ('311_category_collection_added_zeros.csv', 0.0014294793087596973), ('311_category_sewer_added_zeros.csv', 0.002904220056693307), ('311_category_dof_added_zeros.csv', 0.0037344262601937666), ('311_category_Noise_added_zeros.csv', 0.003754136224249021), ('311_category_Housing_Options_added_zeros.csv', 0.004024852777166605), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 0.004038320864428939), ('311_category_Highway_condition_added_zeros.csv', 0.004331901707041547), ('311_category_Snow_added_zeros.csv', 0.005967984655853863), ('311_category_Boilers_added_zeros.csv', 0.0060951737777616725), ('311_category_SCRIE_added_zeros.csv', 0.00621753936720292), ('311_category_Literature_request_added_zeros.csv', 0.0076465808504478094), ('311_category_Housing_added_zeros.csv', 0.008280097635912467), ('311_category_Animal_Abuse_added_zeros.csv', 0.008874014827122734), ('311_category_appliance_added_zeros.csv', 0.010096924992505297), ('311_category_maintenance_added_zeros.csv', 0.011256166519726236), ('311_category_flooring_stairs_added_zeros.csv', 0.014732461966379607), ('311_category_Traffic_added_zeros.csv', 0.01728249365344201), ('311_category_door_window_added_zeros.csv', 0.020774468219504433), ('311_category_Paint_added_zeros.csv', 0.020948429970024805), ('311_category_Plumbing_added_zeros.csv', 0.024000638352503118), ('weather_snow_mean.csv', 0.028400304186527184), ('311_category_School_Maintenance_added_zeros.csv', 0.029656669683404007), ('311_category_electric_added_zeros.csv', 0.03163833265994381)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "names_improvements = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "        candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name),\n",
    "                                        sep='|')\n",
    "        numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "        candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "        initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                            candidate_dataset.set_index('time'), \n",
    "                                                                            'crash_count', \n",
    "                                                                            'time', \n",
    "                                                                            adjusted_r2_score=True, \n",
    "                                                                            model=linreg)\n",
    "        gains_in_r2_score.append(improvement)\n",
    "        names_improvements.append((name, improvement))\n",
    "        if improvement > 0:\n",
    "            true_labels.append('gain')\n",
    "        else:\n",
    "            true_labels.append('loss')\n",
    "print(sorted(names_improvements, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. So now most candidates are reducing the performance. It's not exactly a 'needle in a haystack' scenario, but let's see how our model works here. \n",
    "\n",
    "### First of all, we're gonna have to re-train our model to use ADJUSTED R2-SCORES. Note that the instances that compose the openml_training dataset use random forest, not linear regression! If it works well, it works as evidence that our model can adapt well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n",
      "**** ERROR 62.0 61.0\n"
     ]
    }
   ],
   "source": [
    "def derive_adj_r2_score_from_dataset(dataset):\n",
    "    '''\n",
    "    Given a dataset with instances that can be used to train our model \n",
    "    (or test instances for said model), this function derives adjusted \n",
    "    r2 scores and their corresponding gains for each instance.\n",
    "    '''\n",
    "    adjusted_r2_scores_before = []\n",
    "    adjusted_r2_scores_after = []\n",
    "    gains_in_adjusted_r2_scores = []\n",
    "    for index, row in dataset.iterrows():\n",
    "        number_of_regressors_before = row['query_num_of_columns']\n",
    "        r2_score_before = row['r2_score_before']\n",
    "        \n",
    "        number_of_regressors_after = row['query_num_of_columns'] + row['candidate_num_of_columns']\n",
    "        r2_score_after = row['r2_score_after']\n",
    "\n",
    "        number_of_samples = row['query_num_of_rows'] #after augmentation, this is also the number of samples\n",
    "        \n",
    "        adj_r2_score_before = compute_adjusted_r2_score(number_of_regressors_before,\n",
    "                                                        r2_score_before, \n",
    "                                                        number_of_samples)\n",
    "        \n",
    "        adj_r2_score_after = compute_adjusted_r2_score(number_of_regressors_after, \n",
    "                                                       r2_score_after, \n",
    "                                                       number_of_samples)\n",
    "        \n",
    "        gain_in_adj_r2_score = (adj_r2_score_after - adj_r2_score_before)/np.fabs(adj_r2_score_before)\n",
    "        \n",
    "        adjusted_r2_scores_before.append(adj_r2_score_before)\n",
    "        adjusted_r2_scores_after.append(adj_r2_score_after)\n",
    "        gains_in_adjusted_r2_scores.append(gain_in_adj_r2_score)\n",
    "        \n",
    "    return adjusted_r2_scores_before, adjusted_r2_scores_after, gains_in_adjusted_r2_scores    \n",
    "\n",
    "adj_r2_scores_before, adj_r2_scores_after, gains_in_adj_r2_scores = derive_adj_r2_score_from_dataset(openml_training)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml_training['adj_r2_score_before'] = adj_r2_scores_before\n",
    "openml_training['adj_r2_scores_after'] = adj_r2_scores_after\n",
    "openml_training['gain_in_adj_r2_score'] = gains_in_adj_r2_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml_training['adj_class_pos_neg'] = ['gain' if row['gain_in_adj_r2_score'] > 0 else 'loss' \n",
    "                                        for index, row in openml_training.iterrows()]\n",
    "openml_training_high_containment = openml_training.loc[openml_training['containment_fraction'] >= THETA]\n",
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['adj_class_pos_neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's remember what the current query/initial dataset is and what the candidates are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>taxi_count</th>\n",
       "      <th>crash_count</th>\n",
       "      <th>taxispeed</th>\n",
       "      <th>streetlight_complaints</th>\n",
       "      <th>persons_injured</th>\n",
       "      <th>motorist_injured</th>\n",
       "      <th>pedestrians_injured</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-01</td>\n",
       "      <td>354746</td>\n",
       "      <td>443</td>\n",
       "      <td>17.902601</td>\n",
       "      <td>37</td>\n",
       "      <td>143</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-07-02</td>\n",
       "      <td>412337</td>\n",
       "      <td>475</td>\n",
       "      <td>16.507096</td>\n",
       "      <td>410</td>\n",
       "      <td>138</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>495375</td>\n",
       "      <td>577</td>\n",
       "      <td>15.954152</td>\n",
       "      <td>300</td>\n",
       "      <td>187</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-07-04</td>\n",
       "      <td>345717</td>\n",
       "      <td>353</td>\n",
       "      <td>17.808810</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>100</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-07-05</td>\n",
       "      <td>417036</td>\n",
       "      <td>517</td>\n",
       "      <td>16.603352</td>\n",
       "      <td>290</td>\n",
       "      <td>124</td>\n",
       "      <td>86</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  taxi_count  crash_count  taxispeed  streetlight_complaints  \\\n",
       "0  2012-07-01      354746          443  17.902601                      37   \n",
       "1  2012-07-02      412337          475  16.507096                     410   \n",
       "2  2012-07-03      495375          577  15.954152                     300   \n",
       "3  2012-07-04      345717          353  17.808810                      31   \n",
       "4  2012-07-05      417036          517  16.603352                     290   \n",
       "\n",
       "   persons_injured  motorist_injured  pedestrians_injured  \n",
       "0              143               104                   30  \n",
       "1              138                91                   24  \n",
       "2              187               139                   34  \n",
       "3              125               100                   15  \n",
       "4              124                86                   30  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crash_many_predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal is to see what labels are predicted now with this 'adjusted' model, and finally check how similar they are to the true labels. Note that this is an experiment on EFFECTIVENESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep='|')\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    features = compute_features(crash_many_predictors.set_index('time'), \n",
    "                                candidate_dataset.set_index('time'), \n",
    "                                'time', \n",
    "                                'crash_count')\n",
    "    feature_vectors.append(features[:-1])\n",
    "    \n",
    "predictions = model.predict(normalize_features(np.array(feature_vectors)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'] 76\n"
     ]
    }
   ],
   "source": [
    "print(predictions, len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'] 76\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep='|')\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                        candidate_dataset.set_index('time'), \n",
    "                                                                        'crash_count', \n",
    "                                                                        'time', \n",
    "                                                                        adjusted_r2_score=True, \n",
    "                                                                        model=linreg)\n",
    "    if improvement > 0:\n",
    "        true_labels.append('gain')\n",
    "    else:\n",
    "        true_labels.append('loss')\n",
    "print(np.array(true_labels), len(true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so our method is not pruning anything for this particular example. Let's go back to r2 scores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])\n",
    "predictions = model.predict(normalize_features(np.array(feature_vectors))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.60      0.91      0.72        43\n",
      "        loss       0.64      0.21      0.32        33\n",
      "\n",
      "    accuracy                           0.61        76\n",
      "   macro avg       0.62      0.56      0.52        76\n",
      "weighted avg       0.62      0.61      0.55        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "linreg = LinearRegression()\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep='|')\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    initial, final, improvement = compute_model_performance_improvement(crash_many_predictors.set_index('time'), \n",
    "                                                                        candidate_dataset.set_index('time'), \n",
    "                                                                        'crash_count', \n",
    "                                                                        'time', \n",
    "                                                                        adjusted_r2_score=False, \n",
    "                                                                        model=linreg)\n",
    "    if improvement > 0:\n",
    "        true_labels.append('gain')\n",
    "    else:\n",
    "        true_labels.append('loss')\n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. Note that, in this case, the minority corresponds to 'loss' -- not to 'gain', what would be ideal. But that's ok, we can fix that later. \n",
    "\n",
    "### Note that the effectiveness is pretty ok for class 'gain' but could be better for class 'loss': the low recall for the latter is to blame... \n",
    "\n",
    "### Before we try to understand EFFICIENCY, and see what kind of gains we would have if we used our model before using ARDA, let's see how a containment baseline would work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "containment_ratios = []\n",
    "for name in candidate_names:\n",
    "    candidate_dataset = pd.read_csv(os.path.join('nyc_indicators/', name), \n",
    "                                    sep='|')\n",
    "    numerical_column = [i for i in candidate_dataset.columns if i != 'time'][0]\n",
    "    candidate_dataset = candidate_dataset.rename(columns={numerical_column: name.split('.')[0]})\n",
    "    output = compute_features(crash_many_predictors.set_index('time'), \n",
    "                              candidate_dataset.set_index('time'), \n",
    "                              'time', \n",
    "                              'crash_count')\n",
    "    feature_vectors.append(output[:-1])\n",
    "    containment_ratios.append((name, output[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the containment ratios in order to figure out some kind of threshold for a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('311_category_Agency_Issues_added_zeros.csv', 1.0), ('311_category_SCRIE_added_zeros.csv', 1.0), ('cyclist_killed_sum.csv', 1.0), ('311_category_electric_added_zeros.csv', 1.0), ('311_category_Illegal_parking_added_zeros.csv', 1.0), ('311_category_Vacant_Lot_added_zeros.csv', 1.0), ('311_category_consumer_complaint_added_zeros.csv', 1.0), ('weather_temperature_mean.csv', 1.0), ('311_category_Litter_basket_added_zeros.csv', 1.0), ('311_category_dof_added_zeros.csv', 1.0), ('pedestrians_killed_sum.csv', 1.0), ('311_category_Construction_added_zeros.csv', 0.9397672826830937), ('311_category_DOH_New_License_Application_Request_added_zeros.csv', 1.0), ('311_category_Sidewalk_Condition_added_zeros.csv', 1.0), ('persons_killed_sum.csv', 1.0), ('311_category_Food_Establishment_added_zeros.csv', 1.0), ('311_category_Drinking_added_zeros.csv', 1.0), ('311_category_unsanitary_added_zeros.csv', 1.0), ('311_category_rodent_added_zeros.csv', 1.0), ('turnstile_count.csv', 1.0), ('311_category_flooring_stairs_added_zeros.csv', 0.5790554414784395), ('311_category_door_window_added_zeros.csv', 0.5790554414784395), ('311_category_Air_Quality_added_zeros.csv', 1.0), ('cyclist_injured_sum.csv', 1.0), ('311_category_Heating_added_zeros.csv', 1.0), ('311_category_derelict_added_zeros.csv', 1.0), ('311_category_Vending_added_zeros.csv', 1.0), ('weather_snow_mean.csv', 1.0), ('311_category_Plumbing_added_zeros.csv', 1.0), ('311_category_sewer_added_zeros.csv', 1.0), ('311_category_Asbestos_added_zeros.csv', 1.0), ('311_category_Lead_added_zeros.csv', 1.0), ('311_category_building_added_zeros.csv', 1.0), ('citibike_count.csv', 0.7474332648870636), ('311_category_Animal_in_a_Park_added_zeros.csv', 1.0), ('311_category_Street_Sign_added_zeros.csv', 1.0), ('nypd_count.csv', 0.8754277891854894), ('weather_windspeed_mean.csv', 1.0), ('311_category_nonconst_added_zeros.csv', 0.6228610540725531), ('311_category_Animal_Abuse_added_zeros.csv', 0.7097878165639973), ('311_category_maintenance_added_zeros.csv', 1.0), ('311_category_For_Hire_Vehicle_added_zeros.csv', 1.0), ('311_category_Housing_Options_added_zeros.csv', 1.0), ('311_category_Highway_condition_added_zeros.csv', 1.0), ('weather_pluviometry_mean.csv', 1.0), ('311_category_SPIT_added_zeros.csv', 1.0), ('311_category_Paint_added_zeros.csv', 1.0), ('311_category_broken_meter_added_zeros.csv', 1.0), ('311_category_Snow_added_zeros.csv', 1.0), ('311_category_Industrial_waste_added_zeros.csv', 1.0), ('311_category_Boilers_added_zeros.csv', 1.0), ('311_category_Fire_Safety_Director_-_F58_added_zeros.csv', 1.0), ('311_category_Blocked_Driveway_added_zeros.csv', 1.0), ('311_category_Water_added_zeros.csv', 1.0), ('311_category_Elevator_added_zeros.csv', 1.0), ('311_category_Food_Poisoning_added_zeros.csv', 1.0), ('311_category_Homeless_Encampment_added_zeros.csv', 1.0), ('311_category_Non-Emergency_Police_Matter_added_zeros.csv', 1.0), ('311_category_Violation_of_Park_Rules_added_zeros.csv', 1.0), ('311_category_appliance_added_zeros.csv', 1.0), ('311_category_School_Maintenance_added_zeros.csv', 1.0), ('311_category_DHS_Advantage_added_zeros.csv', 0.5598904859685148), ('311_category_Hazardous_Materials_added_zeros.csv', 1.0), ('311_category_Benefit_Card_Replacement_added_zeros.csv', 1.0), ('311_category_Smoking_added_zeros.csv', 1.0), ('311_category_Housing_added_zeros.csv', 1.0), ('311_category_Noise_added_zeros.csv', 1.0), ('311_category_Literature_request_added_zeros.csv', 1.0), ('311_category_taxi_added_zeros.csv', 1.0), ('311_category_collection_added_zeros.csv', 1.0), ('311_category_homeless_person_assistance_added_zeros.csv', 0.7679671457905544), ('311_category_Traffic_added_zeros.csv', 1.0), ('311_category_Damaged_Tree_added_zeros.csv', 1.0), ('motorist_killed_sum.csv', 1.0), ('311_category_Enforcement_added_zeros.csv', 1.0), ('311_category_graffiti_added_zeros.csv', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(containment_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so most of them are basically equal to one. Let's do the following baseline: if a containment ratio is below \n",
    "### 0.6, let's assume the predicted class will be 'loss'; otherwise, it will be gain. Let's see how this compares to \n",
    "### our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.56      0.95      0.71        43\n",
      "        loss       0.33      0.03      0.06        33\n",
      "\n",
      "    accuracy                           0.55        76\n",
      "   macro avg       0.45      0.49      0.38        76\n",
      "weighted avg       0.46      0.55      0.42        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containment_based_predictions = ['loss' if elem[1] < 0.6 else 'gain' for elem in containment_ratios]\n",
    "print(classification_report(true_labels, containment_based_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results were significantly worse. This is beneficial for us. Now let's implement RIFS, the feature selection algorithm in ARDA, and start digging into efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparsereg.model.base import STRidge # the stype of sparsereg they use is not clear in the ARDA paper\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_PREFIX = 'random_feature_'\n",
    "\n",
    "def augment_with_random_features(dataset, number_of_random_features):\n",
    "    '''\n",
    "    Given a dataset and a number of random features, this function derives\n",
    "    random features that are based on the original ones in the dataset\n",
    "    '''\n",
    "    mean = dataset.T.mean()\n",
    "   # print(dataset.T)\n",
    "    cov = dataset.T.cov()\n",
    "    features = np.random.multivariate_normal(mean, cov, number_of_random_features)\n",
    "    for i in range(number_of_random_features):\n",
    "        dataset[RANDOM_PREFIX + str(i)] = features[i,:]\n",
    "    return dataset\n",
    "\n",
    "def combine_rankings(rf_coefs, regression_coefs, feature_names, lin_comb_coef=0.5):\n",
    "    '''\n",
    "    Given feature coefficients computed with different methods (random forest and a regularized \n",
    "    regression), we scale their values, combine them linearly, rescale them and rank their  \n",
    "    corresponding features according to their values\n",
    "    '''\n",
    "    \n",
    "    # random forest coefficients are already in interval [0, 1]; let's pre-scale the regression \n",
    "    # coefficients, which are all over the place\n",
    "    min_value = min(regression_coefs)\n",
    "    shifted = np.array([elem + min_value*-1 for elem in regression_coefs])\n",
    "    normalized_regression_coefs = shifted/sum(shifted)\n",
    "    \n",
    "    combined_scores = lin_comb_coef*rf_coefs + (1-lin_comb_coef)*normalized_regression_coefs\n",
    "    ranked_features = sorted([(feat_name, score) for feat_name, score in zip(feature_names, combined_scores)], \n",
    "                             key=lambda x: x[1], \n",
    "                             reverse=True)\n",
    "    return [elem[0] for elem in ranked_features]\n",
    "\n",
    "def feature_in_front_of_random(feature_name, ranking):\n",
    "    '''\n",
    "    This function checks whether there are random features before \n",
    "    feature_name\n",
    "    '''\n",
    "    random_not_seen = True\n",
    "    for feat in ranking:\n",
    "        if RANDOM_PREFIX in feat:\n",
    "            return 0\n",
    "        if feat == feature_name:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def aggregate_features_by_quality(rankings):\n",
    "    '''\n",
    "    Given feature rankings, this function counts, for every non-random feature, \n",
    "    how many times it occured before every random feature\n",
    "    '''\n",
    "    feature_names = [elem for elem in rankings[0] if RANDOM_PREFIX not in elem]\n",
    "    \n",
    "    feats_quality = {}\n",
    "    for feature in feature_names:\n",
    "        for rank in rankings:\n",
    "            if feature in feats_quality:\n",
    "                feats_quality[feature] += feature_in_front_of_random(feature, rank)\n",
    "            else:\n",
    "                feats_quality[feature] = feature_in_front_of_random(feature, rank)\n",
    "    sorted_feats =  sorted(feats_quality.items(), \n",
    "                           key=lambda x: x[1], \n",
    "                           reverse=True)\n",
    "    return [(elem[0], elem[1]/len(rankings)) for elem in sorted_feats]\n",
    "    \n",
    "def random_injection_feature_selection(augmented_dataset_features, \n",
    "                                       target_column_data, \n",
    "                                       tau, \n",
    "                                       eta, \n",
    "                                       k_random_seeds):\n",
    "    '''\n",
    "    This is ARDA's feature selection algorithm RIFS. Given an augmented dataset, ideally \n",
    "    created through joins over sketches over the original datasets, a threshold tau for \n",
    "    quality of features, a fraction eta of random features to inject, and k random seeds to perform \n",
    "    k experiments in a reproducible way, it selects the features that should be used in the augmented \n",
    "    dataset\n",
    "    '''\n",
    "    number_of_random_features = int(np.ceil(eta*augmented_dataset_features.shape[1]))\n",
    "    augmented_dataset_with_random = augment_with_random_features(augmented_dataset_features, \n",
    "                                                                 number_of_random_features)\n",
    "    \n",
    "    # Now we obtain rankings using random forests and sparse regression models\n",
    "    ## the paper does not say what hyperparameters were used in the experiment\n",
    "    rankings = []\n",
    "    for seed in k_random_seeds:\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=seed)\n",
    "        rf.fit(augmented_dataset_features, target_column_data)\n",
    "        rf_coefs = rf.feature_importances_\n",
    "        \n",
    "        ## coef = STRidge().fit(augmented_dataset_features, target_column_data).coef_\n",
    "        ## print(coef, augmented_dataset_features.columns)\n",
    "        ## print(len(rf.feature_importances_))\n",
    "        \n",
    "        ## This version of lasso is giving lower weights to random features, which is good\n",
    "        lasso = Lasso(random_state=seed)\n",
    "        lasso.fit(augmented_dataset_features, target_column_data)\n",
    "        lasso_coefs = lasso.coef_\n",
    "        rank = combine_rankings(rf_coefs, lasso_coefs, augmented_dataset_features.columns)\n",
    "        rankings.append(rank)\n",
    "    \n",
    "    # Now, for each non-random feature, we get the number of times it appeared in front of \n",
    "    ## all random features\n",
    "    sorted_features = aggregate_features_by_quality(rankings)\n",
    "    return [elem[0] for elem in sorted_features if elem[1] >= tau]\n",
    "    \n",
    "def wrapper_algorithm(augmented_dataset, target_name, key, thresholds_T, eta, k_random_seeds):\n",
    "    '''\n",
    "    This function searches for the best subset of features by doing an exponential search\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(target_name, axis=1), \n",
    "                                                        augmented_dataset[target_name], \n",
    "                                                        test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "    current_r2_score = float('-inf')\n",
    "    linreg = LinearRegression()\n",
    "    selected = []\n",
    "    for tau in thresholds_T:\n",
    "        selected = random_injection_feature_selection(augmented_dataset.drop([target_name], axis=1), \n",
    "                                                      augmented_dataset[target_name],\n",
    "                                                      tau, \n",
    "                                                      eta, \n",
    "                                                      k_random_seeds)\n",
    "        linreg.fit(X_train[selected], y_train)\n",
    "        y_pred = linreg.predict(X_test[selected])\n",
    "        new_r2_score = r2_score(y_test, y_pred)\n",
    "        if  new_r2_score > current_r2_score:\n",
    "            current_r2_score = new_r2_score\n",
    "        else:\n",
    "            break\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, so now we have the code for RIFS. What we need to do now is check how it behaves when we use *all* available features, and see how things compare in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def check_efficiency_with_ida(base_dataset, \n",
    "                              dataset_directory, \n",
    "                              key, \n",
    "                              target_name, \n",
    "                              training_data, \n",
    "                              thresholds_tau=[0.2, 0.4, 0.6, 0.8], #REFACTOR: this parameter is only used by RIFS\n",
    "                              eta=0.2, #REFACTOR: this parameter is only used by RIFS\n",
    "                              k_random_seeds=[42, 17, 23, 2, 5, 19, 37, 41, 13, 33], #REFACTOR: this parameter is only used by RIFS\n",
    "                              mean_data_imputation=True, \n",
    "                              rename_numerical=True, \n",
    "                              separator='|', \n",
    "                              feature_selector=wrapper_algorithm):\n",
    "    '''\n",
    "    This function compares the time to run a feature selector with and without pre-pruning with IDA\n",
    "    '''\n",
    "    \n",
    "    #Step 1: do the join with every candidate dataset in dataset_directory. \n",
    "    ## This has to be done both with and without IDA. \n",
    "    augmented_dataset = join_datasets(base_dataset, dataset_directory, key, rename_numerical=rename_numerical, separator=separator)\n",
    "    augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()] #removing duplicate columns\n",
    "    print('Done creating the augmented dataset')\n",
    "    \n",
    "    #Step 2: let's see how much time it takes to select features with RIFS, injecting 20% of random features\n",
    "    time1 = time.time()\n",
    "    selected_all = []\n",
    "    if feature_selector == wrapper_algorithm:\n",
    "        selected_all = wrapper_algorithm(augmented_dataset, target_name, key, thresholds_tau, eta, k_random_seeds)\n",
    "    elif feature_selector == boruta_algorithm:\n",
    "        selected_all = boruta_algorithm(augmented_dataset, target_name)\n",
    "    elif feature_selector == stepwise_selection:\n",
    "        selected_all = stepwise_selection(augmented_dataset.drop([target_name], axis=1), augmented_dataset[target_name])\n",
    "    else:\n",
    "        print('feature selector that was passed is not implemented')\n",
    "        exit()\n",
    "    time2 = time.time()\n",
    "    print('time to run feature selector', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 3: let's train our IDA model over the training dataset\n",
    "    time1 = time.time()\n",
    "    feature_scaler, model = train_rbf_svm(training_data[FEATURES], \n",
    "                                          training_data['class_pos_neg'])\n",
    "    time2 = time.time()\n",
    "    print('time to train our model', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 4: generate a label for every feature in the augmented dataset\n",
    "    time1 = time.time()\n",
    "    candidate_names = set(augmented_dataset.columns) - set(base_dataset.columns)\n",
    "    feature_vectors = []\n",
    "    for name in candidate_names:\n",
    "        candidate_dataset = augmented_dataset.reset_index()[[key, name]]\n",
    "        #print('getting features for candidate column', name)\n",
    "        #print(candidate_dataset.set_index(key))\n",
    "        features = compute_features(base_dataset.set_index(key), \n",
    "                                    candidate_dataset.set_index(key), \n",
    "                                    key, \n",
    "                                    target_name, \n",
    "                                    augmented_dataset=augmented_dataset)\n",
    "        feature_vectors.append(features[:-1])\n",
    "    predictions = model.predict(normalize_features(np.array(feature_vectors))) \n",
    "\n",
    "    candidates_to_keep = [name for name, pred in zip(candidate_names, predictions) if pred == 'gain']\n",
    "    time2 = time.time()\n",
    "    print('time to predict what candidates to keep', (time2-time1)*1000.0, 'ms')\n",
    "    \n",
    "    #Step 5: run RIFS only considering the features to keep   \n",
    "    pruned = augmented_dataset[base_dataset.set_index(key).columns.to_list() + candidates_to_keep]\n",
    "    \n",
    "    time1 = time.time()\n",
    "    selected_pruned = []\n",
    "    if feature_selector == wrapper_algorithm:\n",
    "        selected_pruned = wrapper_algorithm(pruned, \n",
    "                                            target_name, \n",
    "                                            key, \n",
    "                                            thresholds_tau, \n",
    "                                            eta, \n",
    "                                            k_random_seeds)\n",
    "    elif feature_selector == boruta_algorithm:\n",
    "        selected_pruned = boruta_algorithm(pruned, target_name)\n",
    "    elif feature_selector == stepwise_selection:\n",
    "        selected_pruned = stepwise_selection(pruned.drop([target_name], axis=1), pruned[target_name])\n",
    "    else:\n",
    "        print('feature selector that was passed is not implemented')\n",
    "        exit()  \n",
    "    time2 = time.time()\n",
    "    print('time to run feature selector over features to keep', (time2-time1)*1000.0, 'ms')\n",
    "    print('size of entire dataset', augmented_dataset.shape[1], 'size of pruned', pruned.shape[1])\n",
    "    return selected_all, candidates_to_keep, selected_pruned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's see how the IDA pruning works for a use case involving college columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_college_dataset = pd.read_csv('datasets_for_use_cases/companion-datasets/college-debt-v2.csv')\n",
    "initial_college_dataset = initial_college_dataset.fillna(initial_college_dataset.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>PCIP16</th>\n",
       "      <th>PPTUG_EF</th>\n",
       "      <th>UGDS_WHITE</th>\n",
       "      <th>UGDS_BLACK</th>\n",
       "      <th>UGDS_HISP</th>\n",
       "      <th>UGDS_ASIAN</th>\n",
       "      <th>SATMTMID</th>\n",
       "      <th>SATVRMID</th>\n",
       "      <th>SATWRMID</th>\n",
       "      <th>UGDS</th>\n",
       "      <th>DEBT_EARNINGS_RATIO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12268508</td>\n",
       "      <td>0.550002</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.187607</td>\n",
       "      <td>0.164215</td>\n",
       "      <td>0.03452</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207564</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229700</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>2164.000000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>420024</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231500</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164492</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262100</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.01230</td>\n",
       "      <td>528.1225</td>\n",
       "      <td>521.880734</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>1057.000000</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234085</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.04200</td>\n",
       "      <td>575.0000</td>\n",
       "      <td>575.000000</td>\n",
       "      <td>515.276297</td>\n",
       "      <td>1713.000000</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UNITID  PCTFLOAN    PCIP16  PPTUG_EF  UGDS_WHITE  UGDS_BLACK  UGDS_HISP  \\\n",
       "0  12268508  0.550002  0.003175  0.232991    0.505208    0.187607   0.164215   \n",
       "1    207564  0.475000  0.000000  0.229700    0.295300    0.029100   0.064700   \n",
       "2    420024  0.812500  0.000000  0.231500    0.280800    0.566500   0.049300   \n",
       "3    164492  0.746500  0.000000  0.262100    0.651800    0.125800   0.102200   \n",
       "4    234085  0.458900  0.032100  0.000000    0.799200    0.060700   0.058400   \n",
       "\n",
       "   UGDS_ASIAN  SATMTMID    SATVRMID    SATWRMID         UGDS  \\\n",
       "0     0.03452  528.1225  521.880734  515.276297  3141.540889   \n",
       "1     0.00510  528.1225  521.880734  515.276297  2164.000000   \n",
       "2     0.00000  528.1225  521.880734  515.276297   203.000000   \n",
       "3     0.01230  528.1225  521.880734  515.276297  1057.000000   \n",
       "4     0.04200  575.0000  575.000000  515.276297  1713.000000   \n",
       "\n",
       "   DEBT_EARNINGS_RATIO  \n",
       "0                   49  \n",
       "1                   36  \n",
       "2                  127  \n",
       "3                   76  \n",
       "4                   53  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_college_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "time to run feature selector 1820325.453042984 ms\n",
      "time to train our model 753.7848949432373 ms\n",
      "time to predict what candidates to keep 168705.08003234863 ms\n",
      "time to run feature selector over features to keep 831579.6699523926 ms\n",
      "size of entire dataset 767 size of pruned 298\n"
     ]
    }
   ],
   "source": [
    "selected_all, candidates_to_keep, selected_pruned = check_efficiency_with_ida(initial_college_dataset, \n",
    "                                                                              'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                                                              'UNITID', \n",
    "                                                                              'DEBT_EARNINGS_RATIO', \n",
    "                                                                              openml_training_high_containment, \n",
    "                                                                              rename_numerical=False, \n",
    "                                                                              separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 21\n",
      "['CONTROL_y', 'HBCU_y', 'NPT41_PRIV_x', 'NPT42_PRIV_x', 'NPT4_PRIV_x', 'OPEID6_y', 'OPEID_x', 'OPEID_y', 'PCIP11_x', 'PCIP12_x', 'PCIP13_x', 'PCIP49_x', 'PCIP52_x', 'PCTFLOAN', 'PCTFLOAN_r_x', 'PCTPELL_x', 'PCTPELL_y', 'PPTUG_EF', 'PPTUG_EF_r_y', 'PREDDEG_y', 'RET_FT4_x', 'RET_FTL4_x', 'RET_PT4_x', 'RET_PTL4_x', 'SAT_AVG_ALL_x', 'UG25ABV_x', 'UGDS', 'UGDS_ASIAN', 'UGDS_ASIAN_r_y', 'UGDS_BLACK', 'UGDS_BLACK_r_x', 'UGDS_HISP', 'UGDS_HISP_r_y', 'UGDS_NRA_x', 'UGDS_UNKN_x', 'UGDS_WHITE', 'UGDS_WHITE_r_x', 'UGDS_r_y']\n",
      "['CONTROL_y', 'HBCU_y', 'OPEID6_y', 'OPEID_x', 'OPEID_y', 'PCIP23_x', 'PCIP47_x', 'PCIP52_x', 'PCTFLOAN', 'PCTFLOAN_r_x', 'PCTPELL_x', 'PCTPELL_y', 'PPTUG_EF', 'PREDDEG_y', 'UGDS', 'UGDS_ASIAN', 'UGDS_BLACK', 'UGDS_BLACK_r_x', 'UGDS_HISP', 'UGDS_HISP_r_y', 'UGDS_WHITE']\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_all), len(selected_pruned))\n",
    "print(sorted(selected_all))\n",
    "print(sorted(selected_pruned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pruning was far more substantial this time. Let's see the quality of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for select_all (no pruning) 0.702429762458535\n",
      "r2_score for select_pruned (pruning) 0.6712446596131255\n"
     ]
    }
   ],
   "source": [
    "augmented_dataset = join_datasets(initial_college_dataset, \n",
    "                                  'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                  'UNITID',\n",
    "                                  rename_numerical=False,\n",
    "                                  separator=',')\n",
    "augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()] #removing duplicate columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(['DEBT_EARNINGS_RATIO'], axis=1), \n",
    "                                                    augmented_dataset['DEBT_EARNINGS_RATIO'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train[selected_all], y_train)\n",
    "y_pred = rf.predict(X_test[selected_all])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_all (no pruning)', r2_score_initial)\n",
    "\n",
    "rf.fit(X_train[selected_pruned], y_train)\n",
    "y_pred = rf.predict(X_test[selected_pruned])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_pruned (pruning)', r2_score_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok. So in this case the quality did drop a bit. Now let's see the concrete quality of this classifier considering the labels for each candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_classifier_quality(classifier, \n",
    "                              base_dataset, \n",
    "                              dataset_directory, \n",
    "                              key, \n",
    "                              target_name, \n",
    "                              rename_numerical=True, \n",
    "                              separator='|'):\n",
    "    '''\n",
    "    This function generates true labels and predictions for a set of candidate datasets and \n",
    "    assesses the quality of the classifier\n",
    "    '''\n",
    "    \n",
    "    augmented_dataset = join_datasets(base_dataset, \n",
    "                                      dataset_directory, \n",
    "                                      key, \n",
    "                                      rename_numerical=rename_numerical, \n",
    "                                      separator=separator)\n",
    "    augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()]\n",
    "    \n",
    "    candidate_names = set(augmented_dataset.columns) - set(base_dataset.columns)\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    for name in candidate_names:\n",
    "        candidate_dataset = augmented_dataset.reset_index()[[key, name]]\n",
    "        \n",
    "        features = compute_features(base_dataset.set_index(key), \n",
    "                                    candidate_dataset.set_index(key), \n",
    "                                    key, \n",
    "                                    target_name, \n",
    "                                    augmented_dataset=augmented_dataset)\n",
    "        feature_vectors.append(features[:-1])\n",
    "        \n",
    "        initial, final, improvement = compute_model_performance_improvement(base_dataset.set_index(key), \n",
    "                                                                            candidate_dataset.set_index(key), \n",
    "                                                                            target_name, \n",
    "                                                                            key, \n",
    "                                                                            adjusted_r2_score=False)\n",
    "        if improvement > 0: \n",
    "            labels.append('gain')\n",
    "        else:\n",
    "            labels.append('loss')\n",
    "    predictions = classifier.predict(normalize_features(np.array(feature_vectors))) \n",
    "    print(labels)\n",
    "    print(predictions)\n",
    "    print(classification_report(labels, predictions))\n",
    "    return (labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain']\n",
      "['loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
      " 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
      " 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
      " 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss'\n",
      " 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'loss' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
      " 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain'\n",
      " 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
      " 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
      " 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
      " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
      " 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
      " 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain'\n",
      " 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
      " 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss'\n",
      " 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain'\n",
      " 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
      " 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
      " 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
      " 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
      " 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'gain' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss'\n",
      " 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
      " 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
      " 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
      " 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss'\n",
      " 'gain' 'gain' 'loss' 'loss' 'loss']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.77      0.33      0.47       663\n",
      "        loss       0.06      0.29      0.10        92\n",
      "\n",
      "    accuracy                           0.33       755\n",
      "   macro avg       0.42      0.31      0.28       755\n",
      "weighted avg       0.69      0.33      0.42       755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])\n",
    "\n",
    "bla = assess_classifier_quality(model, \n",
    "                                initial_college_dataset, \n",
    "                                'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                'UNITID', \n",
    "                                'DEBT_EARNINGS_RATIO', \n",
    "                                rename_numerical=False, \n",
    "                                separator=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. So it looks like we are removing a bunch of features that we should *not* be removing. If we did not use IDA, would RIFS eliminate many features that, individually, bring gain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_selection_matches_individual_improvement(feature_names, selected_features, true_labels):\n",
    "    '''\n",
    "    This function checks how well the features selected by a feature selection method (e.g., RIFS) match \n",
    "    the features that, individually, bring gain to a model.\n",
    "    '''\n",
    "    \n",
    "    individual_gain_features = [feat for feat, label in zip(feature_names, true_labels) if label == 'gain']\n",
    "    print('These are the', len(individual_gain_features), 'features that brought individual gain', individual_gain_features)\n",
    "    print('These are the', len(selected_features), 'features that were selected', selected_features)\n",
    "    return len(set(selected_features) & set(individual_gain_features))/len(set(individual_gain_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_true_labels = ['loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'gain', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss', 'loss']\n",
    "college_ida_predictions = ['loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
    " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
    " 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain'\n",
    " 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain' 'gain'\n",
    " 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
    " 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
    " 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain'\n",
    " 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain'\n",
    " 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
    " 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
    " 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
    " 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain'\n",
    " 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain'\n",
    " 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss'\n",
    " 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
    " 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
    " 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
    " 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
    " 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain'\n",
    " 'gain' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
    " 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
    " 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss'\n",
    " 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain'\n",
    " 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain'\n",
    " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
    " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'gain' 'gain' 'gain' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
    " 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain'\n",
    " 'gain' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss'\n",
    " 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
    " 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
    " 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
    " 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
    " 'gain' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss' 'loss'\n",
    " 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'loss' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss'\n",
    " 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
    " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss'\n",
    " 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'loss' 'gain'\n",
    " 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'gain' 'loss'\n",
    " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'gain'\n",
    " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'gain'\n",
    " 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain'\n",
    " 'gain' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss' 'loss'\n",
    " 'loss' 'gain' 'loss' 'loss' 'loss' 'loss' 'gain' 'loss' 'gain' 'loss'\n",
    " 'gain' 'loss' 'loss' 'loss' 'gain']\n",
    "\n",
    "augmented_dataset = join_datasets(initial_college_dataset, \n",
    "                                  'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                  'UNITID', \n",
    "                                  rename_numerical=False, \n",
    "                                  separator=',')\n",
    "\n",
    "augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 152 features that brought individual gain ['ACTMTMID_x', 'Transferred IH Floor Area', 'Freshmen_On_Track_CPS_Pct_Year_2_x', 'Payee_Zip4', 'X Coordinate', 'Remaining IH Floor Area', 'INSTALLATION_ID', 'HSI_x', 'SATVR75_x', 'Counted Rental Units', 'ACTENMID_x', 'ACTMT75_x', '6-BR+ Units', 'NWEA_Math_Growth_Grade_8_Pct_x', 'Income Taxes & Subtraction Adjustments_x', 'POLICE_PRECINCT', 'NWEA_Math_Attainment_Grade_8_Pct_x', 'Longitude_y', 'Federal Adjustments_y', 'Transferred Floor Area_x', 'LOCALE_x', 'SATVR25_x', '1-BR Units', 'Unknown-BR Units', 'NWEA_Math_Attainment_Grade_2_Pct_y', 'PCIP52_x', 'ACTWR25_x', 'Medical & Dental Expenses_x', 'Loss from Business & Farm Income_x', 'ACTEN25_x', 'NWEA_Math_Growth_Grade_6_Pct_y', 'Total Units', 'NPT41_PRIV_x', 'Behavior_Discipline_Year_2_x', 'Number with Taxes Paid', 'ZIP_x', 'created_date_x', 'HCM2_x', 'FuncArea_ID', 'Number with Dividends_x', 'Census Tract 2010', 'EV Level2 EVSE Num_y', 'Student Attendance Percentage 2013', 'LATITUDE_x', 'NWEA_Reading_Attainment_Grade_6_Pct_y', 'RET_FTL4_x', 'UGDS_NRA_x', 'PCIP23_x', 'SAT_AVG_ALL_x', 'Subtractions Pension & Annuity Exclusion_x', 'Community Board_x', 'Progress_Toward_Graduation_Year_1_y', 'Student_Count_Other_Ethnicity_x', 'Transfer Building ID', 'UNITID_r_x', 'SATVRMID_r_x', 'Council District_x', 'CURROPER_x', 'NPT45_PRIV_y', 'NPT44_PUB_x', 'School_Latitude_y', 'Tax Before Credits_y', 'PCIP40_y', 'PCIP43_y', 'Behavior_Discipline_Year_1_y', 'PBI_y', 'Interest_x', 'PCIP13_x', 'Verification Year', 'PCIP30_x', 'PCIP11_x', 'ACTWR25_y', 'Gain from Business & Farm Income_y', 'New York Standard Deductions Claimed_x', 'Student_Count_Asian_y', 'Other NY Subtractions_y', 'ACTCM75_y', 'CPS Unit', 'PCIP45_y', 'NWEA_Reading_Growth_Grade_6_Pct_x', 'PCIP47_y', 'SAT_AVG_y', 'C150_4_POOLED_SUPP', 'Requirements', 'Latitude (Internal)_y', 'PCIP05_x', 'School_Survey_Teacher_Response_Rate_Avg_Pct_y', 'Other_Metrics_Year_2_y', 'PCIP42_y', 'Other Deductions_y', 'School_Survey_Teacher_Response_Rate_Pct_y', 'Student_Count_Multi_y', 'last_edited_date_y', 'COMMUNITY BOARD', 'NPT43_PRIV_x', 'CENSUS TRACT', 'Growth_Reading_Grades_Tested_Pct_ES_y', 'Student_Count_Other_Ethnicity', 'NPT43_PUB_y', 'RET_PTL4_y', 'UGDS_AIAN_y', 'Attainment_Math_Pct_ES_y', 'PCIP10_x', 'RET_PT4_y', 'COUNCIL DISTRICT', 'NPT41_PRIV_y', 'PCIP26_x', 'Number filing Married Joint_y', 'Community Board', 'Student_Count_Hawaiian_Pacific_Islander', 'UGDS_NRA_y', 'OFFICE_EXPENSE', 'Additions Public Employee Retirement System Contributions_y', 'PCIP09_y', 'Taxable Income_y', 'NPT44_PUB_y', 'CURROPER_y', 'School_Survey_Student_Response_Rate_Avg_Pct_x', 'HouseNumber_x', 'NPT44_PRIV_x', 'Student_Count_Special_Ed_x', 'Federal Amount of NY Adjusted Gross Income_x', 'PCIP22_x', 'OPEID_y', 'PCIP11_y', 'PCIP22_y', 'WOMENONLY_x', 'PCIP29_x', 'NPT41_PUB_y', 'Stories_x', 'NWEA_Reading_Growth_Grade_6_Pct_y', 'Generating Building ID', 'PCIP39_y', 'Block_x', 'Number filing Head of Household_x', 'Student_Count_White_y', 'Workers Present', 'RELAFFIL_y', 'Number with Deductions Used_x', 'OracleID', 'College_Enrollment_CPS_Pct_Year_2_x', 'New York State Amount of NY Adjusted Gross Income_y', 'NPT44_PRIV_y', 'BBL', 'School_Latitude', 'SATMTMID_r_y', 'Stories_y', 'Longitude', 'College_Enrollment_CPS_Pct_Year_1', 'Number with Interest Paid_y', '5-BR Units', 'College_Persistence_CPS_Pct_Year_2']\n",
      "These are the 35 features that were selected ['PCTFLOAN', 'RET_FT4_x', 'PCTFLOAN_r_x', 'PREDDEG_y', 'CONTROL_y', 'PCIP52_x', 'UGDS_BLACK_r_x', 'PCTPELL_x', 'PCTPELL_y', 'OPEID6_y', 'PCIP12_x', 'UGDS_BLACK', 'UG25ABV_x', 'UGDS', 'RET_FTL4_x', 'PPTUG_EF_r_y', 'HBCU_y', 'NPT41_PRIV_x', 'UGDS_ASIAN', 'PPTUG_EF', 'UGDS_r_y', 'OPEID_x', 'UGDS_HISP', 'OPEID_y', 'UGDS_UNKN_x', 'UGDS_WHITE', 'UGDS_ASIAN_r_y', 'PCIP49_x', 'UGDS_NRA_x', 'PCIP13_x', 'NPT4_PRIV_x', 'UGDS_WHITE_r_x', 'PCIP11_x', 'UGDS_HISP_r_y', 'SAT_AVG_ALL_x']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05263157894736842"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_selection_matches_individual_improvement(augmented_dataset.columns, selected_all, college_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. So RIFS, in and of itself does not select that many features that bring individual gain. Let's see for the NYC data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "time to run feature selector 92338.8159275055 ms\n",
      "time to train our model 702.4819850921631 ms\n",
      "time to predict what candidates to keep 3347.064971923828 ms\n",
      "time to run feature selector over features to keep 80316.83206558228 ms\n",
      "size of entire dataset 83 size of pruned 64\n",
      "features selected by boruta without ida 4 and with ida 4\n"
     ]
    }
   ],
   "source": [
    "selected_all, candidates_to_keep, selected_pruned = check_efficiency_with_ida(crash_many_predictors, \n",
    "                                                                            'nyc_indicators/', \n",
    "                                                                            'time', \n",
    "                                                                            'crash_count', \n",
    "                                                                            openml_training_high_containment)\n",
    "print('features selected by boruta without ida', len(selected_all), 'and with ida', len(selected_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'loss', 'loss', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain']\n",
      "['loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'loss'\n",
      " 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
      " 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain'\n",
      " 'loss' 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain' 'loss'\n",
      " 'gain' 'gain' 'gain' 'gain' 'gain' 'loss']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        gain       0.65      0.73      0.69        51\n",
      "        loss       0.26      0.20      0.23        25\n",
      "\n",
      "    accuracy                           0.55        76\n",
      "   macro avg       0.46      0.46      0.46        76\n",
      "weighted avg       0.52      0.55      0.53        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_scaler, model = train_rbf_svm(openml_training_high_containment[FEATURES], \n",
    "                                      openml_training_high_containment['class_pos_neg'])\n",
    "\n",
    "bla = assess_classifier_quality(model, \n",
    "                                crash_many_predictors, \n",
    "                                'nyc_indicators/', \n",
    "                                'time', \n",
    "                                'crash_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "crash_true_labels = ['loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'loss', 'loss', 'gain', 'loss', 'loss', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'loss', 'gain', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain', 'gain', 'gain', 'gain', 'loss', 'gain', 'loss', 'gain', 'gain']\n",
    "crash_predictions = ['loss' 'gain' 'loss' 'gain' 'loss' 'loss' 'loss' 'gain' 'gain' 'gain'\n",
    " 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
    " 'loss' 'loss' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain'\n",
    " 'gain' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'loss' 'gain'\n",
    " 'gain' 'gain' 'loss' 'gain' 'loss' 'gain' 'gain' 'loss' 'gain' 'gain'\n",
    " 'gain' 'loss' 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
    " 'loss' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain' 'gain'\n",
    " 'loss' 'gain' 'gain' 'gain' 'gain' 'gain']\n",
    "\n",
    "augmented_dataset = join_datasets(crash_many_predictors, \n",
    "                                  'nyc_indicators/', \n",
    "                                  'time')\n",
    "augmented_dataset = augmented_dataset.loc[:,~augmented_dataset.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the 51 features that brought individual gain ['crash_count', 'taxispeed', 'persons_injured', 'pedestrians_injured', 'cyclist_killed_sum', '311_category_Illegal_parking_added_zeros', '311_category_Vacant_Lot_added_zeros', '311_category_consumer_complaint_added_zeros', 'weather_temperature_mean', '311_category_Litter_basket_added_zeros', '311_category_Construction_added_zeros', '311_category_DOH_New_License_Application_Request_added_zeros', '311_category_Sidewalk_Condition_added_zeros', 'persons_killed_sum', '311_category_Drinking_added_zeros', '311_category_rodent_added_zeros', 'turnstile_count', '311_category_flooring_stairs_added_zeros', '311_category_door_window_added_zeros', '311_category_Air_Quality_added_zeros', 'cyclist_injured_sum', '311_category_Heating_added_zeros', '311_category_derelict_added_zeros', '311_category_Vending_added_zeros', 'weather_snow_mean', '311_category_Plumbing_added_zeros', '311_category_Asbestos_added_zeros', '311_category_Lead_added_zeros', 'citibike_count', 'nypd_count', '311_category_maintenance_added_zeros', '311_category_For_Hire_Vehicle_added_zeros', '311_category_Highway_condition_added_zeros', 'weather_pluviometry_mean', '311_category_SPIT_added_zeros', '311_category_broken_meter_added_zeros', '311_category_Snow_added_zeros', '311_category_Boilers_added_zeros', '311_category_Fire_Safety_Director_-_F58_added_zeros', '311_category_Blocked_Driveway_added_zeros', '311_category_Water_added_zeros', '311_category_Food_Poisoning_added_zeros', '311_category_Non-Emergency_Police_Matter_added_zeros', '311_category_appliance_added_zeros', '311_category_School_Maintenance_added_zeros', '311_category_DHS_Advantage_added_zeros', '311_category_Hazardous_Materials_added_zeros', '311_category_Benefit_Card_Replacement_added_zeros', '311_category_Housing_added_zeros', '311_category_Literature_request_added_zeros', '311_category_taxi_added_zeros']\n",
      "These are the 4 features that were selected ['persons_injured', '311_category_dof_added_zeros', 'streetlight_complaints', 'taxispeed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0392156862745098"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_selection_matches_individual_improvement(augmented_dataset.columns, selected_all, crash_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for this case. Let's re-check the quality for the model with and without pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score for select_all (no pruning) 0.6432832521733789\n",
      "r2_score for select_pruned (pruning) 0.6432832521733789\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop(['crash_count'], axis=1), \n",
    "                                                    augmented_dataset['crash_count'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train[selected_all], y_train)\n",
    "y_pred = linreg.predict(X_test[selected_all])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_all (no pruning)', r2_score_initial)\n",
    "\n",
    "linreg.fit(X_train[selected_pruned], y_train)\n",
    "y_pred = linreg.predict(X_test[selected_pruned])\n",
    "r2_score_initial = r2_score(y_test.to_list(), y_pred)\n",
    "print('r2_score for select_pruned (pruning)', r2_score_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If, instead of using RIFS, we use other feature selection methods, what happens? Let's start with boruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def boruta_algorithm(dataset, target_name):\n",
    "    '''\n",
    "    This function selects features in the dataset using an implementation \n",
    "    of the boruta algorithm\n",
    "    '''\n",
    "    print('USING BORUTA')\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    feat_selector = BorutaPy(rf, n_estimators='auto', random_state=1)\n",
    "    feat_selector.fit(dataset.drop([target_name], axis=1).values, dataset[target_name].values.ravel())\n",
    "    filtered = feat_selector.transform(dataset.drop([target_name], axis=1).values)\n",
    "    generously_selected = feat_selector.support_weak_\n",
    "    feat_names = dataset.drop([target_name], axis=1).columns\n",
    "    return [name for name, mask in zip(feat_names, generously_selected) if mask]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING BORUTA\n"
     ]
    }
   ],
   "source": [
    "bla = boruta_algorithm(augmented_dataset, 'crash_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "USING BORUTA\n",
      "time to run feature selector 173437.69478797913 ms\n",
      "time to train our model 733.130931854248 ms\n",
      "time to predict what candidates to keep 3456.9151401519775 ms\n",
      "USING BORUTA\n",
      "time to run feature selector over features to keep 150560.6849193573 ms\n",
      "size of entire dataset 83 size of pruned 64\n"
     ]
    }
   ],
   "source": [
    "selected_all_boruta, candidates_to_keep_boruta, selected_pruned_boruta = check_efficiency_with_ida(crash_many_predictors, \n",
    "                                                                                                   'nyc_indicators/', \n",
    "                                                                                                   'time', \n",
    "                                                                                                   'crash_count', \n",
    "                                                                                                   openml_training_high_containment, \n",
    "                                                                                                   feature_selector=boruta_algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features selected by boruta without ida 1 and with ida 1\n"
     ]
    }
   ],
   "source": [
    "print('number of features selected by boruta without ida', len(selected_all_boruta), 'and with ida', len(selected_pruned_boruta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "USING BORUTA\n",
      "time to run feature selector 675668.5070991516 ms\n",
      "time to train our model 1311.9659423828125 ms\n",
      "time to predict what candidates to keep 173383.51821899414 ms\n",
      "USING BORUTA\n",
      "time to run feature selector over features to keep 460453.4831047058 ms\n",
      "size of entire dataset 767 size of pruned 298\n"
     ]
    }
   ],
   "source": [
    "selected_all_college_boruta, candidates_to_keep_college_boruta, selected_pruned_college_boruta = check_efficiency_with_ida(initial_college_dataset, \n",
    "                                                                                                                           'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                                                                                                           'UNITID', \n",
    "                                                                                                                           'DEBT_EARNINGS_RATIO', \n",
    "                                                                                                                           openml_training_high_containment, \n",
    "                                                                                                                           rename_numerical=False, \n",
    "                                                                                                                           separator=',', \n",
    "                                                                                                                           feature_selector=boruta_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features selected by boruta without ida 0 and with ida 1\n"
     ]
    }
   ],
   "source": [
    "print('number of features selected by boruta without ida', \n",
    "      len(selected_all_college_boruta), 'and with ida', len(selected_pruned_college_boruta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whoah, it seems like boruta is very harsh. Let's see how other feature selector algorithms work. Let's do stepwise selection because, in a way, it combines backward and forward selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "def stepwise_selection(data, target):\n",
    "    print('USING STEPWISE_SELECTION')\n",
    "    # Build RF classifier to use in feature selection\n",
    "    rf = RandomForestRegressor(n_estimators=10, n_jobs=-1, random_state=42)\n",
    "    # Build step forward feature selection\n",
    "    sfs1 = sfs(rf,\n",
    "               k_features=5,\n",
    "               forward=True,\n",
    "               floating=True,\n",
    "               verbose=2,\n",
    "               scoring='r2',\n",
    "               cv=5)\n",
    "\n",
    "    # Perform SFFS\n",
    "    sfs1 = sfs1.fit(data, target)\n",
    "    all_features = data.columns.tolist()\n",
    "    chosen_features = list(sfs1.k_feature_idx_)\n",
    "    return [all_features[i] for i in chosen_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "USING STEPWISE_SELECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:   52.7s finished\n",
      "\n",
      "[2020-11-04 19:57:34] Features: 1/5 -- score: 0.18758008237132076[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:   53.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s finished\n",
      "\n",
      "[2020-11-04 19:58:29] Features: 2/5 -- score: 0.3156333430624147[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   53.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.3s finished\n",
      "\n",
      "[2020-11-04 19:59:23] Features: 3/5 -- score: 0.5307343198705456[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:   53.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.0s finished\n",
      "\n",
      "[2020-11-04 20:00:18] Features: 4/5 -- score: 0.5929200530042202[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:   53.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.7s finished\n",
      "\n",
      "[2020-11-04 20:01:14] Features: 5/5 -- score: 0.6129872210817885"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run feature selector 272944.11396980286 ms\n",
      "time to train our model 1108.201265335083 ms\n",
      "time to predict what candidates to keep 3606.549024581909 ms\n",
      "USING STEPWISE_SELECTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:   40.7s finished\n",
      "\n",
      "[2020-11-04 20:02:00] Features: 1/5 -- score: 0.18758008237132076[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:   40.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s finished\n",
      "\n",
      "[2020-11-04 20:02:41] Features: 2/5 -- score: 0.3156333430624147[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:   40.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.3s finished\n",
      "\n",
      "[2020-11-04 20:03:23] Features: 3/5 -- score: 0.5307343198705456[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   40.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.0s finished\n",
      "\n",
      "[2020-11-04 20:04:05] Features: 4/5 -- score: 0.5929200530042202[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:   40.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to run feature selector over features to keep 208754.93097305298 ms\n",
      "size of entire dataset 83 size of pruned 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.7s finished\n",
      "\n",
      "[2020-11-04 20:04:48] Features: 5/5 -- score: 0.6122477279389021"
     ]
    }
   ],
   "source": [
    "selected_all_stepwise, candidates_to_keep_stepwise, selected_pruned_stepwise = check_efficiency_with_ida(crash_many_predictors, \n",
    "                                                                                                   'nyc_indicators/', \n",
    "                                                                                                   'time', \n",
    "                                                                                                   'crash_count', \n",
    "                                                                                                   openml_training_high_containment, \n",
    "                                                                                                   feature_selector=stepwise_selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n",
      "USING STEPWISE_SELECTION\n",
      "initial features 767\n",
      "remaining features 767\n",
      "remaining features 766\n",
      "remaining features 765\n",
      "remaining features 764\n",
      "remaining features 763\n",
      "remaining features 762\n",
      "remaining features 761\n",
      "remaining features 760\n",
      "remaining features 759\n",
      "remaining features 758\n",
      "remaining features 757\n",
      "remaining features 756\n",
      "remaining features 755\n",
      "remaining features 754\n",
      "remaining features 753\n",
      "remaining features 752\n",
      "remaining features 751\n",
      "remaining features 750\n",
      "remaining features 749\n",
      "remaining features 748\n",
      "remaining features 747\n",
      "remaining features 746\n",
      "remaining features 745\n",
      "remaining features 744\n",
      "remaining features 743\n",
      "remaining features 742\n",
      "remaining features 741\n",
      "remaining features 740\n",
      "remaining features 739\n",
      "remaining features 738\n",
      "remaining features 737\n",
      "remaining features 736\n",
      "remaining features 735\n",
      "remaining features 734\n",
      "remaining features 733\n",
      "remaining features 732\n",
      "remaining features 731\n",
      "remaining features 730\n",
      "remaining features 729\n",
      "remaining features 728\n",
      "remaining features 727\n",
      "remaining features 726\n",
      "remaining features 725\n",
      "remaining features 724\n",
      "remaining features 723\n",
      "remaining features 722\n",
      "remaining features 721\n",
      "remaining features 720\n",
      "remaining features 719\n",
      "remaining features 718\n",
      "remaining features 717\n",
      "remaining features 716\n",
      "remaining features 715\n",
      "remaining features 714\n",
      "remaining features 713\n",
      "remaining features 712\n",
      "remaining features 711\n",
      "remaining features 710\n",
      "remaining features 709\n",
      "remaining features 708\n",
      "remaining features 707\n",
      "remaining features 706\n",
      "remaining features 705\n",
      "remaining features 704\n",
      "remaining features 703\n",
      "remaining features 702\n",
      "remaining features 701\n",
      "remaining features 700\n",
      "remaining features 699\n",
      "remaining features 698\n",
      "remaining features 697\n",
      "remaining features 696\n",
      "remaining features 695\n",
      "remaining features 694\n",
      "remaining features 693\n",
      "remaining features 692\n",
      "remaining features 691\n",
      "remaining features 690\n",
      "remaining features 689\n",
      "remaining features 688\n",
      "remaining features 687\n",
      "remaining features 686\n",
      "remaining features 685\n",
      "remaining features 684\n",
      "remaining features 683\n",
      "remaining features 682\n",
      "remaining features 681\n",
      "remaining features 680\n",
      "remaining features 679\n",
      "remaining features 678\n",
      "remaining features 677\n",
      "remaining features 676\n",
      "remaining features 675\n",
      "remaining features 674\n",
      "remaining features 673\n",
      "remaining features 672\n",
      "remaining features 671\n",
      "remaining features 670\n",
      "remaining features 669\n",
      "remaining features 668\n",
      "remaining features 667\n",
      "remaining features 666\n",
      "remaining features 665\n",
      "remaining features 664\n",
      "remaining features 663\n",
      "remaining features 662\n",
      "remaining features 661\n",
      "remaining features 660\n",
      "remaining features 659\n",
      "remaining features 658\n",
      "remaining features 657\n",
      "remaining features 656\n",
      "remaining features 655\n",
      "remaining features 654\n",
      "remaining features 653\n",
      "remaining features 652\n",
      "remaining features 651\n",
      "remaining features 650\n",
      "remaining features 649\n",
      "remaining features 648\n",
      "remaining features 647\n",
      "remaining features 646\n",
      "remaining features 645\n",
      "remaining features 644\n",
      "remaining features 643\n",
      "remaining features 642\n",
      "remaining features 641\n",
      "remaining features 640\n",
      "remaining features 639\n",
      "remaining features 638\n",
      "remaining features 637\n",
      "remaining features 636\n",
      "remaining features 635\n",
      "remaining features 634\n",
      "remaining features 633\n",
      "remaining features 632\n",
      "remaining features 631\n",
      "remaining features 630\n",
      "remaining features 629\n",
      "remaining features 628\n",
      "remaining features 627\n",
      "remaining features 626\n",
      "remaining features 625\n",
      "remaining features 624\n",
      "remaining features 623\n",
      "remaining features 622\n",
      "remaining features 621\n",
      "remaining features 620\n",
      "remaining features 619\n",
      "remaining features 618\n",
      "remaining features 617\n",
      "remaining features 616\n",
      "remaining features 615\n",
      "remaining features 614\n",
      "remaining features 613\n",
      "remaining features 612\n",
      "remaining features 611\n",
      "remaining features 610\n",
      "remaining features 609\n",
      "remaining features 608\n",
      "remaining features 607\n",
      "remaining features 606\n",
      "remaining features 605\n",
      "remaining features 604\n",
      "remaining features 603\n",
      "remaining features 602\n",
      "remaining features 601\n",
      "remaining features 600\n",
      "remaining features 599\n",
      "remaining features 598\n",
      "remaining features 597\n",
      "remaining features 596\n",
      "remaining features 595\n",
      "remaining features 594\n",
      "remaining features 593\n",
      "remaining features 592\n",
      "remaining features 591\n",
      "remaining features 590\n",
      "remaining features 589\n",
      "remaining features 588\n",
      "remaining features 587\n",
      "remaining features 586\n",
      "remaining features 585\n",
      "remaining features 584\n",
      "remaining features 583\n",
      "remaining features 582\n",
      "remaining features 581\n",
      "remaining features 580\n",
      "remaining features 579\n",
      "remaining features 578\n",
      "remaining features 577\n",
      "remaining features 576\n",
      "remaining features 575\n",
      "remaining features 574\n",
      "remaining features 573\n",
      "remaining features 572\n",
      "remaining features 571\n",
      "remaining features 570\n",
      "remaining features 569\n",
      "remaining features 568\n",
      "remaining features 567\n",
      "remaining features 566\n",
      "remaining features 565\n",
      "remaining features 564\n",
      "remaining features 563\n",
      "remaining features 562\n",
      "remaining features 561\n",
      "remaining features 560\n",
      "remaining features 559\n",
      "remaining features 558\n",
      "remaining features 557\n",
      "remaining features 556\n",
      "remaining features 555\n",
      "remaining features 554\n",
      "remaining features 553\n",
      "remaining features 552\n",
      "remaining features 551\n",
      "remaining features 550\n",
      "remaining features 549\n",
      "remaining features 548\n",
      "remaining features 547\n",
      "remaining features 546\n",
      "remaining features 545\n",
      "remaining features 544\n",
      "remaining features 543\n",
      "remaining features 542\n",
      "remaining features 541\n",
      "remaining features 540\n",
      "remaining features 539\n",
      "remaining features 538\n",
      "remaining features 537\n",
      "remaining features 536\n",
      "remaining features 535\n",
      "remaining features 534\n",
      "remaining features 533\n",
      "remaining features 532\n",
      "remaining features 531\n",
      "remaining features 530\n",
      "remaining features 529\n",
      "remaining features 528\n",
      "remaining features 527\n",
      "remaining features 526\n",
      "remaining features 525\n",
      "remaining features 524\n",
      "remaining features 523\n",
      "remaining features 522\n",
      "remaining features 521\n",
      "remaining features 520\n",
      "remaining features 519\n",
      "remaining features 518\n",
      "remaining features 517\n",
      "remaining features 516\n",
      "remaining features 515\n",
      "remaining features 514\n",
      "remaining features 513\n",
      "remaining features 512\n",
      "remaining features 511\n",
      "remaining features 510\n",
      "remaining features 509\n",
      "remaining features 508\n",
      "remaining features 507\n",
      "remaining features 506\n",
      "remaining features 505\n",
      "remaining features 504\n",
      "remaining features 503\n",
      "remaining features 502\n",
      "remaining features 501\n",
      "remaining features 500\n",
      "remaining features 499\n",
      "remaining features 498\n",
      "remaining features 497\n",
      "remaining features 496\n",
      "remaining features 495\n",
      "remaining features 494\n",
      "remaining features 493\n",
      "remaining features 492\n",
      "remaining features 491\n",
      "remaining features 490\n",
      "remaining features 489\n",
      "remaining features 488\n",
      "remaining features 487\n",
      "remaining features 486\n",
      "remaining features 485\n",
      "remaining features 484\n",
      "remaining features 483\n",
      "remaining features 482\n",
      "remaining features 481\n",
      "remaining features 480\n",
      "remaining features 479\n",
      "remaining features 478\n",
      "remaining features 477\n",
      "remaining features 476\n",
      "remaining features 475\n",
      "remaining features 474\n",
      "remaining features 473\n",
      "remaining features 472\n",
      "remaining features 471\n",
      "remaining features 470\n",
      "remaining features 469\n",
      "remaining features 468\n",
      "remaining features 467\n",
      "remaining features 466\n",
      "remaining features 465\n",
      "remaining features 464\n",
      "remaining features 463\n",
      "remaining features 462\n",
      "remaining features 461\n",
      "remaining features 460\n",
      "remaining features 459\n",
      "remaining features 458\n",
      "remaining features 457\n",
      "remaining features 456\n",
      "remaining features 455\n",
      "remaining features 454\n",
      "remaining features 453\n",
      "remaining features 452\n",
      "remaining features 451\n",
      "remaining features 450\n",
      "remaining features 449\n",
      "remaining features 448\n",
      "remaining features 447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-682-f71b56bfab2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                                                                            \u001b[0mrename_numerical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                                                                            \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                                                                            feature_selector=stepwise_selection)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-663-05661858b776>\u001b[0m in \u001b[0;36mcheck_efficiency_with_ida\u001b[0;34m(base_dataset, dataset_directory, key, target_name, training_data, thresholds_tau, eta, k_random_seeds, mean_data_imputation, rename_numerical, separator, feature_selector)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mselected_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboruta_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfeature_selector\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstepwise_selection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mselected_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepwise_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature selector that was passed is not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-681-ee337f6cd1b2>\u001b[0m in \u001b[0;36mstepwise_selection\u001b[0;34m(data, target, SL_in, SL_out)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnew_pval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremaining_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnew_column\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremaining_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_features\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnew_pval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmin_p_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;31m# Cache these singular values for use later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwexog_singular_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingular_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingular_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv_wexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwendog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mmatrix_rank\u001b[0;34m(M, tol, hermitian)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_uv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->d'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1638\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1639\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_all_college_stepwise, candidates_to_keep_college_stepwise, selected_pruned_college_stepwise = check_efficiency_with_ida(initial_college_dataset, \n",
    "                                                                                                                           'datasets_for_use_cases/companion-datasets/college-debt-single-column/', \n",
    "                                                                                                                           'UNITID', \n",
    "                                                                                                                           'DEBT_EARNINGS_RATIO', \n",
    "                                                                                                                           openml_training_high_containment, \n",
    "                                                                                                                           rename_numerical=False, \n",
    "                                                                                                                           separator=',', \n",
    "                                                                                                                           feature_selector=stepwise_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This method takes too long to run, so I'll save it for the weekend, potentially verifying whether I can make it run faster. Let me see how IDA+RIFS behave again over the datasets provided by Nadiia (ARDA's author)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_query_dataset = pd.read_csv('arda_datasets/airline/flights.csv')\n",
    "categorical_columns = flight_query_dataset.set_index('key').select_dtypes(exclude=['int64', 'float64'])\n",
    "categorical_columns = categorical_columns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>departure_time</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>airline</th>\n",
       "      <th>flight</th>\n",
       "      <th>plane</th>\n",
       "      <th>delay</th>\n",
       "      <th>key</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1420096800</td>\n",
       "      <td>SFO</td>\n",
       "      <td>CLT</td>\n",
       "      <td>US</td>\n",
       "      <td>N171US</td>\n",
       "      <td>840</td>\n",
       "      <td>5</td>\n",
       "      <td>1420096800--SFO</td>\n",
       "      <td>884363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1420097100</td>\n",
       "      <td>SFO</td>\n",
       "      <td>MSP</td>\n",
       "      <td>DL</td>\n",
       "      <td>N3730B</td>\n",
       "      <td>806</td>\n",
       "      <td>8</td>\n",
       "      <td>1420097100--SFO</td>\n",
       "      <td>884363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1420098480</td>\n",
       "      <td>SFO</td>\n",
       "      <td>IAH</td>\n",
       "      <td>UA</td>\n",
       "      <td>N78448</td>\n",
       "      <td>1197</td>\n",
       "      <td>-7</td>\n",
       "      <td>1420098480--SFO</td>\n",
       "      <td>884363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1420115460</td>\n",
       "      <td>SFO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>UA</td>\n",
       "      <td>N77066</td>\n",
       "      <td>1532</td>\n",
       "      <td>26</td>\n",
       "      <td>1420115460--SFO</td>\n",
       "      <td>884363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1420116900</td>\n",
       "      <td>SFO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>WN</td>\n",
       "      <td>N742SW</td>\n",
       "      <td>526</td>\n",
       "      <td>-2</td>\n",
       "      <td>1420116900--SFO</td>\n",
       "      <td>884363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   departure_time origin destination airline  flight  plane  delay  \\\n",
       "0      1420096800    SFO         CLT      US  N171US    840      5   \n",
       "1      1420097100    SFO         MSP      DL  N3730B    806      8   \n",
       "2      1420098480    SFO         IAH      UA  N78448   1197     -7   \n",
       "3      1420115460    SFO         DEN      UA  N77066   1532     26   \n",
       "4      1420116900    SFO         DEN      WN  N742SW    526     -2   \n",
       "\n",
       "               key  population  \n",
       "0  1420096800--SFO      884363  \n",
       "1  1420097100--SFO      884363  \n",
       "2  1420098480--SFO      884363  \n",
       "3  1420115460--SFO      884363  \n",
       "4  1420116900--SFO      884363  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#school_query_dataset = school_query_dataset.drop(categorical_columns, axis=1)\n",
    "categorical_columns\n",
    "flight_query_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating the augmented dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "selected_all_airplane, candidates_to_keep_airplane, selected_pruned_airplane = check_efficiency_with_ida(flight_query_dataset, \n",
    "                                                                              'arda_datasets/airline/candidates/', \n",
    "                                                                              'key', \n",
    "                                                                              'population', \n",
    "                                                                              openml_training_high_containment, \n",
    "                                                                              rename_numerical=False, \n",
    "                                                                              separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('arda_datasets/airline/candidates/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = pd.merge(augmented_dataset, \n",
    "                                         dataset,\n",
    "                                         how='left',\n",
    "                                         on=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_performance(augmented_dataset, \n",
    "                              features,\n",
    "                              target_name):\n",
    "    \n",
    "    # Now let's split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(augmented_dataset.drop([target_name], axis=1),\n",
    "                                                        augmented_dataset[target_name],\n",
    "                                                        test_size=0.33,\n",
    "                                                        random_state=42)\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train[features], y_train.ravel())\n",
    "    y_pred = model.predict(X_test[features])\n",
    "    print(r2_score(y_test, y_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
