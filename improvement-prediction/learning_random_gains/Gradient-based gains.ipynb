{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we analyze whether using big drops (calculated in a gradient-based fashion) in the gains brought by candidate features might help us determine classes \"gain\" and \"loss\" in a way that is more meaningul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import median_absolute_deviation\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start by analyzing the initial gain for the college dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(data, target_variable_name):\n",
    "    \"\"\"Builds a model using data to predict the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.drop(target_variable_name, axis=1),\n",
    "        data[target_variable_name],\n",
    "        test_size=0.33,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # normalizing data first\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train.values.reshape(-1, 1))\n",
    "    X_train = scaler_X.transform(X_train)\n",
    "    y_train = scaler_y.transform(y_train.values.reshape(-1, 1))\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    y_test = scaler_y.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "    forest = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        max_depth=len(data.columns)-1\n",
    "    )\n",
    "    forest.fit(X_train, y_train.ravel())\n",
    "    yfit = forest.predict(X_test)\n",
    "\n",
    "    return r2_score(y_test, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_scores(data, target_variable_name, missing_value_imputation):\n",
    "    \"\"\"Builds a model using data to predict the target variable,\n",
    "    returning different performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    if missing_value_imputation:\n",
    "        \n",
    "        # imputation on data\n",
    "        fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        new_data = pd.DataFrame(fill_NaN.fit_transform(data))\n",
    "        new_data.columns = data.columns\n",
    "        new_data.index = data.index\n",
    "\n",
    "        # training and testing model\n",
    "        return train_and_test_model(new_data, target_variable_name)\n",
    "\n",
    "    else:\n",
    "        return train_and_test_model(data, target_variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCTFLOAN</th>\n",
       "      <th>PCIP16</th>\n",
       "      <th>PPTUG_EF</th>\n",
       "      <th>UGDS_WHITE</th>\n",
       "      <th>UGDS_BLACK</th>\n",
       "      <th>UGDS_HISP</th>\n",
       "      <th>UGDS_ASIAN</th>\n",
       "      <th>SATMTMID</th>\n",
       "      <th>SATVRMID</th>\n",
       "      <th>SATWRMID</th>\n",
       "      <th>UGDS</th>\n",
       "      <th>DEBT_EARNINGS_RATIO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNITID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12268508</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207564</th>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2297</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420024</th>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2315</td>\n",
       "      <td>0.2808</td>\n",
       "      <td>0.5665</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>203.0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164492</th>\n",
       "      <td>0.7465</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>0.6518</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.0123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234085</th>\n",
       "      <td>0.4589</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>575.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1713.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          PCTFLOAN  PCIP16  PPTUG_EF  UGDS_WHITE  UGDS_BLACK  UGDS_HISP  \\\n",
       "UNITID                                                                    \n",
       "12268508       NaN     NaN       NaN         NaN         NaN        NaN   \n",
       "207564      0.4750  0.0000    0.2297      0.2953      0.0291     0.0647   \n",
       "420024      0.8125  0.0000    0.2315      0.2808      0.5665     0.0493   \n",
       "164492      0.7465  0.0000    0.2621      0.6518      0.1258     0.1022   \n",
       "234085      0.4589  0.0321    0.0000      0.7992      0.0607     0.0584   \n",
       "\n",
       "          UGDS_ASIAN  SATMTMID  SATVRMID  SATWRMID    UGDS  \\\n",
       "UNITID                                                       \n",
       "12268508         NaN       NaN       NaN       NaN     NaN   \n",
       "207564        0.0051       NaN       NaN       NaN  2164.0   \n",
       "420024        0.0000       NaN       NaN       NaN   203.0   \n",
       "164492        0.0123       NaN       NaN       NaN  1057.0   \n",
       "234085        0.0420     575.0     575.0       NaN  1713.0   \n",
       "\n",
       "          DEBT_EARNINGS_RATIO  \n",
       "UNITID                         \n",
       "12268508                   49  \n",
       "207564                     36  \n",
       "420024                    127  \n",
       "164492                     76  \n",
       "234085                     53  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college_debt = pd.read_csv('data/college-debt-v2.csv')\n",
    "college_debt.index = college_debt['UNITID']\n",
    "college_debt.drop(['UNITID'], axis=1, inplace=True)\n",
    "college_debt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39565581099809455"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_query = get_performance_scores(\n",
    "    college_debt,\n",
    "    'DEBT_EARNINGS_RATIO',\n",
    "    True\n",
    ")\n",
    "scores_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's see the gains that we get for all of its candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>target</th>\n",
       "      <th>candidate</th>\n",
       "      <th>query_num_of_columns</th>\n",
       "      <th>query_num_of_rows</th>\n",
       "      <th>query_row_column_ratio</th>\n",
       "      <th>query_max_mean</th>\n",
       "      <th>query_max_outlier_percentage</th>\n",
       "      <th>query_max_skewness</th>\n",
       "      <th>query_max_kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th>decrease_in_mae</th>\n",
       "      <th>decrease_in_mse</th>\n",
       "      <th>decrease_in_medae</th>\n",
       "      <th>gain_in_r2_score</th>\n",
       "      <th>r2_score_before</th>\n",
       "      <th>r2_score_after</th>\n",
       "      <th>class</th>\n",
       "      <th>p(gain)</th>\n",
       "      <th>p(loss)</th>\n",
       "      <th>eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>DEBT_EARNINGS_RATIO</td>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>415.833333</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>5.838487</td>\n",
       "      <td>50.966273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.018932</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>0.395656</td>\n",
       "      <td>0.357789</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.27</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>DEBT_EARNINGS_RATIO</td>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>415.833333</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>5.838487</td>\n",
       "      <td>50.966273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.018932</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>0.395656</td>\n",
       "      <td>0.357789</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>DEBT_EARNINGS_RATIO</td>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>415.833333</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>5.838487</td>\n",
       "      <td>50.966273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.018932</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>0.395656</td>\n",
       "      <td>0.357789</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>DEBT_EARNINGS_RATIO</td>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>415.833333</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>5.838487</td>\n",
       "      <td>50.966273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.018932</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>0.395656</td>\n",
       "      <td>0.357789</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>DEBT_EARNINGS_RATIO</td>\n",
       "      <td>/Users/fchirigati/projects/dataset-ranking/use...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>415.833333</td>\n",
       "      <td>3141.540889</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>5.838487</td>\n",
       "      <td>50.966273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012156</td>\n",
       "      <td>-0.018932</td>\n",
       "      <td>-0.013892</td>\n",
       "      <td>-0.095708</td>\n",
       "      <td>0.395656</td>\n",
       "      <td>0.357789</td>\n",
       "      <td>loss</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "      <td>fp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query               target  \\\n",
       "0  /Users/fchirigati/projects/dataset-ranking/use...  DEBT_EARNINGS_RATIO   \n",
       "1  /Users/fchirigati/projects/dataset-ranking/use...  DEBT_EARNINGS_RATIO   \n",
       "2  /Users/fchirigati/projects/dataset-ranking/use...  DEBT_EARNINGS_RATIO   \n",
       "3  /Users/fchirigati/projects/dataset-ranking/use...  DEBT_EARNINGS_RATIO   \n",
       "4  /Users/fchirigati/projects/dataset-ranking/use...  DEBT_EARNINGS_RATIO   \n",
       "\n",
       "                                           candidate  query_num_of_columns  \\\n",
       "0  /Users/fchirigati/projects/dataset-ranking/use...                  12.0   \n",
       "1  /Users/fchirigati/projects/dataset-ranking/use...                  12.0   \n",
       "2  /Users/fchirigati/projects/dataset-ranking/use...                  12.0   \n",
       "3  /Users/fchirigati/projects/dataset-ranking/use...                  12.0   \n",
       "4  /Users/fchirigati/projects/dataset-ranking/use...                  12.0   \n",
       "\n",
       "   query_num_of_rows  query_row_column_ratio  query_max_mean  \\\n",
       "0             4990.0              415.833333     3141.540889   \n",
       "1             4990.0              415.833333     3141.540889   \n",
       "2             4990.0              415.833333     3141.540889   \n",
       "3             4990.0              415.833333     3141.540889   \n",
       "4             4990.0              415.833333     3141.540889   \n",
       "\n",
       "   query_max_outlier_percentage  query_max_skewness  query_max_kurtosis  ...  \\\n",
       "0                      0.009218            5.838487           50.966273  ...   \n",
       "1                      0.009218            5.838487           50.966273  ...   \n",
       "2                      0.009218            5.838487           50.966273  ...   \n",
       "3                      0.009218            5.838487           50.966273  ...   \n",
       "4                      0.009218            5.838487           50.966273  ...   \n",
       "\n",
       "   decrease_in_mae  decrease_in_mse  decrease_in_medae  gain_in_r2_score  \\\n",
       "0         0.012156        -0.018932          -0.013892         -0.095708   \n",
       "1         0.012156        -0.018932          -0.013892         -0.095708   \n",
       "2         0.012156        -0.018932          -0.013892         -0.095708   \n",
       "3         0.012156        -0.018932          -0.013892         -0.095708   \n",
       "4         0.012156        -0.018932          -0.013892         -0.095708   \n",
       "\n",
       "   r2_score_before  r2_score_after  class  p(gain)  p(loss)  eval  \n",
       "0         0.395656        0.357789   loss     0.73     0.27    fp  \n",
       "1         0.395656        0.357789   loss     0.68     0.32    fp  \n",
       "2         0.395656        0.357789   loss     0.69     0.31    fp  \n",
       "3         0.395656        0.357789   loss     0.69     0.31    fp  \n",
       "4         0.395656        0.357789   loss     0.69     0.31    fp  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college_candidate_gains = pd.read_csv('../classification/college-debt-records-features-single-column-w-class')\n",
    "college_candidate_gains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the gain distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "OUTLIER_THRESHOLD_MAD = 2\n",
    "\n",
    "def remove_outliers_based_on_mad(data):\n",
    "    mad = median_absolute_deviation(data)\n",
    "    median = np.median(data)\n",
    "    return [i for i in data if np.fabs((i - median)/mad) < OUTLIER_THRESHOLD_MAD]\n",
    "\n",
    "def plot_histogram(case_study, gains, remove_outliers_mad=False):\n",
    "    if remove_outliers_mad:\n",
    "        gains = remove_outliers_based_on_mad(gains)\n",
    "\n",
    "    weights = np.ones_like(gains)/float(len(gains))\n",
    "    plt.hist(gains, bins=20, weights=weights)\n",
    "    plt.xlabel('Value Ranges')\n",
    "    plt.ylabel('Percentages')\n",
    "    plt.title('Gains in R2-score - ' + case_study)\n",
    "    plt.show()\n",
    "\n",
    "gains = college_candidate_gains['gain_in_r2_score']\n",
    "plot_histogram('College-Debt', gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute the gradients to see when the gains start dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.upload.a817349748524c618bec5505f46feaef_UNITID_RET_FTL4',\n",
       "  0.19484267796557944,\n",
       "  0.0],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.socrata.data-wa-gov.wajg-ig9g_UNITID_OPEID6',\n",
       "  0.2443284449850829,\n",
       "  0.04948576701950347],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.upload.a817349748524c618bec5505f46feaef_UNITID_OPEID6',\n",
       "  0.2443284449850829,\n",
       "  0.0],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.socrata.data-wa-gov.wajg-ig9g_UNITID_OPEID',\n",
       "  0.2719458459180579,\n",
       "  0.027617400932975017],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.upload.a817349748524c618bec5505f46feaef_UNITID_OPEID',\n",
       "  0.2719458459180582,\n",
       "  2.7755575615628914e-16],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.socrata.data-wa-gov.wajg-ig9g_UNITID_RET_FT4',\n",
       "  0.28538799230431744,\n",
       "  0.013442146386259235],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.upload.a817349748524c618bec5505f46feaef_UNITID_RET_FT4',\n",
       "  0.28538799230431744,\n",
       "  0.0],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.socrata.data-wa-gov.wajg-ig9g_UNITID_PCIP52',\n",
       "  0.2924862570862705,\n",
       "  0.007098264781953079],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.upload.a817349748524c618bec5505f46feaef_UNITID_PCIP52',\n",
       "  0.2924862570862705,\n",
       "  0.0],\n",
       " ['/Users/fchirigati/projects/dataset-ranking/use-cases/data/college-debt/college-debt-v2.csv',\n",
       "  '/Users/fchirigati/projects/dataset-ranking/use-cases/datamart-data/companion-datasets-single-column/datamart.socrata.data-wa-gov.wajg-ig9g_UNITID_PREDDEG',\n",
       "  0.3632643909555502,\n",
       "  0.07077813386927967]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_gradients(candidate_gains):\n",
    "    '''\n",
    "    given a table with features 'candidate' and 'gain_in_r2_score', \n",
    "    this method creates a list with these two fields, sorts it in ascending order of gains \n",
    "    and computes the gradients between elements in positions x and x - 1\n",
    "    '''\n",
    "    gains = sorted([[row['query'], row['candidate'], row['gain_in_r2_score']] \n",
    "                    for index, row in candidate_gains.iterrows()], \n",
    "                   key=lambda x: x[2])\n",
    "    gradients = [float('-inf')]\n",
    "    for index, elem in enumerate(gains):\n",
    "        if index < len(gains) - 1:\n",
    "            gradients.append(gains[index + 1][2] - gains[index][2])\n",
    "            \n",
    "    gains_gradients = []\n",
    "    for gain, gradient in zip(gains, gradients):\n",
    "        gain.append(gradient)\n",
    "        gains_gradients.append(gain)\n",
    "        \n",
    "    return gains_gradients\n",
    "\n",
    "college_gains_gradients = compute_gradients(college_candidate_gains)\n",
    "college_gains_gradients[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like the biggest drop is between the top-rated candidate and the second one. Is it safe to assume that we only have one really good candidate? Maybe we can implement a few different policies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harsh_gradient_policy(candidates_gains_gradients):\n",
    "    '''\n",
    "    Given a list of candidates such that each entry corresponds to \n",
    "    \n",
    "        ['query', candidate', 'gain', 'gradient']\n",
    "    \n",
    "    and the list is sorted (ascending order) based on 'gain',\n",
    "    this method finds the position that corresponds to the largest gradient, \n",
    "    and then sets the class of every candidate equals to or below it position-wise to 'loss'. \n",
    "    The other candidates are linked to label 'gain' unless their 'gain' values are negative.\n",
    "    '''\n",
    "    \n",
    "    # Can't use Python's \"index\" function because it gets the first occurrence of a value. If \n",
    "    # the maximum gradient occurs more than once, we're interested in its last occurrence.\n",
    "    max_gradient = float('-inf')\n",
    "    position = -1\n",
    "    for index, elem in enumerate(reversed(candidates_gains_gradients)):\n",
    "        if elem[3] >= max_gradient:\n",
    "            max_gradient = elem[3]\n",
    "        else: # drop in gradient\n",
    "            position = len(candidates_gains_gradients) - index -1\n",
    "            break\n",
    "    \n",
    "    labels = [(elem[0], elem[1], 'gain') if index > position and elem[2] > 0 else (elem[0], elem[1], 'loss') \n",
    "              for index, elem in enumerate(candidates_gains_gradients)]\n",
    "     \n",
    "    return pd.DataFrame(labels, columns = ['query', 'candidate', 'harsh_grad_class'])\n",
    "\n",
    "def nth_gradient_drop_policy(candidates_gains_gradients, n):\n",
    "    '''\n",
    "    Given a list of candidates such that each entry corresponds to \n",
    "    \n",
    "        ['query', candidate', 'gain', 'gradient']\n",
    "    \n",
    "    and the list is sorted (ascending order) based on 'gain', \n",
    "    and an integer n, this method finds the position that corresponds \n",
    "    to the n-th gradient drop. Next, it sets the class of every candidate \n",
    "    equals to or below it (position-wise) to 'loss'. The other candidates are \n",
    "    linked to label 'gain'.\n",
    "    '''\n",
    "    \n",
    "    max_gradient = float('-inf')\n",
    "    position = -1\n",
    "    drops_seen = 0\n",
    "    for index, elem in enumerate(reversed(candidates_gains_gradients)):\n",
    "        if elem[3] >= max_gradient:\n",
    "            max_gradient = elem[3]\n",
    "        else: # drop in gradient\n",
    "            drops_seen += 1\n",
    "            position = len(candidates_gains_gradients) - index -1\n",
    "            if drops_seen == n:\n",
    "                break\n",
    "            else:\n",
    "                max_gradient = elem[3] #refactor this repetition\n",
    "   \n",
    "    labels = [(elem[0], elem[1], 'gain') if index > position and elem[2] > 0 else (elem[0], elem[1], 'loss') \n",
    "              for index, elem in enumerate(candidates_gains_gradients)]\n",
    "     \n",
    "    return pd.DataFrame(labels, columns = ['query', 'candidate', str(n) + 'th_grad_drop_class'])\n",
    "\n",
    "def get_order_of_magnitude(value):\n",
    "    '''\n",
    "    Returns order of magnitude of a given value\n",
    "    '''\n",
    "    if not value:\n",
    "        return float('-inf')\n",
    "    try:\n",
    "        return math.floor(math.log(np.fabs(value), 10))\n",
    "    except ValueError:\n",
    "        print('*** ERRRRRROOOOOORRR', value)\n",
    "        return None\n",
    "\n",
    "def order_of_magnitude_drop_policy(candidates_gains_gradients):\n",
    "    '''\n",
    "    Given a list of candidates such that each entry corresponds to \n",
    "    \n",
    "        ['query', candidate', 'gain', 'gradient']\n",
    "    \n",
    "    and the list is sorted (ascending order) based on 'gain', this method \n",
    "    finds the position that corresponds to the first drop in an order of \n",
    "    magnitude and sets the class of every candidate equals to or below it \n",
    "    (position-wise) to 'loss'. The other candidates are linked to label 'gain'.\n",
    "    '''\n",
    "    highest_order_of_magnitude = get_order_of_magnitude(candidates_gains_gradients[-1][2])\n",
    "    position = -1\n",
    "    for index, elem in enumerate(reversed(candidates_gains_gradients)):\n",
    "        if get_order_of_magnitude(elem[2]) < highest_order_of_magnitude:\n",
    "            position = len(candidates_gains_gradients) - index -1\n",
    "            break\n",
    "\n",
    "    labels = [(elem[0], elem[1], 'gain') if index > position and elem[2] > 0 else (elem[0], elem[1], 'loss') \n",
    "              for index, elem in enumerate(candidates_gains_gradients)]\n",
    "     \n",
    "    return pd.DataFrame(labels, columns = ['query', 'candidate', 'order_of_mag_drop_class'])\n",
    "\n",
    "def median_based_policy(candidates_gains_gradients):\n",
    "    '''\n",
    "    Given a list of candidates such that each entry corresponds to \n",
    "    \n",
    "        ['query', candidate', 'gain', 'gradient']\n",
    "    \n",
    "    and the list is sorted (ascending order) based on 'gain', this method \n",
    "    gets the median position and sets the class of every candidate equals to or below it \n",
    "    (position-wise) to 'loss'. The other candidates are linked to label 'gain'.\n",
    "    '''\n",
    "    position = math.ceil(len(candidates_gains_gradients)/2)\n",
    "    labels = [(elem[0], elem[1], 'gain') if index > position and elem[2] > 0 else (elem[0], elem[1], 'loss') \n",
    "              for index, elem in enumerate(candidates_gains_gradients)]\n",
    "     \n",
    "    return pd.DataFrame(labels, columns = ['query', 'candidate', 'median_based_class'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's create class labels for datasets based on these different policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original column 'class' in the dataset is such that if 'gain_in_r2_score' > 0, \n",
    "# the label is 'gain'; otherwise, it is 'loss'.\n",
    "college_candidate_gains = college_candidate_gains.rename(columns={'class': 'class_pos_neg'})\n",
    "harsh_gradient_classes = harsh_gradient_policy(college_gains_gradients)\n",
    "second_grad_drop_classes = nth_gradient_drop_policy(college_gains_gradients, 2)\n",
    "order_of_magnitude_drop_classes = order_of_magnitude_drop_policy(college_gains_gradients)\n",
    "median_based_classes = median_based_policy(college_gains_gradients)\n",
    "\n",
    "college_candidate_gains = college_candidate_gains.merge(harsh_gradient_classes, on=['query', 'candidate']) \n",
    "college_candidate_gains = college_candidate_gains.merge(second_grad_drop_classes, on=['query', 'candidate'])\n",
    "college_candidate_gains = college_candidate_gains.merge(order_of_magnitude_drop_classes, on=['query', 'candidate'])\n",
    "college_candidate_gains = college_candidate_gains.merge(median_based_classes, on=['query', 'candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 47)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college_candidate_gains.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the same for taxi, poverty, and openml-based datasets that were created for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (447, 39)\n",
      "after (447, 43)\n"
     ]
    }
   ],
   "source": [
    "taxi_candidate_gains = pd.read_csv('../classification/taxi-vehicle-collision-records-features-single-column-w-class')\n",
    "taxi_gains_gradients = compute_gradients(taxi_candidate_gains)\n",
    "print('before', taxi_candidate_gains.shape)\n",
    "\n",
    "taxi_candidate_gains = taxi_candidate_gains.rename(columns={'class': 'class_pos_neg'})\n",
    "harsh_gradient_classes = harsh_gradient_policy(taxi_gains_gradients)\n",
    "second_grad_drop_classes = nth_gradient_drop_policy(taxi_gains_gradients, 2)\n",
    "order_of_magnitude_drop_classes = order_of_magnitude_drop_policy(taxi_gains_gradients)\n",
    "median_based_classes = median_based_policy(taxi_gains_gradients)\n",
    "\n",
    "taxi_candidate_gains = taxi_candidate_gains.merge(harsh_gradient_classes, on=['query', 'candidate']) \n",
    "taxi_candidate_gains = taxi_candidate_gains.merge(second_grad_drop_classes, on=['query', 'candidate'])\n",
    "taxi_candidate_gains = taxi_candidate_gains.merge(order_of_magnitude_drop_classes, on=['query', 'candidate'])\n",
    "taxi_candidate_gains = taxi_candidate_gains.merge(median_based_classes, on=['query', 'candidate'])\n",
    "print('after', taxi_candidate_gains.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (130928, 37)\n",
      "after (130928, 41)\n"
     ]
    }
   ],
   "source": [
    "poverty_candidate_gains = pd.read_csv('../classification/poverty-estimation-results-features-and-targets-training.csv')\n",
    "poverty_candidate_gains.drop_duplicates(subset=['query', 'candidate'])\n",
    "poverty_gains_gradients = compute_gradients(poverty_candidate_gains)\n",
    "print('before', poverty_candidate_gains.shape)\n",
    "\n",
    "poverty_candidate_gains = poverty_candidate_gains.rename(columns={'class': 'class_pos_neg'})\n",
    "harsh_gradient_classes = harsh_gradient_policy(poverty_gains_gradients).drop_duplicates(subset=['query', 'candidate'])\n",
    "second_grad_drop_classes = nth_gradient_drop_policy(poverty_gains_gradients, 2).drop_duplicates(subset=['query', 'candidate'])\n",
    "order_of_magnitude_drop_classes = order_of_magnitude_drop_policy(poverty_gains_gradients).drop_duplicates(subset=['query', 'candidate'])\n",
    "median_based_classes = median_based_policy(poverty_gains_gradients).drop_duplicates(subset=['query', 'candidate'])\n",
    "\n",
    "poverty_candidate_gains = poverty_candidate_gains.merge(harsh_gradient_classes, on=['query', 'candidate'], how='left') \n",
    "poverty_candidate_gains = poverty_candidate_gains.merge(second_grad_drop_classes, on=['query', 'candidate'], how='left')\n",
    "poverty_candidate_gains = poverty_candidate_gains.merge(order_of_magnitude_drop_classes, on=['query', 'candidate'], how='left')\n",
    "poverty_candidate_gains = poverty_candidate_gains.merge(median_based_classes, on=['query', 'candidate'], how='left')\n",
    "print('after', poverty_candidate_gains.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, regarding the openml-based datasets, we need to separate query by query and treat each one of them as a subproblem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NUMBER OF QUERIES 5138\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "openml_training_candidate_gains = pd.read_csv('../classification/training-simplified-data-generation-many-candidates-per-query_with_median_and_mean_based_classes.csv')\n",
    "openml_training_candidate_gains['class_pos_neg'] = ['loss' if row['gain_in_r2_score'] <= 0 else 'gain' \n",
    "                                                    for index, row in openml_training_candidate_gains.iterrows()]\n",
    "openml_training_candidate_gains = openml_training_candidate_gains.drop_duplicates(subset=['query', 'candidate'])\n",
    "\n",
    "harsh_gradient_classes = []\n",
    "second_grad_drop_classes = []\n",
    "order_of_magnitude_drop_classes = []\n",
    "median_based_classes = []\n",
    "\n",
    "queries = set(openml_training_candidate_gains['query'])\n",
    "number_of_queries = len(queries); i = 0\n",
    "print('*** NUMBER OF QUERIES', number_of_queries)\n",
    "for q in queries:\n",
    "    subtable = openml_training_candidate_gains.loc[openml_training_candidate_gains['query'] == q]\n",
    "    sub_gains_gradients = compute_gradients(subtable)    \n",
    "    \n",
    "    sub_harsh_gradient_classes = harsh_gradient_policy(sub_gains_gradients)\n",
    "    harsh_gradient_classes.append(sub_harsh_gradient_classes)\n",
    "    \n",
    "    sub_second_grad_drop_classes = nth_gradient_drop_policy(sub_gains_gradients, 2)\n",
    "    second_grad_drop_classes.append(sub_second_grad_drop_classes)\n",
    "    \n",
    "    sub_order_of_magnitude_drop_classes = order_of_magnitude_drop_policy(sub_gains_gradients)\n",
    "    order_of_magnitude_drop_classes.append(sub_order_of_magnitude_drop_classes)\n",
    "    \n",
    "    sub_median_based_classes = median_based_policy(sub_gains_gradients)\n",
    "    median_based_classes.append(sub_median_based_classes)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "harsh_gradient_classes = pd.concat(harsh_gradient_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "second_grad_drop_classes = pd.concat(second_grad_drop_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "order_of_magnitude_drop_classes = pd.concat(order_of_magnitude_drop_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "median_based_classes = pd.concat(median_based_classes).drop_duplicates(subset=['query', 'candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('openml_training_many_candidates_harsh_grad_classes.csv', 'w')\n",
    "f.write(harsh_gradient_classes.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('openml_training_many_candidates_second_grad_drop_classes.csv', 'w')\n",
    "f.write(second_grad_drop_classes.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('openml_training_many_candidates_order_magnitude_drop_classes.csv', 'w')\n",
    "f.write(order_of_magnitude_drop_classes.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('openml_training_many_candidates_median_based_classes.csv', 'w')\n",
    "f.write(median_based_classes.to_csv(index=False))\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(529214, 3) (529214, 3) (529214, 3) (529214, 3) (529214, 39)\n",
      "before (529214, 39)\n",
      "after (529214, 43)\n"
     ]
    }
   ],
   "source": [
    "print(harsh_gradient_classes.shape, \n",
    "      second_grad_drop_classes.shape, \n",
    "      order_of_magnitude_drop_classes.shape,\n",
    "      median_based_classes.shape,\n",
    "      openml_training_candidate_gains.shape\n",
    "     )\n",
    "\n",
    "print('before', openml_training_candidate_gains.shape)\n",
    "openml_training_candidate_gains = openml_training_candidate_gains.merge(harsh_gradient_classes, on=['query', 'candidate'], how='left') \n",
    "openml_training_candidate_gains = openml_training_candidate_gains.merge(second_grad_drop_classes, on=['query', 'candidate'], how='left')\n",
    "openml_training_candidate_gains = openml_training_candidate_gains.merge(order_of_magnitude_drop_classes, on=['query', 'candidate'], how='left')\n",
    "openml_training_candidate_gains = openml_training_candidate_gains.merge(median_based_classes, on=['query', 'candidate'], how='left')\n",
    "print('after', openml_training_candidate_gains.shape)\n",
    "#openml_training_candidate_gains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do it for the openml test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NUMBER OF QUERIES 1942\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "openml_test_candidate_gains = pd.read_csv('../classification/test-simplified-data-generation-many-candidates-per-query_with_median_and_mean_based_classes.csv')\n",
    "openml_test_candidate_gains['class_pos_neg'] = ['loss' if row['gain_in_r2_score'] <= 0 else 'gain' \n",
    "                                                    for index, row in openml_test_candidate_gains.iterrows()]\n",
    "openml_test_candidate_gains = openml_test_candidate_gains.drop_duplicates(subset=['query', 'candidate'])\n",
    "\n",
    "harsh_gradient_classes = []\n",
    "second_grad_drop_classes = []\n",
    "order_of_magnitude_drop_classes = []\n",
    "median_based_classes = []\n",
    "\n",
    "queries = set(openml_test_candidate_gains['query'])\n",
    "number_of_queries = len(queries); i = 0\n",
    "print('*** NUMBER OF QUERIES', number_of_queries)\n",
    "for q in queries:\n",
    "    subtable = openml_test_candidate_gains.loc[openml_test_candidate_gains['query'] == q]\n",
    "    sub_gains_gradients = compute_gradients(subtable)    \n",
    "    \n",
    "    sub_harsh_gradient_classes = harsh_gradient_policy(sub_gains_gradients)\n",
    "    harsh_gradient_classes.append(sub_harsh_gradient_classes)\n",
    "    \n",
    "    sub_second_grad_drop_classes = nth_gradient_drop_policy(sub_gains_gradients, 2)\n",
    "    second_grad_drop_classes.append(sub_second_grad_drop_classes)\n",
    "    \n",
    "    sub_order_of_magnitude_drop_classes = order_of_magnitude_drop_policy(sub_gains_gradients)\n",
    "    order_of_magnitude_drop_classes.append(sub_order_of_magnitude_drop_classes)\n",
    "    \n",
    "    sub_median_based_classes = median_based_policy(sub_gains_gradients)\n",
    "    median_based_classes.append(sub_median_based_classes)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "\n",
    "harsh_gradient_classes = pd.concat(harsh_gradient_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "second_grad_drop_classes = pd.concat(second_grad_drop_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "order_of_magnitude_drop_classes = pd.concat(order_of_magnitude_drop_classes).drop_duplicates(subset=['query', 'candidate'])\n",
    "median_based_classes = pd.concat(median_based_classes).drop_duplicates(subset=['query', 'candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75738, 3) (75738, 3) (75738, 3) (75738, 3) (75738, 39)\n",
      "before (75738, 39)\n",
      "after (75738, 43)\n"
     ]
    }
   ],
   "source": [
    "print(harsh_gradient_classes.shape, \n",
    "      second_grad_drop_classes.shape, \n",
    "      order_of_magnitude_drop_classes.shape,\n",
    "      median_based_classes.shape,\n",
    "      openml_test_candidate_gains.shape\n",
    "     )\n",
    "\n",
    "print('before', openml_test_candidate_gains.shape)\n",
    "openml_test_candidate_gains = openml_test_candidate_gains.merge(harsh_gradient_classes, on=['query', 'candidate'], how='left') \n",
    "openml_test_candidate_gains = openml_test_candidate_gains.merge(second_grad_drop_classes, on=['query', 'candidate'], how='left')\n",
    "openml_test_candidate_gains = openml_test_candidate_gains.merge(order_of_magnitude_drop_classes, on=['query', 'candidate'], how='left')\n",
    "openml_test_candidate_gains = openml_test_candidate_gains.merge(median_based_classes, on=['query', 'candidate'], how='left')\n",
    "print('after', openml_test_candidate_gains.shape)\n",
    "#openml_training_candidate_gains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's save all these new versions of the datasets on disk now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../classification/college-debt-different-class-definitions.csv', 'w')\n",
    "f.write(college_candidate_gains.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('../classification/taxi-vehicle-collision-different-class-definitions.csv', 'w')\n",
    "f.write(taxi_candidate_gains.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('../classification/poverty-estimation-different-class-definitions.csv', 'w')\n",
    "f.write(poverty_candidate_gains.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('../classification/training-simplified-data-generation-many-candidates-different-class-definitions.csv', 'w')\n",
    "f.write(openml_training_candidate_gains.to_csv(index=False))\n",
    "f.close()\n",
    "\n",
    "f = open('../classification/test-simplified-data-generation-many-candidates-different-class-definitions.csv', 'w')\n",
    "f.write(openml_test_candidate_gains.to_csv(index=False))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
