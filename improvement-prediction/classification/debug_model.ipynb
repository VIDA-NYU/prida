{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we implement different strategies to debugging our model, trying to detect those training instances that are leading to most confusion across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "FEATURES = ['query_num_of_columns', 'query_num_of_rows', 'query_row_column_ratio', 'query_max_skewness', \n",
    "            'query_max_kurtosis', 'query_max_unique', 'candidate_num_rows', 'candidate_row_column_ratio', \n",
    "            'candidate_max_skewness', 'candidate_max_kurtosis', 'candidate_max_unique', 'query_target_max_pearson', \n",
    "            'query_target_max_spearman', 'query_target_max_covariance', 'query_target_max_mutual_info', \n",
    "            'candidate_target_max_pearson', 'candidate_target_max_spearman', 'candidate_target_max_covariance', \n",
    "            'candidate_target_max_mutual_info', 'max_pearson_difference', 'containment_fraction']\n",
    "TARGET = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test_over_same_data(data):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(data[FEATURES], data[TARGET])\n",
    "    preds = rf.predict(data[FEATURES])\n",
    "    print(classification_report(data[TARGET], preds))\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a742412a579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'good_gain'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_in_r2_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'loss'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training-simplified-data-generation-many-candidates-per-query_with_median_and_mean_based_classes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'good_gain'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gain_in_r2_score'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'loss'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    229\u001b[0m                                        raise_cast_failure=True)\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             block = make_block(block,\n\u001b[1;32m   3074\u001b[0m                                \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m                                ndim=1, fastpath=True)\n\u001b[0m\u001b[1;32m   3076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m     return klass(values, ndim=ndim, fastpath=fastpath,\n\u001b[0;32m-> 1847\u001b[0;31m                  placement=placement)\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, ndim, fastpath, placement)\u001b[0m\n\u001b[1;32m   1243\u001b[0m         super(ObjectBlock, self).__init__(values, ndim=ndim,\n\u001b[1;32m   1244\u001b[0m                                           \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1245\u001b[0;31m                                           placement=placement)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/alibezz/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mmgr_locs\u001b[0;34m(self, new_mgr_locs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmgr_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlockPlacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockPlacement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_2 = pd.read_csv('training-simplified-data-generation.csv')\n",
    "dataset_2[TARGET] = ['good_gain' if row['gain_in_r2_score'] > 0 else 'loss' for index, row in dataset_2.iterrows()] \n",
    "dataset_3 = pd.read_csv('training-simplified-data-generation-many-candidates-per-query_with_median_and_mean_based_classes.csv')\n",
    "dataset_3[TARGET] = ['good_gain' if row['gain_in_r2_score'] > 0 else 'loss' for index, row in dataset_3.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       1.00      1.00      1.00      5707\n",
      "        loss       1.00      1.00      1.00      4177\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      9884\n",
      "   macro avg       1.00      1.00      1.00      9884\n",
      "weighted avg       1.00      1.00      1.00      9884\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_dataset_2 = train_and_test_over_same_data(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       1.00      1.00      1.00   1019700\n",
      "        loss       1.00      1.00      1.00   1097156\n",
      "\n",
      "   micro avg       1.00      1.00      1.00   2116856\n",
      "   macro avg       1.00      1.00      1.00   2116856\n",
      "weighted avg       1.00      1.00      1.00   2116856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_dataset_3 = train_and_test_over_same_data(dataset_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So it seems like these two models model their own instances \"perfectly\", i.e., the in-training error is 0. Let's double-check how these models behave over the college use case --- our focus now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "college = pd.read_csv('college-debt-records-features-single-column-w-class')\n",
    "college[TARGET] = ['good_gain' if row['gain_in_r2_score'] > 0 else 'loss' for index, row in college.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(data):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(data[FEATURES], data[TARGET])\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "college_preds_dataset_2 = rf_dataset_2.predict(college[FEATURES])\n",
    "college_preds_dataset_3 = rf_dataset_3.predict(college[FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.16      0.97      0.27       130\n",
      "        loss       0.99      0.30      0.46       973\n",
      "\n",
      "   micro avg       0.38      0.38      0.38      1103\n",
      "   macro avg       0.57      0.63      0.36      1103\n",
      "weighted avg       0.89      0.38      0.44      1103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(college[TARGET], college_preds_dataset_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.12      0.88      0.21       130\n",
      "        loss       0.90      0.15      0.26       973\n",
      "\n",
      "   micro avg       0.23      0.23      0.23      1103\n",
      "   macro avg       0.51      0.51      0.23      1103\n",
      "weighted avg       0.81      0.23      0.25      1103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(college[TARGET], college_preds_dataset_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check how often the predictions for both models are positive --- my impression is that this model generates too many 'good_gain' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"number of 'good_gain' predictions using dataset_2:\", 808)\n",
      "(\"number of 'good_gain' predictions using dataset_3:\", 942)\n"
     ]
    }
   ],
   "source": [
    "print('number of \\'good_gain\\' predictions using dataset_2:', len([i for i in college_preds_dataset_2 if i == 'good_gain']))\n",
    "print('number of \\'good_gain\\' predictions using dataset_3:', len([i for i in college_preds_dataset_3 if i == 'good_gain']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So yes: for dataset_2, we have 808 out of 1103 'good_gain' predictions (73%); for dataset_3, 942 out of 1103 (85%). \n",
    "\n",
    "### How is the classification report for the synthetic test that was generated along with dataset_2? Is it the case that most predictions are also positive there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.67      0.73      0.70      2496\n",
      "        loss       0.57      0.50      0.53      1780\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4276\n",
      "   macro avg       0.62      0.62      0.62      4276\n",
      "weighted avg       0.63      0.64      0.63      4276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_for_dataset_2 = pd.read_csv('test-simplified-data-generation.csv')\n",
    "test_for_dataset_2[TARGET] = ['good_gain' if row['gain_in_r2_score'] > 0 else 'loss' for index, row in test_for_dataset_2.iterrows()]\n",
    "test_for_dataset_2_preds = rf_dataset_2.predict(test_for_dataset_2[FEATURES])\n",
    "print(classification_report(test_for_dataset_2[TARGET], test_for_dataset_2_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"number of 'good_gain' predictions using dataset_2:\", 2716)\n"
     ]
    }
   ],
   "source": [
    "print('number of \\'good_gain\\' predictions using dataset_2:', \n",
    "      len([i for i in test_for_dataset_2_preds if i == 'good_gain']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the problem of predicting 'good_gain' far more than the necessary did not happen for the synthetic test data. Is it possible to remove some instances from dataset 2 and get good results for both college and synthetic test datasets?\n",
    "\n",
    "### Let's try removing everything with relative gain in a certain interval first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>target</th>\n",
       "      <th>candidate</th>\n",
       "      <th>query_num_of_columns</th>\n",
       "      <th>query_num_of_rows</th>\n",
       "      <th>query_row_column_ratio</th>\n",
       "      <th>query_max_mean</th>\n",
       "      <th>query_max_outlier_percentage</th>\n",
       "      <th>query_max_skewness</th>\n",
       "      <th>query_max_kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th>candidate_target_max_mutual_info</th>\n",
       "      <th>max_pearson_difference</th>\n",
       "      <th>containment_fraction</th>\n",
       "      <th>decrease_in_mae</th>\n",
       "      <th>decrease_in_mse</th>\n",
       "      <th>decrease_in_medae</th>\n",
       "      <th>gain_in_r2_score</th>\n",
       "      <th>r2_score_before</th>\n",
       "      <th>r2_score_after</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 36dcadcb-1b0d-4429-886e-1ec1b8b65b94</td>\n",
       "      <td> -2.1218834e+000</td>\n",
       "      <td> caefd73a-d31a-420e-bb46-56019fa79bec</td>\n",
       "      <td>  6</td>\n",
       "      <td>  99</td>\n",
       "      <td> 16.500000</td>\n",
       "      <td>  0.021433</td>\n",
       "      <td> 0.010101</td>\n",
       "      <td> 0.895298</td>\n",
       "      <td> 1.355716</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.024278</td>\n",
       "      <td>-0.250439</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>-0.017368</td>\n",
       "      <td>-0.022395</td>\n",
       "      <td> 0.003941</td>\n",
       "      <td> 0.206048</td>\n",
       "      <td> 0.098032</td>\n",
       "      <td> 0.118231</td>\n",
       "      <td> good_gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0f33be7a-4627-4057-a8e5-5845991e7d77</td>\n",
       "      <td>  4.1085606e-001</td>\n",
       "      <td> 47b0d820-e6bb-4482-9422-972f20c3d0bd</td>\n",
       "      <td>  9</td>\n",
       "      <td>  99</td>\n",
       "      <td> 11.000000</td>\n",
       "      <td>  0.014549</td>\n",
       "      <td> 0.000000</td>\n",
       "      <td> 0.480805</td>\n",
       "      <td> 1.331217</td>\n",
       "      <td>...</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>-0.468890</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.001482</td>\n",
       "      <td> 0.018816</td>\n",
       "      <td> 0.165079</td>\n",
       "      <td>-0.009627</td>\n",
       "      <td> 0.661532</td>\n",
       "      <td> 0.655164</td>\n",
       "      <td>      loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 108e3ab1-a27b-44f4-bc31-2a3732d775d0</td>\n",
       "      <td>  1.0913311e-001</td>\n",
       "      <td> 50da1264-30ba-45d2-9898-29686873b921</td>\n",
       "      <td> 19</td>\n",
       "      <td> 999</td>\n",
       "      <td> 52.578947</td>\n",
       "      <td>  0.001559</td>\n",
       "      <td> 0.004004</td>\n",
       "      <td> 0.752183</td>\n",
       "      <td> 1.245622</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.183599</td>\n",
       "      <td>-0.428453</td>\n",
       "      <td> 0.414414</td>\n",
       "      <td> 0.002859</td>\n",
       "      <td> 0.003995</td>\n",
       "      <td>-0.025124</td>\n",
       "      <td>-0.007954</td>\n",
       "      <td> 0.334363</td>\n",
       "      <td> 0.331704</td>\n",
       "      <td>      loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td> 60040170-eb69-4e6b-afab-a7b201eb4728</td>\n",
       "      <td>           class</td>\n",
       "      <td> 16ff95e0-84d5-441b-bcc9-75f9cd5d96d3</td>\n",
       "      <td>  9</td>\n",
       "      <td> 270</td>\n",
       "      <td> 30.000000</td>\n",
       "      <td>  0.661290</td>\n",
       "      <td> 0.007407</td>\n",
       "      <td> 1.262893</td>\n",
       "      <td> 1.964043</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.097627</td>\n",
       "      <td>-0.187404</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.012919</td>\n",
       "      <td> 0.000148</td>\n",
       "      <td> 0.075000</td>\n",
       "      <td>-0.000191</td>\n",
       "      <td> 0.435986</td>\n",
       "      <td> 0.435903</td>\n",
       "      <td>      loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td> e1989182-d772-4410-94b4-9e31f1973d00</td>\n",
       "      <td>             fat</td>\n",
       "      <td> 44666071-33bf-4ea2-bbc8-cc08ccab3fe0</td>\n",
       "      <td> 60</td>\n",
       "      <td> 240</td>\n",
       "      <td>  4.000000</td>\n",
       "      <td> 62.845375</td>\n",
       "      <td> 0.020833</td>\n",
       "      <td> 1.231087</td>\n",
       "      <td> 6.123410</td>\n",
       "      <td>...</td>\n",
       "      <td> 0.946263</td>\n",
       "      <td>-0.960529</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.030768</td>\n",
       "      <td> 0.021367</td>\n",
       "      <td> 0.095869</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td> 0.985557</td>\n",
       "      <td> 0.985248</td>\n",
       "      <td>      loss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query           target  \\\n",
       "1  36dcadcb-1b0d-4429-886e-1ec1b8b65b94  -2.1218834e+000   \n",
       "2  0f33be7a-4627-4057-a8e5-5845991e7d77   4.1085606e-001   \n",
       "3  108e3ab1-a27b-44f4-bc31-2a3732d775d0   1.0913311e-001   \n",
       "5  60040170-eb69-4e6b-afab-a7b201eb4728            class   \n",
       "6  e1989182-d772-4410-94b4-9e31f1973d00              fat   \n",
       "\n",
       "                              candidate  query_num_of_columns  \\\n",
       "1  caefd73a-d31a-420e-bb46-56019fa79bec                     6   \n",
       "2  47b0d820-e6bb-4482-9422-972f20c3d0bd                     9   \n",
       "3  50da1264-30ba-45d2-9898-29686873b921                    19   \n",
       "5  16ff95e0-84d5-441b-bcc9-75f9cd5d96d3                     9   \n",
       "6  44666071-33bf-4ea2-bbc8-cc08ccab3fe0                    60   \n",
       "\n",
       "   query_num_of_rows  query_row_column_ratio  query_max_mean  \\\n",
       "1                 99               16.500000        0.021433   \n",
       "2                 99               11.000000        0.014549   \n",
       "3                999               52.578947        0.001559   \n",
       "5                270               30.000000        0.661290   \n",
       "6                240                4.000000       62.845375   \n",
       "\n",
       "   query_max_outlier_percentage  query_max_skewness  query_max_kurtosis  \\\n",
       "1                      0.010101            0.895298            1.355716   \n",
       "2                      0.000000            0.480805            1.331217   \n",
       "3                      0.004004            0.752183            1.245622   \n",
       "5                      0.007407            1.262893            1.964043   \n",
       "6                      0.020833            1.231087            6.123410   \n",
       "\n",
       "          ...          candidate_target_max_mutual_info  \\\n",
       "1         ...                                  0.024278   \n",
       "2         ...                                  1.000000   \n",
       "3         ...                                  0.183599   \n",
       "5         ...                                  0.097627   \n",
       "6         ...                                  0.946263   \n",
       "\n",
       "   max_pearson_difference  containment_fraction  decrease_in_mae  \\\n",
       "1               -0.250439              1.000000        -0.017368   \n",
       "2               -0.468890              1.000000         0.001482   \n",
       "3               -0.428453              0.414414         0.002859   \n",
       "5               -0.187404              1.000000         0.012919   \n",
       "6               -0.960529              1.000000         0.030768   \n",
       "\n",
       "   decrease_in_mse  decrease_in_medae  gain_in_r2_score  r2_score_before  \\\n",
       "1        -0.022395           0.003941          0.206048         0.098032   \n",
       "2         0.018816           0.165079         -0.009627         0.661532   \n",
       "3         0.003995          -0.025124         -0.007954         0.334363   \n",
       "5         0.000148           0.075000         -0.000191         0.435986   \n",
       "6         0.021367           0.095869         -0.000313         0.985557   \n",
       "\n",
       "   r2_score_after      class  \n",
       "1        0.118231  good_gain  \n",
       "2        0.655164       loss  \n",
       "3        0.331704       loss  \n",
       "5        0.435903       loss  \n",
       "6        0.985248       loss  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset_2 = dataset_2.loc[(dataset_2['gain_in_r2_score'] > 0.025) | (dataset_2['gain_in_r2_score'] < 0)]\n",
    "filtered_dataset_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_filtered_dataset_2 = create_model(filtered_dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.63      0.72      0.67       130\n",
      "        loss       0.96      0.94      0.95       973\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1103\n",
      "   macro avg       0.79      0.83      0.81      1103\n",
      "weighted avg       0.92      0.92      0.92      1103\n",
      "\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "college_preds_filtered_dataset_2 = rf_filtered_dataset_2.predict(college[FEATURES])\n",
    "print(classification_report(college[TARGET], college_preds_filtered_dataset_2))\n",
    "print(len([pred for pred in college_preds_filtered_dataset_2 if pred == 'good_gain']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.73      0.48      0.58      2496\n",
      "        loss       0.51      0.75      0.61      1780\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      4276\n",
      "   macro avg       0.62      0.61      0.59      4276\n",
      "weighted avg       0.64      0.59      0.59      4276\n",
      "\n",
      "1623\n"
     ]
    }
   ],
   "source": [
    "test_for_filtered_dataset_2_preds = rf_filtered_dataset_2.predict(test_for_dataset_2[FEATURES])\n",
    "print(classification_report(test_for_dataset_2[TARGET], test_for_filtered_dataset_2_preds))\n",
    "print(len([pred for pred in test_for_filtered_dataset_2_preds if pred == 'good_gain']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we filter the synthetic test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   good_gain       0.70      0.65      0.67      1560\n",
      "        loss       0.71      0.75      0.73      1768\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      3328\n",
      "   macro avg       0.70      0.70      0.70      3328\n",
      "weighted avg       0.70      0.70      0.70      3328\n",
      "\n",
      "1448\n"
     ]
    }
   ],
   "source": [
    "filtered_test_for_dataset_2 = test_for_dataset_2.loc[(test_for_dataset_2['gain_in_r2_score'] > 0.025) | (test_for_dataset_2['gain_in_r2_score'] < 0)]\n",
    "filtered_test_for_filtered_dataset_2_preds = rf_filtered_dataset_2.predict(filtered_test_for_dataset_2[FEATURES])\n",
    "print(classification_report(filtered_test_for_dataset_2[TARGET], filtered_test_for_filtered_dataset_2_preds))\n",
    "print(len([pred for pred in filtered_test_for_filtered_dataset_2_preds if pred == 'good_gain']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok. So the results become more compatible with the college use case when we filter *both* training and synthetic test. FYI, the maximum gain_in_r2_score for college is 0.36 and the minimum is -0.10. There's a large concentration of gains around zero and a bit above -0.1. \n",
    "\n",
    "#### My hunch is that gains close to zero are not a problem, but overlapping features for different classes. Using TSNE, I noticed that features 'query_row_column_ratio', 'candidate_target_max_pearson', 'candidate_target_max_spearman', 'max_pearson_difference', and 'containment_fraction' seem to separate the data better than all features taken into account at once. That said, when we use just these features and dataset_2, the results get super similar.\n",
    "\n",
    "#### Let me use HDBSCAN and see if I can remove instances from clusters with high entropy, using both top and all features...\n",
    "\n",
    "#### For college, there is a cluster that is a bit dirty when we use just the top five features to cluster: cluster size 109 positive fraction 0.7706422018348624 negative fraction 0.22935779816513763. These top features, by the way, led to 30 clusters. With all features, we have 65 clusters and some of them are dirty, but they're not as big as the one we just mentioned. With all features, there is a \"non-cluster\" (label = -1) with 285 instances. In other words, with all features, around 26% of all 1103 instances are too messy to even be assigned to a proper cluster... Maybe these features simply don't separate these cases well. In this \"non-cluster\", by the way, 12% of the instances are positive and 88% are negative. When we use just the top features, there's a \"non-cluster\" with 200 instances (18% of the data), where 4% of them are positive and 96% are negative --- i.e., it's not really a \"dirty\" cluster, even if it was assigned as such (maybe by chance?).\n",
    "\n",
    "#### When we use all features to cluster dataset_2, we get 494 clusters. With the top features, 450. With the top features, 2462 out of 9884 instances were assigned to a cluster with label -1 (a cluster for data samples that look so noisy that they are not assigned to any cluster). This specific \"cluster\" is 62% positive and 38% negative. When we use all features, the \"cluster\" with label -1 has 2342 instances: 54% positive and 46% negative. If we remove these very messy instances, what do we get for college? Well.. with all features, results don't really get any better. Not even when we remove other clusters that are messy (i.e., the proportional_difference between positive and negative fractions is below 0.6). When we use only top features, the results for college are only marginally better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-198d47356379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrf_just_five_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "rf_just_five_features = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
